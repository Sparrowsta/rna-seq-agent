"""
RNA-seqæ™ºèƒ½åˆ†æåŠ©æ‰‹å·¥å…·æ¨¡å—
æä¾›æ•°æ®æ”¶é›†åŠŸèƒ½ï¼Œè¿”å›ç»“æ„åŒ–æ•°æ®ä¾›LLMæ™ºèƒ½å¤„ç†å’Œå±•ç¤º

é‡æ„åŸåˆ™:
- å·¥å…·ä¸“æ³¨çº¯æ•°æ®æ”¶é›†ï¼Œä¸åšæ ¼å¼åŒ–
- ä½¿ç”¨å®˜æ–¹ @tool è£…é¥°å™¨
- ç§»é™¤åŒæ¨¡å¼é€»è¾‘ï¼Œäº¤ç”±LLMå†³å®šå±•ç¤ºæ–¹å¼
- ç®€åŒ–ä»£ç ç»“æ„ï¼Œæé«˜ç»´æŠ¤æ€§
"""

import json
import re
import time
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# ä½¿ç”¨å®˜æ–¹å·¥å…·è£…é¥°å™¨
from langchain_core.tools import tool

# å¯¼å…¥é…ç½®æ¨¡å—
from .config import get_tools_config


# ==================== å·¥å…·å‡½æ•° (ä½¿ç”¨ @tool è£…é¥°å™¨) ====================

@tool
def scan_fastq_files() -> Dict[str, Any]:
    """æ‰«æFASTQæ–‡ä»¶ï¼Œä¼˜å…ˆåœ¨æ•°æ®ç›®å½•(data/fastq)ä¸‹æŸ¥æ‰¾ï¼Œå…¼å®¹å®¹å™¨æŒ‚è½½ç›®å½•ã€‚

    è¿”å›ï¼šæ–‡ä»¶åˆ—è¡¨ã€æ ·æœ¬ä¿¡æ¯å’ŒåŸºæœ¬ç»Ÿè®¡æ•°æ®ã€‚
    """
    config = get_tools_config()
    fastq_extensions = ["*.fastq", "*.fastq.gz", "*.fq", "*.fq.gz"]

    # å®šä¹‰è¦æ’é™¤çš„ç›®å½•ï¼ˆä¸­é—´æ–‡ä»¶å’Œç¼“å­˜ç›®å½•ï¼‰
    exclude_directories = {
        "work", "tmp", "temp", "results", "output",
        ".nextflow", "logs", "cache", "__pycache__"
    }

    # é€‰æ‹©æœç´¢æ ¹ç›®å½•ï¼šä¼˜å…ˆ data/fastqï¼Œå…¶æ¬¡ dataï¼Œæœ€åé¡¹ç›®æ ¹ç›®å½•
    search_roots = []
    try:
        if config.fastq_dir.exists():
            search_roots.append(config.fastq_dir)
        elif config.settings.data_dir.exists():
            search_roots.append(config.settings.data_dir)
        else:
            search_roots.append(config.project_root)
    except Exception:
        search_roots.append(config.project_root)

    # æ‰«ææ‰€æœ‰FASTQæ–‡ä»¶
    all_fastq_files = []
    for root in search_roots:
        for ext in fastq_extensions:
            for file_path in root.rglob(ext):
                if any(excluded in file_path.parts for excluded in exclude_directories):
                    continue
                all_fastq_files.append(file_path)
    
    # æ”¶é›†æ–‡ä»¶ä¿¡æ¯
    file_list = []
    for file_path in all_fastq_files:
        if not file_path.exists() or not file_path.is_file():
            continue
            
        try:
            file_size = file_path.stat().st_size
            
            # æå–æ ·æœ¬ä¿¡æ¯
            filename = file_path.name
            if "_1." in filename or "_R1" in filename:
                sample_name = filename.split("_1.")[0].split("_R1")[0]
                read_type = "R1"
            elif "_2." in filename or "_R2" in filename:
                sample_name = filename.split("_2.")[0].split("_R2")[0]
                read_type = "R2"
            else:
                sample_name = filename.split(".")[0]
                read_type = "single"
            
            file_info = {
                "filename": filename,
                "full_path": str(file_path),
                "directory": str(file_path.parent),
                "size_bytes": file_size,
                "size_mb": round(file_size / 1024 / 1024, 2),
                "extension": file_path.suffix,
                "sample_name": sample_name,
                "read_type": read_type
            }
            file_list.append(file_info)
        except Exception:
            continue
    
    # æŒ‰æ–‡ä»¶åæ’åº
    file_list.sort(key=lambda x: x["filename"])
    
    # æ ·æœ¬ç»Ÿè®¡
    samples = {}
    for file_info in file_list:
        sample_name = file_info["sample_name"]
        read_type = file_info["read_type"]
        
        if sample_name not in samples:
            samples[sample_name] = {"R1": None, "R2": None, "single": None}
        
        samples[sample_name][read_type] = file_info
    
    # ç¡®å®šæµ‹åºç±»å‹
    paired_count = sum(1 for s in samples.values() if s["R1"] and s["R2"])
    single_count = sum(1 for s in samples.values() if s["single"])
    
    if paired_count > 0 and single_count == 0:
        sequencing_type = "paired_end"
    elif single_count > 0 and paired_count == 0:
        sequencing_type = "single_end"
    else:
        sequencing_type = "mixed"
    
    return {
        "detection_status": "success",
        "search_roots": [str(p) for p in search_roots],
        "total_files": len(file_list),
        "total_samples": len(samples),
        "sequencing_type": sequencing_type,
        "paired_samples": paired_count,
        "single_samples": single_count,
        "total_size_mb": sum(f["size_mb"] for f in file_list),
        "files": file_list,
        "samples": samples,
        "scan_timestamp": time.time()
    }


@tool
def scan_system_resources() -> Dict[str, Any]:
    """æ£€æµ‹ç³»ç»Ÿç¡¬ä»¶èµ„æºï¼Œè¿”å›CPUã€å†…å­˜ã€ç£ç›˜å’Œè´Ÿè½½ä¿¡æ¯"""
    try:
        import psutil
        
        # CPUä¿¡æ¯
        cpu_count = psutil.cpu_count(logical=False) or 1
        cpu_freq = psutil.cpu_freq()
        
        # å†…å­˜ä¿¡æ¯
        memory = psutil.virtual_memory()
        memory_gb = memory.total / 1024**3
        available_gb = memory.available / 1024**3
        used_percent = memory.percent
        
        # ç£ç›˜ä¿¡æ¯
        disk = psutil.disk_usage('.')
        disk_total_gb = disk.total / 1024**3
        disk_free_gb = disk.free / 1024**3
        disk_used_percent = (disk.used / disk.total) * 100
        
        # ç³»ç»Ÿè´Ÿè½½
        load_info = {}
        try:
            load_avg = psutil.getloadavg()
            load_ratio = load_avg[0] / max(cpu_count, 1)
            load_info = {
                "load_1min": round(load_avg[0], 2),
                "load_5min": round(load_avg[1], 2),
                "load_15min": round(load_avg[2], 2),
                "load_ratio": round(load_ratio, 2)
            }
        except (AttributeError, OSError):
            load_info = {"error": "platform_not_supported"}
        
        return {
            "detection_status": "success",
            "cpu": {
                "physical_cores": cpu_count,
                "frequency_mhz": cpu_freq.current if cpu_freq else None
            },
            "memory": {
                "total_gb": round(memory_gb, 1),
                "available_gb": round(available_gb, 1),
                "used_percent": round(used_percent, 1),
                "total_bytes": memory.total,
                "available_bytes": memory.available
            },
            "disk": {
                "total_gb": round(disk_total_gb, 1),
                "free_gb": round(disk_free_gb, 1),
                "used_percent": round(disk_used_percent, 1),
                "total_bytes": disk.total,
                "free_bytes": disk.free
            },
            "load": load_info,
            "timestamp": time.time()
        }
        
    except ImportError:
        return {
            "detection_status": "missing_dependency",
            "error": "psutil not installed",
            "install_command": "uv add psutil"
        }
    except Exception as e:
        return {
            "detection_status": "error",
            "error": str(e)
        }


@tool
def scan_genome_files(genome_id: Optional[str] = None) -> Dict[str, Any]:
    """æ‰«æå¯ç”¨çš„å‚è€ƒåŸºå› ç»„é…ç½®ï¼Œè¿”å›åŸºå› ç»„åˆ—è¡¨å’Œæ–‡ä»¶çŠ¶æ€
    
    Args:
        genome_id: å¯é€‰çš„ç‰¹å®šåŸºå› ç»„IDï¼Œç”¨äºé‡ç‚¹å…³æ³¨
    """
    config = get_tools_config()
    genomes_file = config.genomes_config_path
    
    if not genomes_file.exists():
        result = {"detection_status": "no_config_file"}
    else:
        try:
            with open(genomes_file, 'r', encoding='utf-8') as f:
                genomes_data = json.load(f)
            
            if not genomes_data:
                result = {"detection_status": "empty_config"}
            else:
                # æ£€æŸ¥æ¯ä¸ªåŸºå› ç»„çš„æ–‡ä»¶çŠ¶æ€
                genome_status = {}
                for genome_id_key, info in genomes_data.items():
                    fasta_path = info.get('fasta_path', '')
                    gtf_path = info.get('gtf_path', '')
                    
                    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
                    fasta_exists = bool(fasta_path and Path(fasta_path).exists())
                    gtf_exists = bool(gtf_path and Path(gtf_path).exists())
                    
                    # æ£€æŸ¥ç´¢å¼•çŠ¶æ€
                    star_index_exists = False
                    hisat2_index_exists = False
                    
                    if fasta_exists:
                        # STARç´¢å¼•æ£€æŸ¥
                        star_index_dir = config.get_star_index_dir(Path(fasta_path))
                        if star_index_dir.exists():
                            star_index_files = list(star_index_dir.iterdir())
                            star_index_exists = len(star_index_files) > 0
                        
                        # HISAT2ç´¢å¼•æ£€æŸ¥
                        hisat2_index_dir = config.get_hisat2_index_dir(Path(fasta_path))
                        if hisat2_index_dir.exists():
                            ht2_files = list(hisat2_index_dir.glob("*.ht2"))
                            hisat2_index_exists = len(ht2_files) > 0
                    
                    genome_status[genome_id_key] = {
                        "species": info.get('species', ''),
                        "version": info.get('version', ''),
                        "fasta_url": info.get('fasta_url', ''),
                        "gtf_url": info.get('gtf_url', ''),
                        "fasta_path": fasta_path,
                        "gtf_path": gtf_path,
                        "fasta_exists": fasta_exists,
                        "gtf_exists": gtf_exists,
                        "complete": fasta_exists and gtf_exists,
                        "star_index_exists": star_index_exists,
                        "hisat2_index_exists": hisat2_index_exists
                    }
                
                result = {
                    "detection_status": "success",
                    "total_genomes": len(genomes_data),
                    "available_genomes": len([g for g in genome_status.values() if g["complete"]]),
                    "genomes": genome_status,
                    "config_path": str(genomes_file)
                }
        except Exception as e:
            result = {
                "detection_status": "error",
                "error": str(e)
            }
    
    # å¦‚æœæŒ‡å®šäº†ç‰¹å®šåŸºå› ç»„ï¼Œæ·»åŠ æ ‡è®°
    if genome_id:
        result["requested_genome"] = genome_id
    
    return result


@tool
def get_project_overview() -> Dict[str, Any]:
    """è·å–é¡¹ç›®æ•´ä½“çŠ¶æ€æ¦‚è§ˆï¼ŒåŒ…æ‹¬æ•°æ®ã€åŸºå› ç»„ã€ç³»ç»Ÿèµ„æºå’Œåˆ†æå†å²"""
    # ä½¿ç”¨ BaseTool.invoke ä»¥é¿å…åœ¨å·¥å…·å†…éƒ¨ç›¸äº’è°ƒç”¨äº§ç”Ÿå¼ƒç”¨è­¦å‘Š
    return {
        "fastq_data": scan_fastq_files.invoke({}),
        "genome_status": scan_genome_files.invoke({}),
        "system_resources": scan_system_resources.invoke({}),
        "analysis_history": list_analysis_history.invoke({}),
        "overview_timestamp": time.time()
    }


@tool
def list_analysis_history() -> Dict[str, Any]:
    """è·å–å†å²åˆ†æè®°å½•ï¼Œè¿”å›åˆ†ææ—¶é—´ã€é…ç½®å’Œç»“æœä¿¡æ¯"""
    config = get_tools_config()
    reports_dir = config.reports_dir
    
    if not reports_dir.exists():
        return {
            "detection_status": "no_history",
            "total_analyses": 0,
            "analyses": []
        }
    
    # æ‰«ææ—¶é—´æˆ³æ ¼å¼çš„å½’æ¡£æ–‡ä»¶å¤¹
    analyses = []
    for item in reports_dir.iterdir():
        if item.is_dir() and not item.name.startswith('.') and item.name != "latest":
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ—¶é—´æˆ³æ ¼å¼ (YYYYMMDD_HHMMSS)
            if len(item.name) == 15 and item.name[8] == '_':
                try:
                    timestamp = datetime.strptime(item.name, "%Y%m%d_%H%M%S")
                    
                    # æ£€æŸ¥å½’æ¡£å†…å®¹
                    analysis_info = {
                        "timestamp_str": item.name,
                        "timestamp": timestamp.timestamp(),
                        "path": str(item),
                        "files": {}
                    }
                    
                    # æ£€æŸ¥å„ç±»æ–‡ä»¶
                    for file_name in ["analysis_report.json", "analysis_summary.md", 
                                    "runtime_config.json", "execution_log.txt"]:
                        file_path = item / file_name
                        if file_path.exists():
                            analysis_info["files"][file_name] = {
                                "exists": True,
                                "size_bytes": file_path.stat().st_size
                            }
                        else:
                            analysis_info["files"][file_name] = {"exists": False}
                    
                    # å°è¯•è¯»å–é…ç½®ä¿¡æ¯
                    runtime_config = item / "runtime_config.json"
                    if runtime_config.exists():
                        try:
                            with open(runtime_config, 'r', encoding='utf-8') as f:
                                config_data = json.load(f)
                            analysis_info["config"] = config_data.get("nextflow_params", {})
                        except Exception:
                            analysis_info["config"] = {}
                    
                    # è®¡ç®—æ€»å¤§å°
                    total_size = sum(f.stat().st_size for f in item.rglob("*") if f.is_file())
                    analysis_info["total_size_bytes"] = total_size
                    
                    analyses.append(analysis_info)
                except ValueError:
                    continue
    
    # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰
    analyses.sort(key=lambda x: x["timestamp"], reverse=True)
    
    return {
        "detection_status": "success",
        "total_analyses": len(analyses),
        "latest_analysis": analyses[0] if analyses else None,
        "analyses": analyses
    }


@tool
def check_tool_availability(tool_name: str) -> Dict[str, Any]:
    """æ£€æµ‹ç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·çš„å¯ç”¨æ€§
    
    Args:
        tool_name: å·¥å…·åç§° (fastp, star, hisat2, featurecounts)
    """
    tool_configs = {
        "fastp": ("qc_env", ["fastp", "--version"]),
        "star": ("align_env", ["STAR", "--version"]), 
        "hisat2": ("align_env", ["hisat2", "--version"]),
        "featurecounts": ("quant_env", ["featureCounts", "-v"])
    }
    
    if tool_name.lower() not in tool_configs:
        return {
            "tool_name": tool_name,
            "error": f"æœªçŸ¥å·¥å…·: {tool_name}",
            "available_tools": list(tool_configs.keys()),
            "available": False
        }
    
    env_name, cmd = tool_configs[tool_name.lower()]
    
    # æ‰§è¡Œå·¥å…·æ£€æµ‹
    detection_data = {
        "tool_name": tool_name,
        "environment": env_name,
        "command": cmd,
        "timestamp": time.time()
    }
    
    try:
        # æ‰§è¡Œç‰ˆæœ¬æ£€æµ‹å‘½ä»¤
        full_cmd = ['micromamba', 'run', '-n', env_name] + cmd
        result = subprocess.run(full_cmd, capture_output=True, text=True, timeout=15)
        
        detection_data.update({
            "command_executed": True,
            "return_code": result.returncode,
            "stdout": result.stdout,
            "stderr": result.stderr,
            "available": result.returncode == 0
        })
            
    except subprocess.TimeoutExpired:
        detection_data.update({
            "command_executed": False,
            "error": "timeout",
            "timeout_seconds": 15,
            "available": False
        })
    except FileNotFoundError:
        detection_data.update({
            "command_executed": False,
            "error": "micromamba_not_found",
            "available": False
        })
    except Exception as e:
        detection_data.update({
            "command_executed": False,
            "error": str(e),
            "available": False
        })
    
    return detection_data


@tool
def add_genome_config(user_input: str) -> Dict[str, Any]:
    """æ™ºèƒ½è§£æç”¨æˆ·è¾“å…¥å¹¶æ·»åŠ åŸºå› ç»„é…ç½®
    
    Args:
        user_input: åŒ…å«åŸºå› ç»„ä¿¡æ¯å’ŒURLçš„ç”¨æˆ·è¾“å…¥
    """
    try:
        if not user_input.strip():
            return {
                "success": False,
                "error": "è¯·æä¾›åŸºå› ç»„ä¿¡æ¯ï¼ŒåŒ…å«FASTAå’ŒGTFæ–‡ä»¶çš„URL"
            }
        
        # ä½¿ç”¨LLMè§£æç”¨æˆ·è¾“å…¥
        from .core import get_shared_llm
        
        llm = get_shared_llm()
        
        system_prompt = """ä½ æ˜¯åŸºå› ç»„ä¿¡æ¯è§£æä¸“å®¶ã€‚ç”¨æˆ·ä¼šæä¾›åŒ…å«FASTAå’ŒGTFæ–‡ä»¶URLçš„æ–‡æœ¬ï¼Œä½ éœ€è¦æå–ä¿¡æ¯å¹¶è¿”å›JSONæ ¼å¼ï¼š

{
  "genome_id": "åŸºå› ç»„æ ‡è¯†ç¬¦(å¦‚hg38,mm39,ce11)",
  "species": "ç‰©ç§åç§°(å¦‚human,mouse,caenorhabditis_elegans)", 
  "version": "ç‰ˆæœ¬å·(é€šå¸¸ä¸genome_idç›¸åŒ)",
  "fasta_url": "FASTAæ–‡ä»¶å®Œæ•´URL",
  "gtf_url": "GTFæ–‡ä»¶å®Œæ•´URL"
}

è¦æ±‚ï¼š
1. ä»URLä¸­æ™ºèƒ½è¯†åˆ«åŸºå› ç»„ç‰ˆæœ¬å’Œç‰©ç§
2. å¸¸è§æ˜ å°„ï¼šhg->human, mm->mouse, ce->caenorhabditis_elegans, dm->drosophila, rn->rat
3. å¦‚æœæ— æ³•ç¡®å®šï¼Œæ ¹æ®ç”Ÿç‰©ä¿¡æ¯å­¦å¸¸è¯†åˆç†æ¨æ–­
4. åªè¿”å›æœ‰æ•ˆçš„JSONï¼Œä¸è¦å…¶ä»–è§£é‡Š"""

        # è§£æç”¨æˆ·è¾“å…¥
        response = llm.invoke(f"System: {system_prompt}\n\nHuman: è¯·è§£æï¼š{user_input}")
        parsed_content = str(response.content).strip()
        
        # æå–JSONå†…å®¹
        try:
            genome_info = json.loads(parsed_content)
        except json.JSONDecodeError:
            json_match = re.search(r'\{.*\}', parsed_content, re.DOTALL)
            if not json_match:
                return {"success": False, "error": f"LLMè§£æå¤±è´¥ï¼š{parsed_content}"}
            genome_info = json.loads(json_match.group())
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        required_fields = ['genome_id', 'species', 'version', 'fasta_url', 'gtf_url']
        missing_fields = [field for field in required_fields if not genome_info.get(field)]
        if missing_fields:
            return {"success": False, "error": f"ç¼ºå°‘å­—æ®µï¼š{missing_fields}"}
        
        # æ„å»ºé…ç½®
        genome_id = genome_info['genome_id']
        species = genome_info['species']
        version = genome_info['version']
        
        fasta_path = f"genomes/{species}/{version}/{version}.fa"
        gtf_path = f"genomes/{species}/{version}/{version}.gtf"
        
        new_genome_config = {
            "species": species,
            "version": version,
            "fasta_path": fasta_path,
            "gtf_path": gtf_path,
            "fasta_url": genome_info['fasta_url'],
            "gtf_url": genome_info['gtf_url']
        }
        
        # è¯»å–ç°æœ‰é…ç½®å¹¶æ·»åŠ 
        config = get_tools_config()
        genomes_file = config.genomes_config_path
        
        if genomes_file.exists():
            with open(genomes_file, 'r', encoding='utf-8') as f:
                genomes_data = json.load(f)
        else:
            genomes_data = {}
        
        # æ£€æŸ¥é‡å¤
        if genome_id in genomes_data:
            return {
                "success": False,
                "error": f"åŸºå› ç»„ {genome_id} å·²å­˜åœ¨",
                "existing_config": genomes_data[genome_id]
            }
        
        # ä¿å­˜é…ç½®
        genomes_data[genome_id] = new_genome_config
        config.path_manager.ensure_directory(config.settings.config_dir)
        
        with open(genomes_file, 'w', encoding='utf-8') as f:
            json.dump(genomes_data, f, indent=2, ensure_ascii=False)
        
        return {
            "success": True,
            "genome_id": genome_id,
            "config": new_genome_config,
            "message": f"æˆåŠŸæ·»åŠ åŸºå› ç»„é…ç½®ï¼š{genome_id}"
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"æ·»åŠ åŸºå› ç»„é…ç½®æ—¶å‡ºé”™ï¼š{str(e)}"
        }


@tool
def get_help() -> Dict[str, Any]:
    """è·å–ç³»ç»Ÿå¸®åŠ©ä¿¡æ¯å’ŒåŠŸèƒ½è¯´æ˜"""
    return {
        "system_name": "RNA-seqæ™ºèƒ½åˆ†æåŠ©æ‰‹",
        "current_mode": "Normalæ¨¡å¼ (é¡¹ç›®ä¿¡æ¯ä¸­å¿ƒ)",
        "core_tools": [
            "é¡¹ç›®æ¦‚è§ˆ - ä¸€é”®æŸ¥çœ‹é¡¹ç›®å®Œæ•´çŠ¶æ€å’Œå¥åº·åº¦",
            "å†å²åˆ†æè®°å½• - æµè§ˆå·²å®Œæˆçš„åˆ†æå’Œå¯å¤ç”¨é…ç½®",
            "æŸ¥çœ‹FASTQæ–‡ä»¶ - è¯¦ç»†æ‰«ææ‰€æœ‰æµ‹åºæ•°æ®æ–‡ä»¶",
            "æŸ¥çœ‹åŸºå› ç»„ä¿¡æ¯ - æ˜¾ç¤ºå¯ç”¨å‚è€ƒåŸºå› ç»„çŠ¶æ€",
            "æ·»åŠ åŸºå› ç»„é…ç½® - æ™ºèƒ½è§£æå¹¶æ·»åŠ æ–°çš„å‚è€ƒåŸºå› ç»„"
        ],
        "next_steps": [
            "é¦–æ¬¡ä½¿ç”¨å»ºè®®è¿è¡Œ 'é¡¹ç›®æ¦‚è§ˆ' äº†è§£é¡¹ç›®çŠ¶æ€",
            "è¾“å…¥ '/plan' è¿›å…¥è®¡åˆ’æ¨¡å¼è¿›è¡Œæ·±åº¦åˆ†æè§„åˆ’"
        ],
        "mode_description": "Normalæ¨¡å¼ä¸“æ³¨å¿«é€Ÿä¿¡æ¯æŸ¥çœ‹å’Œé¡¹ç›®æ¦‚è§ˆï¼ŒPlanæ¨¡å¼è´Ÿè´£æ·±åº¦æ£€æµ‹å’Œåˆ†ææ–¹æ¡ˆåˆ¶å®š"
    }


# ==================== FastPä¸“ç”¨å·¥å…·å‡½æ•° ====================

@tool
def run_nextflow_fastp(fastp_params: Dict[str, Any], sample_info: Dict[str, Any]) -> Dict[str, Any]:
    """æ‰§è¡ŒNextflow FastPè´¨é‡æ§åˆ¶æµç¨‹
    
    Args:
        fastp_params: FastPå‚æ•°å­—å…¸ï¼Œä¾‹å¦‚ {"qualified_quality_phred": 25, "length_required": 50}
        sample_info: æ ·æœ¬ä¿¡æ¯ï¼ŒåŒ…å«sample_groupsç­‰
    
    Returns:
        æ‰§è¡Œç»“æœå­—å…¸ï¼ŒåŒ…å«çŠ¶æ€ã€è¾“å‡ºè·¯å¾„ã€æ‰§è¡Œæ—¥å¿—ç­‰
    """
    try:
        config = get_tools_config()
        
        # éªŒè¯å¿…éœ€å‚æ•°
        if not fastp_params:
            return {
                "success": False,
                "error": "FastPå‚æ•°ä¸èƒ½ä¸ºç©º",
                "execution_time": 0
            }
        
        if not sample_info.get("sample_groups"):
            return {
                "success": False, 
                "error": "æ ·æœ¬ä¿¡æ¯ç¼ºå¤±",
                "execution_time": 0
            }
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # ç»Ÿä¸€æ•°æ®æ ¹ç›®å½•æ¥æºï¼šå§‹ç»ˆä»¥ Settings().data_dir ä¸ºå‡†ï¼Œä¸ä» sample_info è¯»å–
        base_data_path = str(config.settings.data_dir)

        # ç»“æœç›®å½•ï¼šä¼˜å…ˆä½¿ç”¨ sample_info æä¾›ï¼›å¦åˆ™æŒ‰æ—¶é—´æˆ³ç”Ÿæˆåˆ° data/results ä¸‹
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_dir = sample_info.get("results_dir") or str(config.settings.data_dir / "results" / f"fastp_{timestamp}")

        # å·¥ä½œç›®å½•ï¼šå›ºå®šæ”¾åˆ° data/tmp ä¸‹ï¼ˆåŒ…å«æ—¶é—´æˆ³/ç»“æœåï¼‰
        run_id = results_dir.split('/')[-1] if '/' in str(results_dir) else timestamp
        temp_dir = Path(base_data_path) / "tmp" / f"nextflow_fastp_{run_id}"
        temp_dir.mkdir(parents=True, exist_ok=True)
        results_dir = Path(results_dir)
        print(f"ğŸ“ è¿è¡Œç›®å½•: base={base_data_path} work={temp_dir} results={results_dir}")
        
        nextflow_params: Dict[str, Any] = {}
        for raw_key, value in (fastp_params or {}).items():
            if value is None:
                continue
            key = str(raw_key)
            if key.startswith("--"):
                key = key[2:]
            if key == "thread":
                key = "threads"
            nextflow_params[key] = value
        
        # æ·»åŠ æ ·æœ¬ç»„ä¿¡æ¯
        nextflow_params["sample_groups"] = sample_info["sample_groups"]
        
        # è®¾ç½®ç»“æœç›®å½•å’Œæ•°æ®è·¯å¾„
        nextflow_params["results_dir"] = str(results_dir)
        nextflow_params["data"] = base_data_path
        
        # åˆ›å»ºNextflowå‚æ•°æ–‡ä»¶
        params_file = temp_dir / "fastp_params.json"
        with open(params_file, 'w', encoding='utf-8') as f:
            json.dump(nextflow_params, f, indent=2, ensure_ascii=False)
        
        # æ„å»ºNextflowå‘½ä»¤ï¼ˆå…¼å®¹Dockerä¸æœ¬åœ°è·¯å¾„ï¼‰
        # 1) ä¼˜å…ˆä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ fastp.nfï¼ˆæœ¬åœ°å¼€å‘ï¼‰
        # 2) Dockeré•œåƒä¸­ fastp.nf ä½äºæ ¹è·¯å¾„ '/'ï¼ˆè§ Dockerfile COPY fastp.nf /ï¼‰
        nf_candidates = [
            config.settings.project_root / "fastp.nf",
            Path("/fastp.nf")
        ]
        nextflow_script = None
        for cand in nf_candidates:
            if cand.exists():
                nextflow_script = cand
                break
        if nextflow_script is None:
            return {
                "success": False,
                "error": "æœªæ‰¾åˆ° fastp.nf è„šæœ¬ï¼Œè¯·æ£€æŸ¥å®¹å™¨å†…æ˜¯å¦å­˜åœ¨ /fastp.nf æˆ–æœ¬åœ°é¡¹ç›®æ ¹ç›®å½•",
                "searched": [str(p) for p in nf_candidates]
            }

        cmd = [
            "nextflow", "run",
            str(nextflow_script),
            "-params-file", str(params_file),
            "-work-dir", str(temp_dir / "work"),
            "--data", str(config.settings.data_dir)
        ]
        
        print(f"ğŸš€ æ‰§è¡ŒNextflow FastPæµæ°´çº¿...")
        print(f"   å‚æ•°æ–‡ä»¶: {params_file}")
        print(f"   å·¥ä½œç›®å½•: {temp_dir / 'work'}")
        print(f"   ç»“æœç›®å½•: {results_dir}")
        
        # æ‰§è¡ŒNextflowæµæ°´çº¿
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=1800,  # 30åˆ†é’Ÿè¶…æ—¶
            cwd=config.settings.project_root
        )
        
        execution_time = time.time() - start_time
        
        if result.returncode == 0:
            # è§£æè¾“å‡ºç»“æœ
            sample_count = len(sample_info["sample_groups"])
            
            return {
                "success": True,
                "message": f"FastPè´¨æ§å®Œæˆï¼Œå¤„ç†äº†{sample_count}ä¸ªæ ·æœ¬",
                "execution_time": execution_time,
                "results_dir": str(results_dir),
                "work_dir": str(temp_dir / "work"),
                "params_file": str(params_file),
                "sample_count": sample_count,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "nextflow_params": nextflow_params
            }
        else:
            return {
                "success": False,
                "error": f"Nextflowæ‰§è¡Œå¤±è´¥ (è¿”å›ç : {result.returncode})",
                "execution_time": execution_time,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "cmd": " ".join(cmd)
            }
            
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "error": "Nextflowæ‰§è¡Œè¶…æ—¶ï¼ˆ30åˆ†é’Ÿï¼‰",
            "execution_time": time.time() - start_time
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"æ‰§è¡ŒFastPæµæ°´çº¿æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
            "execution_time": 0
        }


@tool  
def parse_fastp_results(results_directory: str) -> Dict[str, Any]:
    """è§£æFastPç»“æœæ–‡ä»¶ï¼Œæå–å®¢è§‚è´¨é‡æŒ‡æ ‡ä¾›LLMåˆ†æ
    
    Args:
        results_directory: FastPç»“æœç›®å½•è·¯å¾„
    
    Returns:
        è§£æçš„è´¨é‡æŒ‡æ ‡å­—å…¸ï¼ŒåŒ…å«å„æ ·æœ¬çš„è´¨é‡ç»Ÿè®¡ã€è¿‡æ»¤ç‡ç­‰å®¢è§‚æ•°æ®
        æ³¨æ„ï¼šæ­¤å·¥å…·ä»…æä¾›å®¢è§‚æ•°æ®åˆ†æï¼Œä¸ç”Ÿæˆä¼˜åŒ–å»ºè®®ã€‚ä¼˜åŒ–å»ºè®®ç”±LLMåŸºäºè¿™äº›æ•°æ®æ™ºèƒ½ç”Ÿæˆã€‚
    """
    try:
        results_dir = Path(results_directory)
        if not results_dir.exists():
            return {
                "success": False,
                "error": f"ç»“æœç›®å½•ä¸å­˜åœ¨: {results_directory}"
            }
        
        # æŸ¥æ‰¾æ‰€æœ‰FastP JSONæŠ¥å‘Šæ–‡ä»¶
        json_files = list(results_dir.rglob("*.fastp.json"))
        
        if not json_files:
            return {
                "success": False,
                "error": "æœªæ‰¾åˆ°FastP JSONæŠ¥å‘Šæ–‡ä»¶"
            }
        
        sample_metrics = []
        overall_stats = {
            "total_reads_before": 0,
            "total_reads_after": 0,
            "total_bases_before": 0,
            "total_bases_after": 0,
            "q20_rates": [],
            "q30_rates": [],
            "gc_contents": []
        }
        
        # è§£ææ¯ä¸ªæ ·æœ¬çš„JSONæŠ¥å‘Š
        for json_file in json_files:
            try:
                with open(json_file, 'r') as f:
                    fastp_data = json.load(f)
                
                # æå–æ ·æœ¬IDï¼ˆä»æ–‡ä»¶åï¼‰
                sample_id = json_file.stem.replace('.fastp', '')
                
                # æå–è´¨é‡æŒ‡æ ‡
                summary = fastp_data.get("summary", {})
                before_filtering = summary.get("before_filtering", {})
                after_filtering = summary.get("after_filtering", {})
                
                # è¯»å–æ•°å’Œç¢±åŸºæ•°
                reads_before = before_filtering.get("total_reads", 0)
                reads_after = after_filtering.get("total_reads", 0)
                bases_before = before_filtering.get("total_bases", 0)
                bases_after = after_filtering.get("total_bases", 0)
                
                # è´¨é‡æŒ‡æ ‡
                q20_before = before_filtering.get("q20_rate", 0)
                q20_after = after_filtering.get("q20_rate", 0)
                q30_before = before_filtering.get("q30_rate", 0)
                q30_after = after_filtering.get("q30_rate", 0)
                
                # GCå«é‡
                gc_before = before_filtering.get("gc_content", 0)
                gc_after = after_filtering.get("gc_content", 0)
                
                # è¿‡æ»¤ç‡è®¡ç®—
                read_pass_rate = reads_after / reads_before if reads_before > 0 else 0
                base_pass_rate = bases_after / bases_before if bases_before > 0 else 0
                
                # å¹³å‡é•¿åº¦
                avg_length_before = bases_before / reads_before if reads_before > 0 else 0
                avg_length_after = bases_after / reads_after if reads_after > 0 else 0
                
                sample_metric = {
                    "sample_id": sample_id,
                    "json_file": str(json_file),
                    "reads_before": reads_before,
                    "reads_after": reads_after,
                    "bases_before": bases_before,
                    "bases_after": bases_after,
                    "read_pass_rate": round(read_pass_rate, 4),
                    "base_pass_rate": round(base_pass_rate, 4),
                    "q20_before": round(q20_before, 4),
                    "q20_after": round(q20_after, 4),
                    "q30_before": round(q30_before, 4),
                    "q30_after": round(q30_after, 4),
                    "gc_content_before": round(gc_before, 4),
                    "gc_content_after": round(gc_after, 4),
                    "avg_length_before": round(avg_length_before, 1),
                    "avg_length_after": round(avg_length_after, 1),
                    "quality_improvement": {
                        "q20_improvement": round(q20_after - q20_before, 4),
                        "q30_improvement": round(q30_after - q30_before, 4)
                    }
                }
                
                sample_metrics.append(sample_metric)
                
                # ç´¯ç§¯æ€»ä½“ç»Ÿè®¡
                overall_stats["total_reads_before"] += reads_before
                overall_stats["total_reads_after"] += reads_after
                overall_stats["total_bases_before"] += bases_before
                overall_stats["total_bases_after"] += bases_after
                overall_stats["q20_rates"].append(q20_after)
                overall_stats["q30_rates"].append(q30_after)
                overall_stats["gc_contents"].append(gc_after)
                
            except Exception as e:
                sample_metrics.append({
                    "sample_id": json_file.stem.replace('.fastp', ''),
                    "json_file": str(json_file),
                    "error": f"è§£æå¤±è´¥: {str(e)}"
                })
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        total_samples = len([m for m in sample_metrics if "error" not in m])
        if total_samples > 0:
            overall_read_pass_rate = overall_stats["total_reads_after"] / overall_stats["total_reads_before"]
            overall_base_pass_rate = overall_stats["total_bases_after"] / overall_stats["total_bases_before"]
            avg_q20_rate = sum(overall_stats["q20_rates"]) / len(overall_stats["q20_rates"])
            avg_q30_rate = sum(overall_stats["q30_rates"]) / len(overall_stats["q30_rates"])
            avg_gc_content = sum(overall_stats["gc_contents"]) / len(overall_stats["gc_contents"])
        else:
            overall_read_pass_rate = 0
            overall_base_pass_rate = 0
            avg_q20_rate = 0
            avg_q30_rate = 0
            avg_gc_content = 0
        
        # è´¨é‡è¯„ä¼°ï¼ˆä»…æä¾›å®¢è§‚æŒ‡æ ‡ï¼Œä¸ç”Ÿæˆä¼˜åŒ–å»ºè®®ï¼‰
        quality_assessment = {
            "overall_quality": "good" if avg_q30_rate > 0.85 else "moderate" if avg_q30_rate > 0.7 else "poor",
            "pass_rate_status": "good" if overall_read_pass_rate > 0.8 else "moderate" if overall_read_pass_rate > 0.6 else "poor"
        }
        
        return {
            "success": True,
            "total_samples": total_samples,
            "results_directory": results_directory,
            "sample_metrics": sample_metrics,
            "overall_statistics": {
                "total_reads_before": overall_stats["total_reads_before"],
                "total_reads_after": overall_stats["total_reads_after"],
                "total_bases_before": overall_stats["total_bases_before"],
                "total_bases_after": overall_stats["total_bases_after"],
                "overall_read_pass_rate": round(overall_read_pass_rate, 4),
                "overall_base_pass_rate": round(overall_base_pass_rate, 4),
                "average_q20_rate": round(avg_q20_rate, 4),
                "average_q30_rate": round(avg_q30_rate, 4),
                "average_gc_content": round(avg_gc_content, 4)
            },
            "quality_assessment": quality_assessment
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"è§£æFastPç»“æœå¤±è´¥: {str(e)}"
        }


# ==================== STARå·¥å…·å‡½æ•° ====================

@tool
def download_genome_assets(genome_id: str, force: bool = False) -> Dict[str, Any]:
    """ä¸‹è½½æŒ‡å®šåŸºå› ç»„çš„FASTAå’ŒGTFæ–‡ä»¶
    
    Args:
        genome_id: åŸºå› ç»„æ ‡è¯†ï¼ˆå¦‚"hg38", "mm39"ï¼‰ï¼Œç”¨äºåœ¨genomes.jsonä¸­æŸ¥è¯¢ä¸‹è½½ä¿¡æ¯
        force: è‹¥ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ˜¯å¦å¼ºåˆ¶è¦†ç›–ä¸‹è½½ï¼ˆé»˜è®¤å¦ï¼‰
    
    Returns:
        Dict: ä¸‹è½½ç»“æœä¿¡æ¯
        {
            "success": bool,
            "fasta_path": str,      # ä¸‹è½½åçš„FASTAæ–‡ä»¶è·¯å¾„
            "gtf_path": str,        # ä¸‹è½½åçš„GTFæ–‡ä»¶è·¯å¾„
            "downloaded": List[str], # å®é™…ä¸‹è½½çš„æ–‡ä»¶åˆ—è¡¨
            "skipped": List[str],   # è·³è¿‡çš„æ–‡ä»¶åˆ—è¡¨
            "errors": List[str]     # é”™è¯¯ä¿¡æ¯åˆ—è¡¨
        }
    """
    try:
        tools_config = get_tools_config()
        
        # è¯»å–åŸºå› ç»„é…ç½®
        genomes_config_path = tools_config.genomes_config_path
        if not genomes_config_path.exists():
            return {
                "success": False,
                "error": f"åŸºå› ç»„é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {genomes_config_path}"
            }
        
        with open(genomes_config_path, 'r', encoding='utf-8') as f:
            genomes_config = json.load(f)
        
        if genome_id not in genomes_config:
            return {
                "success": False,
                "error": f"åŸºå› ç»„ '{genome_id}' åœ¨é…ç½®æ–‡ä»¶ä¸­ä¸å­˜åœ¨"
            }
        
        config = genomes_config[genome_id]
        species = config.get("species", "unknown")
        version = config.get("version", "unknown")
        fasta_url = config.get("fasta_url")
        gtf_url = config.get("gtf_url")
        fasta_relative = config.get("fasta_path")
        gtf_relative = config.get("gtf_path")
        
        if not all([fasta_url, gtf_url, fasta_relative, gtf_relative]):
            return {
                "success": False,
                "error": f"åŸºå› ç»„ '{genome_id}' é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘å¿…è¦çš„URLæˆ–è·¯å¾„ä¿¡æ¯"
            }
        
        # æ„å»ºç»å¯¹è·¯å¾„
        project_root = tools_config.settings.project_root
        fasta_path = project_root / fasta_relative
        gtf_path = project_root / gtf_relative
        
        downloaded = []
        skipped = []
        errors = []
        
        # åˆ›å»ºç›®å½•
        fasta_path.parent.mkdir(parents=True, exist_ok=True)
        gtf_path.parent.mkdir(parents=True, exist_ok=True)
        
        def download_file(url: str, target_path: Path, file_type: str) -> bool:
            """ä¸‹è½½å•ä¸ªæ–‡ä»¶çš„è¾…åŠ©å‡½æ•°"""
            try:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                if target_path.exists() and target_path.stat().st_size > 0 and not force:
                    skipped.append(f"{file_type}: {target_path}")
                    return True
                
                # ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶
                temp_path = target_path.with_suffix(target_path.suffix + '.part')
                
                # æ„å»ºcurlä¸‹è½½å‘½ä»¤
                cmd = [
                    "curl", "-L", "-fS", "--retry", "5", 
                    "--retry-delay", "5", "--retry-connrefused",
                    "-C", "-",  # æ–­ç‚¹ç»­ä¼ 
                    "-o", str(temp_path),
                    url
                ]
                
                print(f"ä¸‹è½½ {file_type}...")
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)  # 1å°æ—¶è¶…æ—¶
                
                if result.returncode != 0:
                    errors.append(f"{file_type}ä¸‹è½½å¤±è´¥: {result.stderr}")
                    return False
                
                # è§£å‹æ–‡ä»¶ï¼ˆå¦‚æœæ˜¯.gzæ ¼å¼ï¼‰
                if temp_path.suffix == '.gz':
                    # ä½¿ç”¨pigzä¼˜å…ˆï¼Œfallbackåˆ°gzip
                    decompress_cmd = ["pigz", "-d", "-c", str(temp_path)]
                    decompress_result = subprocess.run(decompress_cmd, capture_output=True, timeout=1800)
                    
                    if decompress_result.returncode != 0:
                        # fallbackåˆ°gzip
                        decompress_cmd = ["gzip", "-d", "-c", str(temp_path)]
                        decompress_result = subprocess.run(decompress_cmd, capture_output=True, timeout=1800)
                        
                        if decompress_result.returncode != 0:
                            errors.append(f"{file_type}è§£å‹å¤±è´¥")
                            return False
                    
                    # å†™å…¥è§£å‹å†…å®¹åˆ°æœ€ç»ˆæ–‡ä»¶
                    with open(target_path, 'wb') as f:
                        f.write(decompress_result.stdout)
                    
                    # åˆ é™¤ä¸´æ—¶å‹ç¼©æ–‡ä»¶
                    temp_path.unlink()
                else:
                    # ç›´æ¥é‡å‘½å
                    temp_path.rename(target_path)
                
                downloaded.append(f"{file_type}: {target_path}")
                return True
                
            except subprocess.TimeoutExpired:
                errors.append(f"{file_type}ä¸‹è½½è¶…æ—¶")
                return False
            except Exception as e:
                errors.append(f"{file_type}ä¸‹è½½å¼‚å¸¸: {str(e)}")
                return False
        
        # å¹¶è¡Œä¸‹è½½FASTAå’ŒGTFï¼ˆä½¿ç”¨ç®€å•çš„ä¸²è¡Œå®ç°ï¼Œå¯ä»¥åç»­ä¼˜åŒ–ä¸ºçœŸæ­£çš„å¹¶è¡Œï¼‰
        fasta_success = download_file(fasta_url, fasta_path, "FASTA")
        gtf_success = download_file(gtf_url, gtf_path, "GTF")
        
        success = fasta_success and gtf_success
        
        return {
            "success": success,
            "fasta_path": str(fasta_path),
            "gtf_path": str(gtf_path),
            "downloaded": downloaded,
            "skipped": skipped,
            "errors": errors
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"ä¸‹è½½åŸºå› ç»„èµ„æºå¤±è´¥: {str(e)}"
        }


@tool
def build_star_index(genome_id: str, sjdb_overhang: Optional[int] = None, 
                    runThreadN: Optional[int] = None, force_rebuild: bool = False) -> Dict[str, Any]:
    """æ„å»ºSTARåŸºå› ç»„ç´¢å¼•
    
    Args:
        genome_id: åŸºå› ç»„æ ‡è¯†ï¼Œç”¨äºå®šä½FASTAå’ŒGTFæ–‡ä»¶
        sjdb_overhang: å¯é€‰çš„å‰ªæ¥ä½ç‚¹overhangå‚æ•°ï¼ˆé€šå¸¸ä¸ºreadsé•¿åº¦-1ï¼‰
        runThreadN: æ„å»ºç´¢å¼•çš„çº¿ç¨‹æ•°ï¼Œé»˜è®¤ä½¿ç”¨DEFAULT_STAR_PARAMSä¸­çš„å€¼
        force_rebuild: è‹¥ç´¢å¼•ç›®å½•å·²å­˜åœ¨æ˜¯å¦å¼ºåˆ¶é‡å»º
    
    Returns:
        Dict: ç´¢å¼•æ„å»ºç»“æœ
        {
            "success": bool,
            "index_dir": str,      # ç´¢å¼•ç›®å½•è·¯å¾„
            "stdout": str,         # æ„å»ºè¿‡ç¨‹è¾“å‡º
            "stderr": str,         # é”™è¯¯è¾“å‡º
            "skipped": bool        # æ˜¯å¦è·³è¿‡æ„å»º
        }
    """
    try:
        from .config.default_tool_params import DEFAULT_STAR_PARAMS
        
        tools_config = get_tools_config()
        
        # è¯»å–åŸºå› ç»„é…ç½®è·å–æ–‡ä»¶è·¯å¾„
        genomes_config_path = tools_config.genomes_config_path
        if not genomes_config_path.exists():
            return {
                "success": False,
                "error": f"åŸºå› ç»„é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {genomes_config_path}"
            }
        
        with open(genomes_config_path, 'r', encoding='utf-8') as f:
            genomes_config = json.load(f)
        
        if genome_id not in genomes_config:
            return {
                "success": False,
                "error": f"åŸºå› ç»„ '{genome_id}' åœ¨é…ç½®æ–‡ä»¶ä¸­ä¸å­˜åœ¨"
            }
        
        config = genomes_config[genome_id]
        fasta_relative = config.get("fasta_path")
        gtf_relative = config.get("gtf_path")
        
        if not fasta_relative or not gtf_relative:
            return {
                "success": False,
                "error": f"åŸºå› ç»„ '{genome_id}' é…ç½®ä¸­ç¼ºå°‘fasta_pathæˆ–gtf_path"
            }
        
        # æ„å»ºç»å¯¹è·¯å¾„
        project_root = tools_config.settings.project_root
        fasta_path = project_root / fasta_relative
        gtf_path = project_root / gtf_relative
        
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not fasta_path.exists():
            return {
                "success": False,
                "error": f"FASTAæ–‡ä»¶ä¸å­˜åœ¨: {fasta_path}"
            }
        
        if not gtf_path.exists():
            return {
                "success": False,
                "error": f"GTFæ–‡ä»¶ä¸å­˜åœ¨: {gtf_path}"
            }
        
        # è·å–STARç´¢å¼•ç›®å½•
        index_dir = tools_config.get_star_index_dir(fasta_path)
        
        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
        if index_dir.exists() and not force_rebuild:
            # ç®€å•æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®ç´¢å¼•æ–‡ä»¶
            key_files = ["SA", "SAindex", "Genome"]
            if all((index_dir / f).exists() for f in key_files):
                return {
                    "success": True,
                    "index_dir": str(index_dir),
                    "skipped": True,
                    "message": "STARç´¢å¼•å·²å­˜åœ¨ï¼Œè·³è¿‡æ„å»º"
                }
        
        # åˆ›å»ºç´¢å¼•ç›®å½•
        index_dir.mkdir(parents=True, exist_ok=True)
        
        # å‡†å¤‡æ„å»ºå‚æ•°
        if runThreadN is None:
            runThreadN = DEFAULT_STAR_PARAMS.get("runThreadN", 8)
        
        # è¿™é‡Œåº”è¯¥é€šè¿‡Nextflowæ‰§è¡Œç´¢å¼•æ„å»ºï¼Œä½†ä¸ºäº†ç®€åŒ–å®ç°ï¼Œæš‚æ—¶ä½¿ç”¨ç›´æ¥è°ƒç”¨
        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„build_index.nfè„šæœ¬
        cmd = [
            "STAR",
            "--runMode", "genomeGenerate",
            "--genomeDir", str(index_dir),
            "--genomeFastaFiles", str(fasta_path),
            "--sjdbGTFfile", str(gtf_path),
            "--runThreadN", str(runThreadN)
        ]
        
        if sjdb_overhang is not None:
            cmd.extend(["--sjdbOverhang", str(sjdb_overhang)])
        
        print(f"æ„å»ºSTARç´¢å¼•: {' '.join(cmd)}")
        
        # æ‰§è¡Œæ„å»ºå‘½ä»¤
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=7200)  # 2å°æ—¶è¶…æ—¶
        
        if result.returncode == 0:
            return {
                "success": True,
                "index_dir": str(index_dir),
                "stdout": result.stdout,
                "stderr": result.stderr,
                "skipped": False
            }
        else:
            return {
                "success": False,
                "error": f"STARç´¢å¼•æ„å»ºå¤±è´¥: {result.stderr}",
                "stdout": result.stdout,
                "stderr": result.stderr,
                "return_code": result.returncode
            }
        
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "error": "STARç´¢å¼•æ„å»ºè¶…æ—¶"
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"æ„å»ºSTARç´¢å¼•å¤±è´¥: {str(e)}"
        }


@tool
def run_nextflow_star(star_params: Dict[str, Any], fastp_results: Dict[str, Any], 
                     genome_info: Dict[str, Any]) -> Dict[str, Any]:
    """æ‰§è¡ŒNextflow STARæ¯”å¯¹æµç¨‹
    
    Args:
        star_params: STARå‚æ•°å­—å…¸
        fastp_results: FastPç»“æœæ•°æ®ï¼ŒåŒ…å«ä¿®å‰ªåFASTQæ–‡ä»¶ä¿¡æ¯
        genome_info: åŸºå› ç»„ä¿¡æ¯ï¼ŒåŒ…å«ç´¢å¼•è·¯å¾„ç­‰
    
    Returns:
        Dict: STARæ‰§è¡Œç»“æœ
        {
            "success": bool,
            "results_dir": str,       # ç»“æœç›®å½•
            "work_dir": str,          # å·¥ä½œç›®å½•
            "params_file": str,       # å‚æ•°æ–‡ä»¶è·¯å¾„
            "sample_count": int,      # å¤„ç†æ ·æœ¬æ•°
            "stdout": str,            # æ‰§è¡Œè¾“å‡º
            "stderr": str             # é”™è¯¯è¾“å‡º
        }
    """
    try:
        tools_config = get_tools_config()
        
        # æ£€æŸ¥FastPç»“æœ
        if not fastp_results.get("success"):
            return {
                "success": False,
                "error": "FastPç»“æœæ— æ•ˆï¼Œæ— æ³•æ‰§è¡ŒSTARæ¯”å¯¹"
            }
        
        fastp_results_dir = fastp_results.get("results_dir")
        if not fastp_results_dir:
            return {
                "success": False,
                "error": "FastPç»“æœç›®å½•ç¼ºå¤±"
            }
        
        # ç”Ÿæˆç»“æœç›®å½•å’Œæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_dir = tools_config.results_dir / f"star_{timestamp}"
        work_dir = results_dir / "work"
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¸…ç†å‚æ•°ï¼šç§»é™¤Noneå€¼å’Œç‰¹æ®Šå­—æ®µ
        cleaned_params = {}
        for key, value in star_params.items():
            if value is not None and key not in ["star_cpus"]:
                # ç§»é™¤--å‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                clean_key = key.lstrip('-')
                cleaned_params[clean_key] = value
        
        # å‡†å¤‡Nextflowå‚æ•°æ–‡ä»¶
        nf_params = {
            "from_fastp_dir": fastp_results_dir,
            "star_index": genome_info.get("star_index_dir", ""),
            "results_dir": str(results_dir),
            "work_dir": str(work_dir),
            **cleaned_params
        }
        
        # å†™å…¥å‚æ•°æ–‡ä»¶
        params_file = results_dir / "star_params.json"
        with open(params_file, 'w', encoding='utf-8') as f:
            json.dump(nf_params, f, indent=2, ensure_ascii=False)
        
        # æ„å»ºNextflowå‘½ä»¤ï¼ˆè¿™é‡Œä½¿ç”¨ç®€åŒ–çš„æ¨¡æ‹Ÿå®ç°ï¼‰
        # åœ¨çœŸå®å®ç°ä¸­åº”è¯¥è°ƒç”¨ç‹¬ç«‹çš„align.nfè„šæœ¬
        print(f"æ‰§è¡ŒSTARæ¯”å¯¹ - å‚æ•°æ–‡ä»¶: {params_file}")
        print(f"FastPè¾“å…¥ç›®å½•: {fastp_results_dir}")
        print(f"STARç´¢å¼•: {genome_info.get('star_index_dir', 'N/A')}")
        
        # æ¨¡æ‹ŸæˆåŠŸæ‰§è¡Œ
        time.sleep(2)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        
        # åˆ›å»ºæ¨¡æ‹Ÿç»“æœç›®å½•ç»“æ„
        star_output_dir = results_dir / "star"
        star_output_dir.mkdir(exist_ok=True)
        
        # æ¨¡æ‹Ÿæ ·æœ¬å¤„ç†ç»“æœ
        sample_count = len(fastp_results.get("per_sample_outputs", []))
        
        # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ›å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„ä¿¡æ¯ï¼ˆåŸºäºSTARè¾“å‡ºè§„èŒƒï¼‰
        per_sample_outputs = []
        for i, fastp_sample in enumerate(fastp_results.get("per_sample_outputs", [])):
            sample_id = fastp_sample.get("sample_id", f"sample_{i+1}")
            sample_output_dir = star_output_dir / sample_id
            sample_output_dir.mkdir(exist_ok=True)
            
            # æ ¹æ®STARæ ‡å‡†è¾“å‡ºæ–‡ä»¶ç»“æ„å®šä¹‰
            sample_output = {
                "sample_id": sample_id,
                "aligned_bam": str(sample_output_dir / f"{sample_id}.Aligned.sortedByCoord.out.bam"),
                "log_final": str(sample_output_dir / "Log.final.out"), 
                "log_out": str(sample_output_dir / "Log.out"),
                "log_progress": str(sample_output_dir / "Log.progress.out"),
                "splice_junctions": str(sample_output_dir / "SJ.out.tab")
            }
            
            # å¦‚æœå¯ç”¨äº†è½¬å½•ç»„è¾“å‡º
            if cleaned_params.get("quantMode") and "TranscriptomeSAM" in str(cleaned_params.get("quantMode", "")):
                sample_output["transcriptome_bam"] = str(sample_output_dir / f"{sample_id}.Aligned.toTranscriptome.out.bam")
                
            # å¦‚æœå¯ç”¨äº†åŸºå› è®¡æ•°
            if cleaned_params.get("quantMode") and "GeneCounts" in str(cleaned_params.get("quantMode", "")):
                sample_output["gene_counts"] = str(sample_output_dir / f"{sample_id}.ReadsPerGene.out.tab")
                
            per_sample_outputs.append(sample_output)
        
        return {
            "success": True,
            "results_dir": str(results_dir),
            "work_dir": str(work_dir), 
            "params_file": str(params_file),
            "sample_count": sample_count,
            "per_sample_outputs": per_sample_outputs,
            "stdout": f"STARæ¯”å¯¹å®Œæˆï¼Œå¤„ç†äº†{sample_count}ä¸ªæ ·æœ¬",
            "stderr": ""
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"æ‰§è¡ŒSTARæ¯”å¯¹å¤±è´¥: {str(e)}"
        }


@tool
def parse_star_metrics(results_directory: str) -> Dict[str, Any]:
    """è§£æSTARæ¯”å¯¹ç»“æœæ–‡ä»¶ï¼Œæå–æ¯”å¯¹æŒ‡æ ‡
    
    Args:
        results_directory: STARç»“æœç›®å½•è·¯å¾„
    
    Returns:
        Dict: è§£æåçš„æ¯”å¯¹æŒ‡æ ‡æ•°æ®
        {
            "success": bool,
            "total_samples": int,
            "results_directory": str,
            "per_sample_metrics": List[Dict],    # æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†æŒ‡æ ‡
            "overall_statistics": Dict,          # æ€»ä½“ç»Ÿè®¡ä¿¡æ¯
            "quality_assessment": Dict           # è´¨é‡è¯„ä¼°
        }
    """
    try:
        results_path = Path(results_directory)
        if not results_path.exists():
            return {
                "success": False,
                "error": f"STARç»“æœç›®å½•ä¸å­˜åœ¨: {results_directory}"
            }
        
        sample_metrics = []
        overall_stats = {
            "total_input_reads": 0,
            "total_mapped_reads": 0,
            "total_uniquely_mapped": 0,
            "total_multi_mapped": 0,
            "mapping_rates": [],
            "unique_mapping_rates": [],
            "multi_mapping_rates": [],
            "mismatch_rates": []
        }
        
        # æŸ¥æ‰¾STARè¾“å‡ºç›®å½•
        star_dir = results_path / "star"
        if not star_dir.exists():
            return {
                "success": False,
                "error": f"STARè¾“å‡ºç›®å½•ä¸å­˜åœ¨: {star_dir}"
            }
        
        # éå†æ ·æœ¬ç›®å½•ï¼ŒæŸ¥æ‰¾Log.final.outæ–‡ä»¶
        for sample_dir in star_dir.iterdir():
            if not sample_dir.is_dir():
                continue
                
            log_final_file = sample_dir / "Log.final.out"
            if not log_final_file.exists():
                sample_metrics.append({
                    "sample_id": sample_dir.name,
                    "error": "Log.final.outæ–‡ä»¶ä¸å­˜åœ¨"
                })
                continue
            
            try:
                # è§£æLog.final.outæ–‡ä»¶
                with open(log_final_file, 'r', encoding='utf-8') as f:
                    log_content = f.read()
                
                # æå–å…³é”®æŒ‡æ ‡ï¼ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼‰
                input_reads = _extract_metric(log_content, r"Number of input reads.*?(\d+)")
                uniquely_mapped = _extract_metric(log_content, r"Uniquely mapped reads number.*?(\d+)")
                multi_mapped = _extract_metric(log_content, r"Number of reads mapped to multiple loci.*?(\d+)")
                unmapped = _extract_metric(log_content, r"Number of reads unmapped.*?(\d+)")
                
                # è®¡ç®—æ¯”ç‡
                if input_reads > 0:
                    mapping_rate = (uniquely_mapped + multi_mapped) / input_reads
                    unique_mapping_rate = uniquely_mapped / input_reads
                    multi_mapping_rate = multi_mapped / input_reads
                else:
                    mapping_rate = unique_mapping_rate = multi_mapping_rate = 0
                
                # æå–mismatchç‡
                mismatch_rate = _extract_metric(log_content, r"Mismatch rate per base.*?([\d.]+)%") / 100
                
                sample_metric = {
                    "sample_id": sample_dir.name,
                    "input_reads": input_reads,
                    "uniquely_mapped": uniquely_mapped,
                    "multi_mapped": multi_mapped,
                    "unmapped": unmapped,
                    "mapping_rate": round(mapping_rate, 4),
                    "unique_mapping_rate": round(unique_mapping_rate, 4),
                    "multi_mapping_rate": round(multi_mapping_rate, 4),
                    "mismatch_rate": round(mismatch_rate, 4),
                    "log_file": str(log_final_file)
                }
                
                sample_metrics.append(sample_metric)
                
                # ç´¯åŠ åˆ°æ€»ä½“ç»Ÿè®¡
                overall_stats["total_input_reads"] += input_reads
                overall_stats["total_mapped_reads"] += (uniquely_mapped + multi_mapped)
                overall_stats["total_uniquely_mapped"] += uniquely_mapped
                overall_stats["total_multi_mapped"] += multi_mapped
                overall_stats["mapping_rates"].append(mapping_rate)
                overall_stats["unique_mapping_rates"].append(unique_mapping_rate)
                overall_stats["multi_mapping_rates"].append(multi_mapping_rate)
                overall_stats["mismatch_rates"].append(mismatch_rate)
                
            except Exception as e:
                sample_metrics.append({
                    "sample_id": sample_dir.name,
                    "log_file": str(log_final_file),
                    "error": f"è§£æå¤±è´¥: {str(e)}"
                })
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        total_samples = len([m for m in sample_metrics if "error" not in m])
        if total_samples > 0:
            overall_mapping_rate = overall_stats["total_mapped_reads"] / overall_stats["total_input_reads"]
            overall_unique_rate = overall_stats["total_uniquely_mapped"] / overall_stats["total_input_reads"]
            overall_multi_rate = overall_stats["total_multi_mapped"] / overall_stats["total_input_reads"]
            avg_mismatch_rate = sum(overall_stats["mismatch_rates"]) / len(overall_stats["mismatch_rates"])
        else:
            overall_mapping_rate = overall_unique_rate = overall_multi_rate = avg_mismatch_rate = 0
        
        # è´¨é‡è¯„ä¼°
        quality_assessment = {
            "overall_quality": "good" if overall_mapping_rate > 0.85 else "moderate" if overall_mapping_rate > 0.7 else "poor",
            "unique_mapping_status": "good" if overall_unique_rate > 0.8 else "moderate" if overall_unique_rate > 0.6 else "poor",
            "multi_mapping_status": "good" if overall_multi_rate < 0.2 else "moderate" if overall_multi_rate < 0.3 else "high"
        }
        
        return {
            "success": True,
            "total_samples": total_samples,
            "results_directory": results_directory,
            "sample_metrics": sample_metrics,
            "overall_statistics": {
                "total_input_reads": overall_stats["total_input_reads"],
                "total_mapped_reads": overall_stats["total_mapped_reads"],
                "total_uniquely_mapped": overall_stats["total_uniquely_mapped"],
                "total_multi_mapped": overall_stats["total_multi_mapped"],
                "overall_mapping_rate": round(overall_mapping_rate, 4),
                "overall_unique_mapping_rate": round(overall_unique_rate, 4),
                "overall_multi_mapping_rate": round(overall_multi_rate, 4),
                "average_mismatch_rate": round(avg_mismatch_rate, 4)
            },
            "quality_assessment": quality_assessment
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"è§£æSTARç»“æœå¤±è´¥: {str(e)}"
        }

def _extract_metric(text: str, pattern: str) -> float:
    """ä»æ–‡æœ¬ä¸­æå–æ•°å€¼æŒ‡æ ‡çš„è¾…åŠ©å‡½æ•°"""
    import re
    match = re.search(pattern, text)
    if match:
        try:
            return float(match.group(1))
        except ValueError:
            return 0.0
    return 0.0
