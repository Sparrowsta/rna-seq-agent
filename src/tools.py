"""
RNA-seqæ™ºèƒ½åˆ†æåŠ©æ‰‹å·¥å…·æ¨¡å—
æä¾›FASTQæ–‡ä»¶æŸ¥è¯¢ã€åŸºå› ç»„ç®¡ç†ã€ç”¨æˆ·æ„å›¾æ”¶é›†ç­‰æ ¸å¿ƒåŠŸèƒ½

é‡æ„è¯´æ˜:
- åˆå¹¶decorators.pyä¸­çš„ä»£ç ï¼Œç»Ÿä¸€ç®¡ç†æ‰€æœ‰å·¥å…·å‡½æ•°
- æŒ‰åŠŸèƒ½æ¨¡å—é‡æ–°ç»„ç»‡ï¼Œè®©è¾…åŠ©å‡½æ•°å’Œä¸»è¦å‡½æ•°ç»„ç»‡åœ¨ä¸€èµ·
- ä¿æŒåŒæ¨¡å¼å·¥å…·æ¶æ„ä¸å˜
- ä½¿ç”¨é…ç½®ç³»ç»Ÿæ›¿ä»£ç¡¬ç¼–ç è·¯å¾„
"""

import os
import json
import glob
import re
import time
import subprocess
from datetime import datetime
from pathlib import Path
from functools import wraps
from typing import Dict, List, Any, Union, Callable, Optional

# å¯¼å…¥é…ç½®ç³»ç»Ÿ
from .config import get_tools_config


# ==================== è£…é¥°å™¨å’Œè¾…åŠ©ç³»ç»Ÿ ====================

def pure_detection(error_prefix: str = "æ£€æµ‹"):
    """çº¯æ£€æµ‹è£…é¥°å™¨ - ç»Ÿä¸€æ£€æµ‹å·¥å…·çš„è¿”å›æ ¼å¼å’Œé”™è¯¯å¤„ç†
    
    Args:
        error_prefix: é”™è¯¯æ¶ˆæ¯å‰ç¼€
    
    Returns:
        æ ‡å‡†åŒ–çš„æ£€æµ‹ç»“æœ: {"result": str, "query_results": dict, "config_updates": {}}
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            try:
                result = func(*args, **kwargs)
                
                # å¦‚æœå‡½æ•°è¿”å›çš„å·²ç»æ˜¯æ ‡å‡†æ ¼å¼
                if isinstance(result, dict) and "query_results" in result:
                    return result
                
                # å¦‚æœå‡½æ•°è¿”å›çš„æ˜¯æŸ¥è¯¢æ•°æ®ï¼Œè‡ªåŠ¨åŒ…è£…
                if isinstance(result, dict):
                    return {
                        "result": f"âœ… {error_prefix}å®Œæˆ",
                        "query_results": result,
                        "config_updates": {}  # æ£€æµ‹ä¸ç”Ÿæˆé…ç½®
                    }
                
                return {
                    "result": str(result),
                    "query_results": {},
                    "config_updates": {}
                }
                
            except Exception as e:
                return {
                    "result": f"âŒ {error_prefix}æ—¶å‡ºé”™: {str(e)}",
                    "query_results": {"status": "error", "error": str(e)},
                    "config_updates": {}
                }
        return wrapper
    return decorator


def tool_detection(tool_name: str, environment: str, version_cmd: List[str]):
    """çº¯å·¥å…·æ£€æµ‹è£…é¥°å™¨ - åªæ”¶é›†ç‰ˆæœ¬ä¿¡æ¯ï¼Œä¸åšå¯ç”¨æ€§åˆ¤æ–­
    
    Args:
        tool_name: å·¥å…·åç§°
        environment: micromambaç¯å¢ƒå
        version_cmd: ç‰ˆæœ¬æ£€æµ‹å‘½ä»¤åˆ—è¡¨
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            detection_data = {
                "tool_name": tool_name,
                "environment": environment,
                "command": version_cmd
            }
            
            try:
                # æ‰§è¡Œç‰ˆæœ¬æ£€æµ‹å‘½ä»¤
                cmd = ['micromamba', 'run', '-n', environment] + version_cmd
                conda_result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)
                
                detection_data.update({
                    "command_executed": True,
                    "return_code": conda_result.returncode,
                    "stdout": conda_result.stdout,
                    "stderr": conda_result.stderr
                })
                    
            except subprocess.TimeoutExpired:
                detection_data.update({
                    "command_executed": False,
                    "error": "timeout",
                    "timeout_seconds": 15
                })
            except FileNotFoundError:
                detection_data.update({
                    "command_executed": False,
                    "error": "micromamba_not_found"
                })
            except Exception as e:
                detection_data.update({
                    "command_executed": False,
                    "error": str(e)
                })
            
            return {
                "result": f"ğŸ”§ {tool_name}æ£€æµ‹å®Œæˆ",
                "query_results": detection_data,
                "config_updates": {}
            }
        return wrapper
    return decorator


def get_system_info() -> Dict[str, Any]:
    """è·å–ç³»ç»Ÿç¡¬ä»¶ä¿¡æ¯"""
    try:
        import psutil
        
        # CPUä¿¡æ¯
        cpu_count = psutil.cpu_count(logical=False) or 1
        logical_count = psutil.cpu_count(logical=True) or 1
        cpu_freq = psutil.cpu_freq()
        
        # å†…å­˜ä¿¡æ¯
        memory = psutil.virtual_memory()
        memory_gb = memory.total / 1024**3
        available_gb = memory.available / 1024**3
        used_percent = memory.percent
        
        # ç£ç›˜ä¿¡æ¯
        disk = psutil.disk_usage('.')
        disk_total_gb = disk.total / 1024**3
        disk_free_gb = disk.free / 1024**3
        disk_used_percent = (disk.used / disk.total) * 100
        
        # ç³»ç»Ÿè´Ÿè½½
        load_info = {}
        try:
            load_avg = psutil.getloadavg()
            load_ratio = load_avg[0] / max(logical_count, 1)
            load_info = {
                "load_1min": round(load_avg[0], 2),
                "load_5min": round(load_avg[1], 2),
                "load_15min": round(load_avg[2], 2),
                "load_ratio": round(load_ratio, 2)
            }
        except (AttributeError, OSError):
            load_info = {"error": "platform_not_supported"}
        
        return {
            "detection_status": "success",
            "cpu": {
                "total_cpus": cpu_count,  # ä½¿ç”¨total_cpusåŒ¹é…PrepareèŠ‚ç‚¹æç¤ºè¯
                "frequency_mhz": cpu_freq.current if cpu_freq else None
            },
            "memory": {
                "total_gb": round(memory_gb, 1),
                "available_gb": round(available_gb, 1),
                "used_percent": round(used_percent, 1),
                "total_bytes": memory.total,
                "available_bytes": memory.available
            },
            "disk": {
                "total_gb": round(disk_total_gb, 1),
                "free_gb": round(disk_free_gb, 1),
                "used_percent": round(disk_used_percent, 1),
                "total_bytes": disk.total,
                "free_bytes": disk.free
            },
            "load": load_info
        }
        
    except ImportError:
        return {
            "detection_status": "missing_dependency",
            "error": "psutil not installed",
            "install_command": "uv add psutil"
        }
    except Exception as e:
        return {
            "detection_status": "error",
            "error": str(e)
        }


# ==================== FASTQæ–‡ä»¶å¤„ç†æ¨¡å— ====================

def _scan_fastq_files() -> Dict[str, Any]:
    """çº¯FASTQæ–‡ä»¶æ‰«æ - æ’é™¤ä¸­é—´æ–‡ä»¶ç›®å½•"""
    # è·å–é…ç½®
    config = get_tools_config()
    project_root = config.project_root
    fastq_extensions = ["*.fastq", "*.fastq.gz", "*.fq", "*.fq.gz"]
    
    # å®šä¹‰è¦æ’é™¤çš„ç›®å½•ï¼ˆä¸­é—´æ–‡ä»¶å’Œç¼“å­˜ç›®å½•ï¼‰
    exclude_directories = {
        "work", "tmp", "temp", "results", "output", 
        ".nextflow", "logs", "cache", "__pycache__"
    }
    
    # æ‰«ææ‰€æœ‰FASTQæ–‡ä»¶ï¼Œä½†æ’é™¤ç‰¹å®šç›®å½•
    all_fastq_files = []
    for ext in fastq_extensions:
        for file_path in project_root.rglob(ext):
            # æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦åŒ…å«æ’é™¤çš„ç›®å½•
            if any(excluded in file_path.parts for excluded in exclude_directories):
                continue
            all_fastq_files.append(file_path)
    
    # æ”¶é›†æ¯ä¸ªæ–‡ä»¶çš„åŸå§‹ä¿¡æ¯
    file_list = []
    for file_path in all_fastq_files:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦çœŸå®å­˜åœ¨ä¸”å¯è®¿é—®
        if not file_path.exists() or not file_path.is_file():
            continue
            
        try:
            file_size = file_path.stat().st_size
            file_info = {
                "filename": file_path.name,
                "full_path": str(file_path),
                "directory": str(file_path.parent),
                "size_bytes": file_size,
                "size_mb": round(file_size / 1024 / 1024, 2),
                "extension": file_path.suffix
            }
            file_list.append(file_info)
        except Exception:
            # è·³è¿‡æ— æ³•è®¿é—®çš„æ–‡ä»¶
            continue
    
    # æŒ‰æ–‡ä»¶åæ’åº
    file_list.sort(key=lambda x: x["filename"])
    
    return {
        "detection_status": "success",
        "total_files_found": len(file_list),
        "fastq_files": file_list,
        "scan_timestamp": time.time()
    }


def _extract_sample_names(file_list: list) -> list:
    """ä»æ–‡ä»¶åˆ—è¡¨æå–æ ·æœ¬åç§°åˆ—è¡¨"""
    sample_names = set()
    for file_info in file_list:
        filename = file_info.get("filename", "")
        sample_name, _ = _extract_sample_name_and_type(filename)
        sample_names.add(sample_name)
    return list(sample_names)


def _extract_sample_name_and_type(filename: str) -> tuple:
    """æå–æ ·æœ¬åç§°å’Œè¯»å–ç±»å‹"""
    if "_1." in filename or "_R1" in filename:
        sample_name = filename.split("_1.")[0].split("_R1")[0]
        read_type = "R1"
    elif "_2." in filename or "_R2" in filename:
        sample_name = filename.split("_2.")[0].split("_R2")[0]
        read_type = "R2"
    else:
        sample_name = filename.split(".")[0]
        read_type = "single"
    
    return sample_name, read_type


def _analyze_sample_pairing(raw_data: dict) -> dict:
    """åˆ†ææ ·æœ¬é…å¯¹å…³ç³»ï¼ˆç”¨äºdetailedæ¨¡å¼ï¼‰"""
    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„é…å¯¹åˆ†æé€»è¾‘
    # ç›®å‰è¿”å›åŸå§‹æ•°æ®ï¼Œåç»­å¯ä»¥æ‰©å±•
    return raw_data


def _format_fastq_report(raw_data: dict, depth: str = "basic") -> str:
    """æ ¼å¼åŒ–FASTQæ•°æ®ä¸ºç”¨æˆ·å‹å¥½çš„æŠ¥å‘Š"""
    file_count = raw_data.get("total_files_found", 0)
    file_list = raw_data.get("fastq_files", [])
    
    if file_count == 0:
        return "åœ¨é¡¹ç›®ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•FASTQæ–‡ä»¶"
    
    # è®¡ç®—åŸºæœ¬ç»Ÿè®¡
    total_size_mb = sum(f["size_mb"] for f in file_list)
    
    # æŒ‰ç›®å½•åˆ†ç»„
    samples_by_dir = {}
    for file_info in file_list:
        directory = file_info["directory"]
        filename = file_info["filename"]
        
        if directory not in samples_by_dir:
            samples_by_dir[directory] = {}
        
        # æå–æ ·æœ¬åç§°å’Œè¯»å–ç±»å‹
        sample_name, read_type = _extract_sample_name_and_type(filename)
        
        if sample_name not in samples_by_dir[directory]:
            samples_by_dir[directory][sample_name] = {"R1": None, "R2": None, "single": None}
        
        samples_by_dir[directory][sample_name][read_type] = {
            "filename": filename,
            "size_mb": file_info["size_mb"],
            "full_path": file_info["full_path"]
        }
    
    # ç»Ÿè®¡æ ·æœ¬æ•°é‡å’Œç±»å‹
    total_samples = sum(len(samples) for samples in samples_by_dir.values())
    paired_count = sum(
        1 for samples in samples_by_dir.values()
        for files in samples.values()
        if files['R1'] and files['R2']
    )
    single_count = sum(
        1 for samples in samples_by_dir.values()
        for files in samples.values()
        if files['single']
    )
    
    # ç”ŸæˆæŠ¥å‘Š
    result = f"ğŸ“Š **FASTQæ•°æ®æ¦‚è§ˆ**\n\n"
    result += f"ğŸ“ˆ **ç»Ÿè®¡ä¿¡æ¯:**\n"
    result += f"   - æ ·æœ¬æ•°é‡: {total_samples} ä¸ª\n"
    result += f"   - æ•°æ®å¤§å°: {total_size_mb:.1f} MB\n"
    result += f"   - åˆ†å¸ƒç›®å½•: {len(samples_by_dir)} ä¸ª\n"
    
    if paired_count > 0 and single_count == 0:
        result += f"   - æµ‹åºç±»å‹: åŒç«¯æµ‹åº (Paired-end)\n"
        sequencing_type = "åŒç«¯æµ‹åº"
    elif single_count > 0 and paired_count == 0:
        result += f"   - æµ‹åºç±»å‹: å•ç«¯æµ‹åº (Single-end)\n"
        sequencing_type = "å•ç«¯æµ‹åº"
    else:
        result += f"   - æµ‹åºç±»å‹: æ··åˆç±»å‹ ({paired_count}åŒç«¯ + {single_count}å•ç«¯)\n"
        sequencing_type = "æ··åˆç±»å‹"
    
    # ä¼°ç®—è¯»æ•°å’Œåˆ†æå»ºè®®
    estimated_reads = total_size_mb * 4  # ç²—ç•¥ä¼°ç®—
    result += f"   - é¢„ä¼°è¯»æ•°: ~{estimated_reads:.0f}M reads\n"
    
    # æ™ºèƒ½åˆ†æå»ºè®®
    result += f"\nğŸ’¡ **åˆ†æå»ºè®®:**\n"
    if total_samples >= 3:
        result += f"   - æ ·æœ¬æ•°é‡å……è¶³ï¼Œé€‚åˆå·®å¼‚è¡¨è¾¾åˆ†æ\n"
    elif total_samples == 2:
        result += f"   - æœ€å°æ¯”è¾ƒç»„ï¼Œå¯è¿›è¡ŒåŸºç¡€å·®å¼‚åˆ†æ\n"
    else:
        result += f"   - å•æ ·æœ¬ï¼Œé€‚åˆè¡¨è¾¾è°±åˆ†ææˆ–è´¨æ§\n"
    
    if sequencing_type == "åŒç«¯æµ‹åº":
        result += f"   - åŒç«¯æ•°æ®è´¨é‡è¾ƒé«˜ï¼Œæ¨èæ ‡å‡†æµç¨‹\n"
    
    if estimated_reads >= 100:
        result += f"   - æµ‹åºæ·±åº¦å……è¶³ï¼Œæ”¯æŒæ·±åº¦åˆ†æ\n"
    elif estimated_reads >= 40:
        result += f"   - æµ‹åºæ·±åº¦é€‚ä¸­ï¼Œæ»¡è¶³åŸºç¡€åˆ†æ\n"
    else:
        result += f"   - æµ‹åºæ·±åº¦è¾ƒä½ï¼Œæ³¨æ„è´¨é‡æ§åˆ¶\n"
    
    if depth == "detailed":
        # è¯¦ç»†æ ·æœ¬ä¿¡æ¯
        result += f"\nğŸ“ **è¯¦ç»†æ ·æœ¬ä¿¡æ¯:**\n"
        for directory, samples in samples_by_dir.items():
            result += f"\nğŸ“‚ {directory}:\n"
            for sample_name, files in samples.items():
                if files["R1"] and files["R2"]:
                    total_sample_size = files['R1']['size_mb'] + files['R2']['size_mb']
                    result += f"   âœ… {sample_name}: åŒç«¯é…å¯¹ ({total_sample_size:.1f}MB)\n"
                    result += f"      â””â”€ R1: {files['R1']['filename']} ({files['R1']['size_mb']}MB)\n"
                    result += f"      â””â”€ R2: {files['R2']['filename']} ({files['R2']['size_mb']}MB)\n"
                elif files["single"]:
                    result += f"   ğŸ“„ {sample_name}: å•ç«¯æ–‡ä»¶ ({files['single']['size_mb']}MB)\n"
                    result += f"      â””â”€ {files['single']['filename']}\n"
    
    # æ·»åŠ åç»­æ­¥éª¤å»ºè®®
    result += f"\nğŸš€ **åç»­æ­¥éª¤:**\n"
    result += f"   â€¢ è¿è¡Œ 'é¡¹ç›®æ¦‚è§ˆ' æŸ¥çœ‹å®Œæ•´é¡¹ç›®çŠ¶æ€\n"
    result += f"   â€¢ ä½¿ç”¨ '/plan' å¼€å§‹é…ç½®åˆ†ææµç¨‹\n"
    
    return result.strip()


# ==================== åŸºå› ç»„æ–‡ä»¶å¤„ç†æ¨¡å— ====================

def _load_genome_config() -> Dict[str, Any]:
    """çº¯åŸºå› ç»„é…ç½®åŠ è½½ - åªæ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§ï¼Œä¸åšçŠ¶æ€åˆ¤æ–­"""
    config = get_tools_config()
    genomes_file = config.genomes_config_path
    
    if not genomes_file.exists():
        return {"detection_status": "no_config_file"}
    
    try:
        with open(genomes_file, 'r', encoding='utf-8') as f:
            genomes_data = json.load(f)
        
        if not genomes_data:
            return {"detection_status": "empty_config"}
            
        # åªæ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§ï¼Œä¸åšä»»ä½•åˆ†ç±»åˆ¤æ–­
        genome_files = {}
        for genome_id, info in genomes_data.items():
            fasta_path = info.get('fasta_path', '')
            gtf_path = info.get('gtf_path', '')
            
            # æ£€æŸ¥æ–‡ä»¶ä¿¡æ¯
            fasta_info = None
            gtf_info = None
            star_index_info = None
            
            if fasta_path:
                fasta_file = Path(fasta_path)
                if fasta_file.exists():
                    fasta_info = {
                        "exists": True,
                        "path": fasta_path,
                        "size_bytes": fasta_file.stat().st_size,
                        "size_mb": round(fasta_file.stat().st_size / 1024**2, 1)
                    }
                else:
                    fasta_info = {"exists": False, "path": fasta_path}
            
            if gtf_path:
                gtf_file = Path(gtf_path)
                if gtf_file.exists():
                    gtf_info = {
                        "exists": True,
                        "path": gtf_path,
                        "size_bytes": gtf_file.stat().st_size,
                        "size_mb": round(gtf_file.stat().st_size / 1024**2, 1)
                    }
                else:
                    gtf_info = {"exists": False, "path": gtf_path}
            
            # æ£€æŸ¥STARç´¢å¼•ç›®å½•
            if fasta_path and Path(fasta_path).exists():
                config = get_tools_config()
                star_index_dir = config.get_star_index_dir(Path(fasta_path))
                if star_index_dir.exists():
                    index_files = list(star_index_dir.iterdir())
                    star_index_info = {
                        "exists": True,
                        "path": str(star_index_dir),
                        "file_count": len(index_files)
                    }
                else:
                    star_index_info = {"exists": False, "path": str(star_index_dir)}
            
            # æ£€æŸ¥HISAT2ç´¢å¼•ç›®å½•
            hisat2_index_info = None
            if fasta_path and Path(fasta_path).exists():
                config = get_tools_config()
                hisat2_index_dir = config.get_hisat2_index_dir(Path(fasta_path))
                if hisat2_index_dir.exists():
                    # æ£€æŸ¥HISAT2ç´¢å¼•ç‰¹å¾æ–‡ä»¶ï¼ˆ.ht2æ ¼å¼ï¼‰
                    ht2_files = list(hisat2_index_dir.glob("*.ht2"))
                    if ht2_files:
                        hisat2_index_info = {
                            "exists": True,
                            "path": str(hisat2_index_dir),
                            "file_count": len(ht2_files)
                        }
                    else:
                        hisat2_index_info = {"exists": False, "path": str(hisat2_index_dir)}
                else:
                    hisat2_index_info = {"exists": False, "path": str(hisat2_index_dir)}
            
            genome_files[genome_id] = {
                "species": info.get('species', ''),
                "version": info.get('version', ''),
                "fasta_url": info.get('fasta_url', ''),
                "gtf_url": info.get('gtf_url', ''),
                "fasta_file": fasta_info,
                "gtf_file": gtf_info,
                "star_index": star_index_info,
                "hisat2_index": hisat2_index_info
            }
        
        return {
            "detection_status": "success",
            "total_genomes": len(genomes_data),
            "genome_files": genome_files,
            "config_path": str(genomes_file)
        }
        
    except Exception as e:
        return {
            "detection_status": "error",
            "error": str(e)
        }


def _format_genome_report(genome_data: dict) -> str:
    """æ ¼å¼åŒ–åŸºå› ç»„æ•°æ®ä¸ºç”¨æˆ·å‹å¥½çš„æŠ¥å‘Š"""
    total_genomes = genome_data.get("total_genomes", 0)
    genome_files = genome_data.get("genome_files", {})
    highlighted_genome = genome_data.get("highlighted_genome")
    
    if total_genomes == 0:
        return "æœªæ‰¾åˆ°åŸºå› ç»„é…ç½®æ–‡ä»¶æˆ–é…ç½®ä¸ºç©º"
    
    result = f"å¯ç”¨åŸºå› ç»„ ({total_genomes} ä¸ª):\n\n"
    
    for genome_id, info in genome_files.items():
        species = info.get('species', 'æœªçŸ¥ç‰©ç§')
        version = info.get('version', genome_id)
        
        # æ£€æŸ¥æœ¬åœ°æ–‡ä»¶çŠ¶æ€
        fasta_info = info.get('fasta_file', {})
        gtf_info = info.get('gtf_file', {})
        star_index_info = info.get('star_index', {}) or {}
        
        fasta_status = "âœ… å·²ä¸‹è½½" if fasta_info.get('exists') else "âŒ æœªä¸‹è½½"
        gtf_status = "âœ… å·²ä¸‹è½½" if gtf_info.get('exists') else "âŒ æœªä¸‹è½½"
        
        # é«˜äº®æ˜¾ç¤ºç‰¹å®šåŸºå› ç»„
        if highlighted_genome == genome_id:
            result += f"ğŸ¯ **{genome_id} ({species})** [é‡ç‚¹å…³æ³¨]\n"
        else:
            result += f"ğŸ§¬ {genome_id} ({species})\n"
            
        result += f"   - ç‰ˆæœ¬: {version}\n"
        result += f"   - FASTA: {fasta_status}\n"
        result += f"   - GTF: {gtf_status}\n"
        
        if fasta_info.get('exists'):
            size_mb = fasta_info.get('size_mb', 0)
            result += f"   - FASTAå¤§å°: {size_mb} MB\n"
        
        if star_index_info.get('exists'):
            file_count = star_index_info.get('file_count', 0)
            result += f"   - STARç´¢å¼•: âœ… {file_count}ä¸ªæ–‡ä»¶\n"
        else:
            result += f"   - STARç´¢å¼•: âŒ æœªæ„å»º\n"
        
        # æ˜¾ç¤ºHISAT2ç´¢å¼•çŠ¶æ€
        hisat2_index_info = info.get('hisat2_index', {}) or {}
        if hisat2_index_info.get('exists'):
            file_count = hisat2_index_info.get('file_count', 0)
            result += f"   - HISAT2ç´¢å¼•: âœ… {file_count}ä¸ª.ht2æ–‡ä»¶\n"
        else:
            result += f"   - HISAT2ç´¢å¼•: âŒ æœªæ„å»º\n"
        
        result += "\n"
    
    return result.strip()


def _format_system_report(system_info: dict) -> str:
    """æ ¼å¼åŒ–ç³»ç»Ÿä¿¡æ¯ä¸ºç”¨æˆ·å‹å¥½çš„æŠ¥å‘Š"""
    if system_info.get("detection_status") == "error":
        return f"âŒ ç³»ç»Ÿä¿¡æ¯æ£€æµ‹é”™è¯¯: {system_info.get('error', '')}"
    
    result = "ğŸ’» **ç³»ç»Ÿç¡¬ä»¶èµ„æºæ£€æµ‹**\n\n"
    
    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    cpu_info = system_info.get("cpu", {})
    memory_info = system_info.get("memory", {})
    disk_info = system_info.get("disk", {})
    load_info = system_info.get("load", {})
    
    result += "ğŸ”§ **CPUèµ„æº:**\n"
    result += f"   - ç‰©ç†æ ¸å¿ƒ: {cpu_info.get('total_cpus', 0)} ä¸ª\n"
    if cpu_info.get('frequency_mhz'):
        result += f"   - åŸºç¡€é¢‘ç‡: {cpu_info['frequency_mhz']:.0f} MHz\n"
    
    result += "\nğŸ§  **å†…å­˜èµ„æº:**\n"
    result += f"   - æ€»å†…å­˜: {memory_info.get('total_gb', 0):.1f} GB\n"
    result += f"   - å¯ç”¨å†…å­˜: {memory_info.get('available_gb', 0):.1f} GB\n"
    result += f"   - å†…å­˜ä½¿ç”¨ç‡: {memory_info.get('used_percent', 0):.1f}%\n"
    
    result += "\nğŸ’¾ **ç£ç›˜ç©ºé—´:**\n"
    result += f"   - æ€»å®¹é‡: {disk_info.get('total_gb', 0):.1f} GB\n"
    result += f"   - å¯ç”¨ç©ºé—´: {disk_info.get('free_gb', 0):.1f} GB\n"
    result += f"   - ä½¿ç”¨ç‡: {disk_info.get('used_percent', 0):.1f}%\n"
    
    if "error" not in load_info:
        result += "\nğŸ“Š **ç³»ç»Ÿè´Ÿè½½:**\n"
        result += f"   - 1åˆ†é’Ÿå¹³å‡è´Ÿè½½: {load_info.get('load_1min', 0):.2f}\n"
        result += f"   - 5åˆ†é’Ÿå¹³å‡è´Ÿè½½: {load_info.get('load_5min', 0):.2f}\n"
        result += f"   - 15åˆ†é’Ÿå¹³å‡è´Ÿè½½: {load_info.get('load_15min', 0):.2f}\n"
        result += f"   - è´Ÿè½½æ¯”ç‡: {load_info.get('load_ratio', 0):.2f} (ç›¸å¯¹äºCPUæ ¸å¿ƒ)\n"
    
    # æ·»åŠ èµ„æºè¯„ä¼°å»ºè®®
    result += "\nğŸ’¡ **èµ„æºè¯„ä¼°:**\n"
    total_gb = memory_info.get('total_gb', 0)
    cpu_cores = cpu_info.get('total_cpus', 0)
    
    if total_gb >= 16:
        result += "   - å†…å­˜å……è¶³ï¼Œæ”¯æŒå¤§å‹åŸºå› ç»„åˆ†æ\n"
    elif total_gb >= 8:
        result += "   - å†…å­˜é€‚ä¸­ï¼Œé€‚åˆä¸­å°å‹æ•°æ®é›†\n"
    else:
        result += "   - å†…å­˜åä½ï¼Œå»ºè®®ä½¿ç”¨æµ‹è¯•æ•°æ®é›†\n"
    
    if cpu_cores >= 8:
        result += "   - CPUæ ¸å¿ƒå……è¶³ï¼Œæ”¯æŒå¹¶è¡Œå¤„ç†\n"
    elif cpu_cores >= 4:
        result += "   - CPUæ ¸å¿ƒé€‚ä¸­ï¼Œæ»¡è¶³åŸºæœ¬åˆ†æéœ€æ±‚\n"
    else:
        result += "   - CPUæ ¸å¿ƒè¾ƒå°‘ï¼Œå¤„ç†æ—¶é—´å¯èƒ½è¾ƒé•¿\n"
    
    return result.strip()


# ==================== åŒæ¨¡å¼å·¥å…·ï¼ˆæ”¯æŒNormalå’ŒDetectèŠ‚ç‚¹ï¼‰ ====================

def scan_fastq_files(mode: str = "normal", depth: str = "basic") -> Union[str, dict]:
    """ç»Ÿä¸€FASTQæ–‡ä»¶æ‰«æå·¥å…· - æ”¯æŒåŒæ¨¡å¼è¾“å‡º
    
    Args:
        mode: "normal" è¿”å›æ ¼å¼åŒ–å­—ç¬¦ä¸²ä¾›ç”¨æˆ·é˜…è¯»
              "detect" è¿”å›ç»“æ„åŒ–dictä¾›ç³»ç»Ÿå¤„ç†
        depth: "basic" å¿«é€Ÿæ‰«æåŸºæœ¬ä¿¡æ¯
               "detailed" æ·±åº¦åˆ†æé…å¯¹å…³ç³»
    
    Returns:
        æ ¹æ®modeè¿”å›å­—ç¬¦ä¸²æˆ–å­—å…¸
    """
    try:
        # ä½¿ç”¨å…±äº«çš„åº•å±‚æ‰«æå‡½æ•°
        raw_data = _scan_fastq_files()
        
        if raw_data.get("detection_status") != "success":
            error_msg = f"FASTQæ–‡ä»¶æ‰«æå¤±è´¥: {raw_data.get('error', 'æœªçŸ¥é”™è¯¯')}"
            if mode == "detect":
                return {
                    "result": error_msg,
                    "query_results": raw_data,
                    "config_updates": {}
                }
            else:
                return error_msg
        
        if mode == "detect":
            # Detectæ¨¡å¼ï¼šè¿”å›ç»“æ„åŒ–æ•°æ®ä¾›ç³»ç»Ÿå¤„ç†
            if depth == "detailed":
                # ä¸ºDetectèŠ‚ç‚¹å¢å¼ºé…å¯¹åˆ†æ
                enhanced_data = _analyze_sample_pairing(raw_data)
                simplified_data = {
                    "detection_status": enhanced_data["detection_status"],
                    "file_count": enhanced_data["total_files_found"],
                    "file_paths": [f["full_path"] for f in enhanced_data["fastq_files"]],
                    "samples": _extract_sample_names(enhanced_data["fastq_files"])
                }
            else:
                simplified_data = {
                    "detection_status": raw_data["detection_status"],
                    "file_count": raw_data["total_files_found"],
                    "file_paths": [f["full_path"] for f in raw_data["fastq_files"]],
                    "samples": _extract_sample_names(raw_data["fastq_files"])
                }
            
            return {
                "result": f"âœ… æ‰«æå®Œæˆï¼Œå‘ç°{raw_data['total_files_found']}ä¸ªFASTQæ–‡ä»¶",
                "query_results": simplified_data,
                "config_updates": {}
            }
        else:
            # Normalæ¨¡å¼ï¼šè¿”å›æ ¼å¼åŒ–æŠ¥å‘Š
            return _format_fastq_report(raw_data, depth)
            
    except Exception as e:
        error_msg = f"FASTQæ–‡ä»¶æ‰«ææ—¶å‡ºé”™: {str(e)}"
        if mode == "detect":
            return {
                "result": error_msg,
                "query_results": {"detection_status": "error", "error": str(e)},
                "config_updates": {}
            }
        else:
            return error_msg


def scan_genome_files(mode: str = "normal", genome_id: Optional[str] = None) -> Union[str, dict]:
    """ç»Ÿä¸€åŸºå› ç»„æ–‡ä»¶æ‰«æå·¥å…· - æ”¯æŒåŒæ¨¡å¼è¾“å‡º
    
    Args:
        mode: "normal" è¿”å›æ ¼å¼åŒ–å­—ç¬¦ä¸²ä¾›ç”¨æˆ·é˜…è¯»
              "detect" è¿”å›ç»“æ„åŒ–dictä¾›ç³»ç»Ÿå¤„ç†
        genome_id: ç‰¹å®šåŸºå› ç»„IDï¼ŒNoneè¡¨ç¤ºæ£€æŸ¥æ‰€æœ‰
    
    Returns:
        æ ¹æ®modeè¿”å›å­—ç¬¦ä¸²æˆ–å­—å…¸
    """
    try:
        # ä½¿ç”¨å…±äº«çš„åŸºå› ç»„é…ç½®åŠ è½½å‡½æ•°
        genome_data = _load_genome_config()
        
        if genome_data.get("detection_status") not in ["success"]:
            error_msg = f"åŸºå› ç»„é…ç½®æ£€æŸ¥å¤±è´¥: {genome_data.get('error', 'é…ç½®æ–‡ä»¶é—®é¢˜')}"
            if mode == "detect":
                return {
                    "result": error_msg,
                    "query_results": genome_data,
                    "config_updates": {}
                }
            else:
                return error_msg
        
        # å¦‚æœæŒ‡å®šäº†ç‰¹å®šåŸºå› ç»„ï¼ŒéªŒè¯å­˜åœ¨æ€§å¹¶æ ‡è®°
        if genome_id and genome_data.get("genome_files"):
            if genome_id not in genome_data["genome_files"]:
                error_msg = f"æœªæ‰¾åˆ°åŸºå› ç»„é…ç½®: {genome_id}"
                if mode == "detect":
                    return {
                        "result": error_msg,
                        "query_results": {"detection_status": "error", "error": error_msg},
                        "config_updates": {}
                    }
                else:
                    return error_msg
            
            # ä¿æŒæ‰€æœ‰åŸºå› ç»„ä¿¡æ¯ï¼Œä½†æ ‡è®°ç‰¹å®šåŸºå› ç»„
            genome_data["highlighted_genome"] = genome_id
        
        if mode == "detect":
            # Detectæ¨¡å¼ï¼šè¿”å›ç»“æ„åŒ–æ•°æ®
            # ç®€åŒ–genome_dataä»¥å‡å°‘JSONå¤æ‚åº¦
            genomes = genome_data.get("genome_files", {})
            available_genomes = []
            for name, info in genomes.items():
                is_complete = all([
                    info.get("fasta_file", {}).get("exists", False),
                    info.get("gtf_file", {}).get("exists", False)
                ])
                available_genomes.append({
                    "name": name,
                    "species": info.get("species"),
                    "complete": is_complete,
                    "has_star_index": (info.get("star_index") or {}).get("exists", False),
                    "has_hisat2_index": (info.get("hisat2_index") or {}).get("exists", False)
                })
            
            simplified_config = {
                "detection_status": genome_data["detection_status"],
                "total_genomes": len(genomes),
                "available_genomes": available_genomes
            }
            
            return {
                "result": f"âœ… æ£€æµ‹åˆ°{len(genomes)}ä¸ªåŸºå› ç»„é…ç½®",
                "query_results": simplified_config,
                "config_updates": {}
            }
        else:
            # Normalæ¨¡å¼ï¼šè¿”å›æ ¼å¼åŒ–æŠ¥å‘Š
            return _format_genome_report(genome_data)
            
    except Exception as e:
        error_msg = f"åŸºå› ç»„æ–‡ä»¶æ‰«ææ—¶å‡ºé”™: {str(e)}"
        if mode == "detect":
            return {
                "result": error_msg,
                "query_results": {"detection_status": "error", "error": str(e)},
                "config_updates": {}
            }
        else:
            return error_msg


def scan_system_resources(mode: str = "normal") -> Union[str, dict]:
    """ç»Ÿä¸€ç³»ç»Ÿèµ„æºæ‰«æå·¥å…· - æ”¯æŒåŒæ¨¡å¼è¾“å‡º
    
    Args:
        mode: "normal" è¿”å›æ ¼å¼åŒ–å­—ç¬¦ä¸²ä¾›ç”¨æˆ·é˜…è¯»
              "detect" è¿”å›ç»“æ„åŒ–dictä¾›ç³»ç»Ÿå¤„ç†
    
    Returns:
        æ ¹æ®modeè¿”å›å­—ç¬¦ä¸²æˆ–å­—å…¸
    """
    try:
        # ä½¿ç”¨å…±äº«çš„ç³»ç»Ÿä¿¡æ¯è·å–å‡½æ•°
        system_info = get_system_info()
        
        if system_info.get("detection_status") == "missing_dependency":
            error_msg = "âŒ æ— æ³•æ£€æµ‹ç³»ç»Ÿèµ„æº (psutilæœªå®‰è£…)\n   è¯·å®‰è£…: uv add psutil"
            if mode == "detect":
                return {
                    "result": error_msg,
                    "query_results": system_info,
                    "config_updates": {}
                }
            else:
                return error_msg
        
        if system_info.get("detection_status") == "error":
            error_msg = f"âš ï¸ ç³»ç»Ÿèµ„æºæ£€æµ‹é”™è¯¯: {system_info.get('error', '')}"
            if mode == "detect":
                return {
                    "result": error_msg,
                    "query_results": system_info,
                    "config_updates": {}
                }
            else:
                return error_msg
        
        if mode == "detect":
            # Detectæ¨¡å¼ï¼šè¿”å›ç»“æ„åŒ–æ•°æ®
            return {
                "result": "âœ… ç³»ç»Ÿèµ„æºæ£€æµ‹å®Œæˆ",
                "query_results": system_info,
                "config_updates": {}
            }
        else:
            # Normalæ¨¡å¼ï¼šè¿”å›æ ¼å¼åŒ–æŠ¥å‘Š
            return _format_system_report(system_info)
            
    except Exception as e:
        error_msg = f"ç³»ç»Ÿèµ„æºæ‰«ææ—¶å‡ºé”™: {str(e)}"
        if mode == "detect":
            return {
                "result": error_msg,
                "query_results": {"detection_status": "error", "error": str(e)},
                "config_updates": {}
            }
        else:
            return error_msg


# ==================== ç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·æ£€æµ‹ ====================

@tool_detection("fastp", "qc_env", ["fastp", "--version"])
def check_fastp_availability() -> dict:
    """æ£€æµ‹fastpå·¥å…·å¯ç”¨æ€§ - ä½¿ç”¨è£…é¥°å™¨"""
    pass


@tool_detection("STAR", "align_env", ["STAR", "--version"])
def check_star_availability() -> dict:
    """æ£€æµ‹STARå·¥å…·å¯ç”¨æ€§ - ä½¿ç”¨è£…é¥°å™¨"""
    pass


@tool_detection("HISAT2", "align_env", ["hisat2", "--version"])
def check_hisat2_availability() -> dict:
    """æ£€æµ‹HISAT2å·¥å…·å¯ç”¨æ€§ - ä½¿ç”¨è£…é¥°å™¨"""
    pass


@tool_detection("featureCounts", "quant_env", ["featureCounts", "-v"])
def check_featurecounts_availability() -> dict:
    """æ£€æµ‹featureCountså·¥å…·å¯ç”¨æ€§ - ä½¿ç”¨è£…é¥°å™¨"""
    pass


# ==================== Normalæ¨¡å¼ä¸“ç”¨å·¥å…· ====================

def get_project_overview(query: str = "") -> str:
    """é¡¹ç›®å…¨è²Œæ¦‚è§ˆ - æ•´åˆæ‰€æœ‰å…³é”®ä¿¡æ¯çš„æ™ºèƒ½ä»ªè¡¨æ¿"""
    try:
        result = "ğŸ¯ **é¡¹ç›®æ¦‚è§ˆä»ªè¡¨æ¿**\n\n"
        
        # 1. æ•°æ®çŠ¶æ€æ€»è§ˆ
        result += "ğŸ“Š **æ•°æ®çŠ¶æ€:**\n"
        
        # æ‰«æFASTQæ–‡ä»¶
        config = get_tools_config()
        project_root = config.project_root
        fastq_extensions = ["*.fastq", "*.fastq.gz", "*.fq", "*.fq.gz"]
        all_fastq_files = []
        for ext in fastq_extensions:
            all_fastq_files.extend(project_root.rglob(ext))
        
        # è¿‡æ»¤åŸå§‹æ–‡ä»¶
        excluded_dirs = ["work", "results", "tmp"]
        processed_indicators = ["trimmed", "fastp", "cutadapt", "filtered", "processed", "qc"]
        raw_fastq_files = []
        
        for file_path in all_fastq_files:
            if not file_path.exists() or any(excluded_dir in file_path.parts for excluded_dir in excluded_dirs):
                continue
            filename_lower = file_path.name.lower()
            if not any(indicator in filename_lower for indicator in processed_indicators):
                raw_fastq_files.append(file_path)
        
        # ç»Ÿè®¡æ ·æœ¬ä¿¡æ¯
        total_samples = 0
        total_size_mb = 0
        sequencing_type = "æœªæ£€æµ‹åˆ°"
        
        if raw_fastq_files:
            # ç®€å•æ ·æœ¬è®¡æ•°å’Œå¤§å°ç»Ÿè®¡
            sample_names = set()
            for file_path in raw_fastq_files:
                filename = file_path.name
                total_size_mb += file_path.stat().st_size / 1024 / 1024
                
                if "_1." in filename or "_R1" in filename:
                    sample_name = filename.split("_1.")[0].split("_R1")[0]
                    sample_names.add(sample_name)
                    sequencing_type = "åŒç«¯æµ‹åº (Paired-end)"
                elif "_2." in filename or "_R2" in filename:
                    sample_name = filename.split("_2.")[0].split("_R2")[0]
                    sample_names.add(sample_name)
                else:
                    sample_name = filename.split(".")[0]
                    sample_names.add(sample_name)
                    sequencing_type = "å•ç«¯æµ‹åº (Single-end)"
            
            total_samples = len(sample_names)
        
        result += f"   - æ ·æœ¬æ•°é‡: {total_samples} ä¸ª\n"
        result += f"   - æ•°æ®å¤§å°: {total_size_mb:.1f} MB\n"
        result += f"   - æµ‹åºç±»å‹: {sequencing_type}\n"
        
        # 2. åŸºå› ç»„çŠ¶æ€
        result += "\nğŸ§¬ **åŸºå› ç»„çŠ¶æ€:**\n"
        config = get_tools_config()
        genomes_file = config.genomes_config_path
        ready_genomes = 0
        total_genomes = 0
        
        if genomes_file.exists():
            with open(genomes_file, 'r', encoding='utf-8') as f:
                genomes_data = json.load(f)
            total_genomes = len(genomes_data)
            
            for genome_id, info in genomes_data.items():
                fasta_path = info.get('fasta_path', '')
                gtf_path = info.get('gtf_path', '')
                if fasta_path and gtf_path and Path(fasta_path).exists() and Path(gtf_path).exists():
                    ready_genomes += 1
        
        result += f"   - å¯ç”¨åŸºå› ç»„: {total_genomes} ä¸ª\n"
        result += f"   - å°±ç»ªåŸºå› ç»„: {ready_genomes} ä¸ª\n"
        
        # 3. å†å²åˆ†æ
        result += "\nğŸ“ˆ **å†å²åˆ†æ:**\n"
        config = get_tools_config()
        results_dir = config.results_dir
        analysis_count = 0
        latest_analysis = "æ— "
        
        if results_dir.exists():
            analysis_dirs = [d for d in results_dir.iterdir() if d.is_dir() and not d.name.startswith('.')]
            analysis_count = len(analysis_dirs)
            
            if analysis_dirs:
                # æ‰¾æœ€æ–°çš„åˆ†æ
                latest_dir = max(analysis_dirs, key=lambda x: x.stat().st_mtime)
                latest_analysis = latest_dir.name
        
        result += f"   - å†å²åˆ†æ: {analysis_count} æ¬¡\n"
        result += f"   - æœ€æ–°åˆ†æ: {latest_analysis}\n"
        
        # 4. é¡¹ç›®å¥åº·åº¦è¯„ä¼°
        result += "\nğŸ’¡ **é¡¹ç›®è¯„ä¼°:**\n"
        
        health_score = 0
        suggestions = []
        
        if total_samples > 0:
            health_score += 40
        else:
            suggestions.append("æœªæ£€æµ‹åˆ°FASTQæ•°æ®æ–‡ä»¶")
        
        if ready_genomes > 0:
            health_score += 30
        else:
            suggestions.append("éœ€è¦ä¸‹è½½åŸºå› ç»„å‚è€ƒæ–‡ä»¶")
        
        if total_samples >= 3:
            health_score += 20
            suggestions.append("æ ·æœ¬æ•°é‡å……è¶³ï¼Œé€‚åˆå·®å¼‚è¡¨è¾¾åˆ†æ")
        elif total_samples >= 2:
            health_score += 10
            suggestions.append("æ ·æœ¬æ•°é‡è¾ƒå°‘ï¼Œè€ƒè™‘å¢åŠ é‡å¤")
        
        if sequencing_type == "åŒç«¯æµ‹åº (Paired-end)":
            health_score += 10
            suggestions.append("åŒç«¯æµ‹åºæ•°æ®ï¼Œè´¨é‡è¾ƒé«˜")
        
        result += f"   - é¡¹ç›®å¥åº·åº¦: {health_score}/100\n"
        
        if health_score >= 80:
            result += "   - çŠ¶æ€: âœ… é¡¹ç›®å°±ç»ªï¼Œå¯å¼€å§‹åˆ†æ\n"
        elif health_score >= 60:
            result += "   - çŠ¶æ€: âš ï¸ åŸºæœ¬å°±ç»ªï¼Œå»ºè®®æ£€æŸ¥é…ç½®\n"
        else:
            result += "   - çŠ¶æ€: âŒ éœ€è¦å®Œå–„é¡¹ç›®é…ç½®\n"
        
        # 5. æ™ºèƒ½å»ºè®®
        if suggestions:
            result += "\nğŸš€ **æ™ºèƒ½å»ºè®®:**\n"
            for suggestion in suggestions[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ªå»ºè®®
                result += f"   â€¢ {suggestion}\n"
        
        result += "\nğŸ’¡ è¾“å…¥ '/plan' å¼€å§‹é…ç½®åˆ†ææµç¨‹"
        
        return result.strip()
        
    except Exception as e:
        return f"ç”Ÿæˆé¡¹ç›®æ¦‚è§ˆæ—¶å‡ºé”™: {str(e)}"


def list_analysis_history(query: str = "") -> str:
    """å†å²åˆ†æç®¡ç† - åŸºäºæ–°çš„å½’æ¡£æ–‡ä»¶å¤¹ç³»ç»Ÿ"""
    try:
        result = "ğŸ“ˆ **åˆ†æå†å²è®°å½•**\n\n"
        
        # åªæ£€æŸ¥æ–°çš„reportså½’æ¡£æ–‡ä»¶å¤¹
        config = get_tools_config()
        reports_dir = config.reports_dir
        if not reports_dir.exists():
            return "ğŸ“­ æš‚æ— åˆ†æå†å²è®°å½•\n\nğŸ’¡ å®Œæˆé¦–æ¬¡åˆ†æåï¼Œå†å²è®°å½•å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ"
        
        # æ‰«ææ—¶é—´æˆ³æ ¼å¼çš„å½’æ¡£æ–‡ä»¶å¤¹
        archive_folders = []
        for item in reports_dir.iterdir():
            if item.is_dir() and not item.name.startswith('.') and item.name != "latest":
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ—¶é—´æˆ³æ ¼å¼ (YYYYMMDD_HHMMSS)
                if len(item.name) == 15 and item.name[8] == '_':
                    try:
                        timestamp = datetime.strptime(item.name, "%Y%m%d_%H%M%S")
                        archive_folders.append({
                            "path": item,
                            "timestamp": timestamp,
                            "name": item.name
                        })
                    except ValueError:
                        continue
        
        if not archive_folders:
            return "ğŸ“­ reportsç›®å½•å­˜åœ¨ä½†æ— å½’æ¡£è®°å½•\n\nğŸ’¡ è¿è¡Œåˆ†æåç»“æœå°†è‡ªåŠ¨å½’æ¡£åˆ°reports/æ—¶é—´æˆ³/ç›®å½•"
        
        # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰
        archive_folders.sort(key=lambda x: x["timestamp"], reverse=True)
        
        result += f"ğŸ“‚ **å‘ç° {len(archive_folders)} ä¸ªå½’æ¡£åˆ†æ:**\n\n"
        
        for i, archive in enumerate(archive_folders):
            if i >= 15:  # æ˜¾ç¤ºå‰15ä¸ªæœ€æ–°çš„åˆ†æ
                break
            
            archive_path = archive["path"]
            timestamp_str = archive["timestamp"].strftime("%Y-%m-%d %H:%M:%S")
            
            result += f"ğŸ“‹ **{archive['name']}**\n"
            result += f"   - åˆ†ææ—¶é—´: {timestamp_str}\n"
            
            # æ£€æŸ¥å½’æ¡£æ–‡ä»¶å†…å®¹
            files_info = []
            file_sizes = {}
            
            json_report = archive_path / "analysis_report.json"
            md_report = archive_path / "analysis_summary.md" 
            runtime_config = archive_path / "runtime_config.json"
            exec_log = archive_path / "execution_log.txt"
            
            if json_report.exists():
                files_info.append("JSONæŠ¥å‘Š")
                file_sizes["json"] = json_report.stat().st_size / 1024  # KB
            
            if md_report.exists():
                files_info.append("MarkdownæŠ¥å‘Š")
                file_sizes["md"] = md_report.stat().st_size / 1024
                
            if runtime_config.exists():
                files_info.append("é…ç½®æ–‡ä»¶")
                file_sizes["config"] = runtime_config.stat().st_size / 1024
                
            if exec_log.exists():
                files_info.append("æ‰§è¡Œæ—¥å¿—")
                file_sizes["log"] = exec_log.stat().st_size / 1024
            
            if files_info:
                result += f"   - å½’æ¡£å†…å®¹: {', '.join(files_info)}\n"
            
            # å°è¯•ä»runtime_config.jsonè¯»å–é…ç½®ä¿¡æ¯
            try:
                if runtime_config.exists():
                    with open(runtime_config, 'r', encoding='utf-8') as f:
                        config_data = json.load(f)
                        
                    tools_used = []
                    nextflow_params = config_data.get("nextflow_params", {})
                    if nextflow_params.get("qc_tool"):
                        tools_used.append(f"è´¨æ§({nextflow_params['qc_tool']})")
                    if nextflow_params.get("align_tool"):
                        tools_used.append(f"æ¯”å¯¹({nextflow_params['align_tool']})")
                    if nextflow_params.get("quant_tool"):
                        tools_used.append(f"å®šé‡({nextflow_params['quant_tool']})")
                    if nextflow_params.get("genome_version"):
                        result += f"   - å‚è€ƒåŸºå› ç»„: {nextflow_params['genome_version']}\n"
                        
                    if tools_used:
                        result += f"   - åˆ†æå·¥å…·: {', '.join(tools_used)}\n"
                        
            except (json.JSONDecodeError, FileNotFoundError, KeyError):
                pass
            
            # è®¡ç®—å½’æ¡£æ–‡ä»¶å¤¹æ€»å¤§å°
            try:
                total_size = sum(f.stat().st_size for f in archive_path.rglob("*") if f.is_file())
                size_kb = total_size / 1024
                if size_kb > 1024:
                    result += f"   - å½’æ¡£å¤§å°: {size_kb / 1024:.1f} MB\n"
                else:
                    result += f"   - å½’æ¡£å¤§å°: {size_kb:.1f} KB\n"
            except:
                result += "   - å½’æ¡£å¤§å°: è®¡ç®—å¤±è´¥\n"
            
            # åˆ†æå®Œæ•´æ€§çŠ¶æ€
            if json_report.exists() and md_report.exists() and runtime_config.exists():
                result += "   - çŠ¶æ€: âœ… å½’æ¡£å®Œæ•´\n"
            elif json_report.exists() or md_report.exists():
                result += "   - çŠ¶æ€: âš ï¸ å½’æ¡£éƒ¨åˆ†å®Œæ•´\n"
            else:
                result += "   - çŠ¶æ€: âŒ å½’æ¡£ä¸å®Œæ•´\n"
            
            result += f"   - å½’æ¡£è·¯å¾„: `reports/{archive['name']}/`\n\n"
        
        # æ˜¾ç¤ºç»Ÿè®¡å’Œä½¿ç”¨æç¤º
        if len(archive_folders) > 15:
            result += f"â­ï¸ åªæ˜¾ç¤ºäº†æœ€æ–°çš„15ä¸ªåˆ†æï¼Œå…±æœ‰{len(archive_folders)}ä¸ªå½’æ¡£è®°å½•\n\n"
        
        result += "ğŸ’¡ **ä½¿ç”¨è¯´æ˜:**\n"
        result += "- æ¯æ¬¡åˆ†æå®Œæˆåä¼šè‡ªåŠ¨åˆ›å»ºå½’æ¡£æ–‡ä»¶å¤¹\n"
        result += "- æœ€æ–°åˆ†æå¯é€šè¿‡ `reports/latest/` è®¿é—®\n" 
        result += "- å½’æ¡£åŒ…å«: é…ç½®ã€JSONæŠ¥å‘Šã€MarkdownæŠ¥å‘Šã€æ‰§è¡Œæ—¥å¿—\n"
        result += "- è¾“å…¥ '/plan' å¼€å§‹æ–°çš„åˆ†æ\n"
        
        return result.strip()
        
    except Exception as e:
        return f"è·å–åˆ†æå†å²æ—¶å‡ºé”™: {str(e)}"
        
        # åˆ†ææˆåŠŸé…ç½®æå–
        result += "ğŸ”„ **å¯å¤ç”¨é…ç½®:**\n"
        successful_configs = []
        
        for analysis_dir in analysis_dirs[:5]:  # æ£€æŸ¥æœ€æ–°5ä¸ªåˆ†æ
            # æŸ¥æ‰¾é…ç½®æ–‡ä»¶
            config_files = []
            for item in analysis_dir.rglob("*config*"):
                if item.is_file() and item.suffix in ['.json', '.yaml', '.yml']:
                    config_files.append(item)
            
            if config_files:
                successful_configs.append({
                    'name': analysis_dir.name,
                    'time': time.strftime("%m-%d %H:%M", time.localtime(analysis_dir.stat().st_mtime)),
                    'configs': len(config_files)
                })
        
        if successful_configs:
            for config in successful_configs:
                result += f"   â€¢ {config['name']} ({config['time']}) - {config['configs']}ä¸ªé…ç½®æ–‡ä»¶\n"
            result += "\nğŸ’¡ è¿™äº›é…ç½®å¯ä»¥åœ¨Planæ¨¡å¼ä¸­å¤ç”¨\n"
        else:
            result += "   â€¢ æš‚æ— å¯è¯†åˆ«çš„é…ç½®æ–‡ä»¶\n"
        
        result += "\nğŸš€ è¾“å…¥ '/plan' å¼€å§‹æ–°çš„åˆ†ææµç¨‹"
        
        return result.strip()
        
    except Exception as e:
        return f"è·å–åˆ†æå†å²æ—¶å‡ºé”™: {str(e)}"


def add_genome_config(user_input: str = "") -> str:
    """ä½¿ç”¨LLMæ™ºèƒ½è§£æç”¨æˆ·è¾“å…¥å¹¶æ·»åŠ åŸºå› ç»„é…ç½®
    
    Args:
        user_input: ç”¨æˆ·è¾“å…¥ï¼ŒåŒ…å«"æ·»åŠ åŸºå› ç»„"å’ŒURLä¿¡æ¯
    
    Returns:
        æ·»åŠ ç»“æœçš„æ–‡æœ¬æè¿°
    """
    try:
        print(f"ğŸ› ï¸ add_genome_configè¢«è°ƒç”¨ï¼Œç”¨æˆ·è¾“å…¥: {user_input}")
        
        if not user_input.strip():
            return "è¯·æä¾›åŸºå› ç»„ä¿¡æ¯ï¼ŒåŒ…å«FASTAå’ŒGTFæ–‡ä»¶çš„URL"
        
        # ä½¿ç”¨LLMè§£æç”¨æˆ·è¾“å…¥
        from src.core import get_shared_llm
        
        llm = get_shared_llm()
        
        system_prompt = """ä½ æ˜¯åŸºå› ç»„ä¿¡æ¯è§£æä¸“å®¶ã€‚ç”¨æˆ·ä¼šæä¾›åŒ…å«FASTAå’ŒGTFæ–‡ä»¶URLçš„æ–‡æœ¬ï¼Œä½ éœ€è¦æå–ä¿¡æ¯å¹¶è¿”å›JSONæ ¼å¼ï¼š

{
  "genome_id": "åŸºå› ç»„æ ‡è¯†ç¬¦(å¦‚hg38,mm39,ce11)",
  "species": "ç‰©ç§åç§°(å¦‚human,mouse,caenorhabditis_elegans)", 
  "version": "ç‰ˆæœ¬å·(é€šå¸¸ä¸genome_idç›¸åŒ)",
  "fasta_url": "FASTAæ–‡ä»¶å®Œæ•´URL",
  "gtf_url": "GTFæ–‡ä»¶å®Œæ•´URL"
}

è¦æ±‚ï¼š
1. ä»URLä¸­æ™ºèƒ½è¯†åˆ«åŸºå› ç»„ç‰ˆæœ¬å’Œç‰©ç§
2. å¸¸è§æ˜ å°„ï¼šhg->human, mm->mouse, ce->caenorhabditis_elegans, dm->drosophila, rn->rat
3. å¦‚æœæ— æ³•ç¡®å®šï¼Œæ ¹æ®ç”Ÿç‰©ä¿¡æ¯å­¦å¸¸è¯†åˆç†æ¨æ–­
4. åªè¿”å›æœ‰æ•ˆçš„JSONï¼Œä¸è¦å…¶ä»–è§£é‡Š

ç¤ºä¾‹ï¼š
è¾“å…¥ï¼š"æ·»åŠ åŸºå› ç»„ https://path/ce11/ce11.fa.gz https://path/ce11/ce11.gtf.gz"
è¾“å‡ºï¼š{"genome_id":"ce11","species":"caenorhabditis_elegans","version":"ce11","fasta_url":"https://path/ce11/ce11.fa.gz","gtf_url":"https://path/ce11/ce11.gtf.gz"}"""

        # ä½¿ç”¨ç®€å•çš„invokeæ–¹æ³•
        response = llm.invoke(f"System: {system_prompt}\n\nHuman: è¯·è§£æï¼š{user_input}")
        parsed_content = response.content.strip()
        
        print(f"ğŸ¤– LLMè§£æç»“æœ: {parsed_content}")
        
        # æå–JSONå†…å®¹
        try:
            # å°è¯•ç›´æ¥è§£æ
            genome_info = json.loads(parsed_content)
        except json.JSONDecodeError:
            # å¦‚æœå¤±è´¥ï¼Œå°è¯•æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{.*\}', parsed_content, re.DOTALL)
            if not json_match:
                return f"LLMè§£æå¤±è´¥ï¼Œæ— æ³•æå–æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚åŸå§‹å“åº”ï¼š{parsed_content}"
            
            try:
                genome_info = json.loads(json_match.group())
            except json.JSONDecodeError as e:
                return f"JSONè§£æé”™è¯¯ï¼š{str(e)}\nåŸå§‹å“åº”ï¼š{parsed_content}"
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        required_fields = ['genome_id', 'species', 'version', 'fasta_url', 'gtf_url']
        missing_fields = [field for field in required_fields if not genome_info.get(field)]
        if missing_fields:
            return f"LLMè§£æç»“æœç¼ºå°‘å¿…éœ€å­—æ®µï¼š{missing_fields}\nè§£æç»“æœï¼š{genome_info}"
        
        print(f"âœ… è§£ææˆåŠŸ: {genome_info}")
        
        # æ‰§è¡Œå®é™…çš„åŸºå› ç»„æ·»åŠ é€»è¾‘
        genome_id = genome_info['genome_id']
        species = genome_info['species']
        version = genome_info['version']
        fasta_url = genome_info['fasta_url']
        gtf_url = genome_info['gtf_url']
        
        # éªŒè¯URLæ ¼å¼
        if not (fasta_url.startswith('http://') or fasta_url.startswith('https://')):
            return f"FASTA URLæ ¼å¼æ— æ•ˆï¼š{fasta_url}"
        if not (gtf_url.startswith('http://') or gtf_url.startswith('https://')):
            return f"GTF URLæ ¼å¼æ— æ•ˆï¼š{gtf_url}"
        
        # ç”Ÿæˆæœ¬åœ°è·¯å¾„ï¼ˆç›¸å¯¹å®¹å™¨å·¥ä½œç›®å½• /dataï¼Œä¸è¦ä»¥ data/ å‰ç¼€å¼€å¤´ï¼‰
        fasta_path = f"genomes/{species}/{version}/{version}.fa"
        gtf_path = f"genomes/{species}/{version}/{version}.gtf"
        
        # æ„å»ºæ–°çš„åŸºå› ç»„é…ç½®
        new_genome_config = {
            "species": species,
            "version": version,
            "fasta_path": fasta_path,
            "gtf_path": gtf_path,
            "fasta_url": fasta_url,
            "gtf_url": gtf_url
        }
        
        # è¯»å–ç°æœ‰é…ç½®
        config = get_tools_config()
        genomes_file = config.genomes_config_path
        if genomes_file.exists():
            with open(genomes_file, 'r', encoding='utf-8') as f:
                genomes_data = json.load(f)
        else:
            genomes_data = {}
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if genome_id in genomes_data:
            existing = genomes_data[genome_id]
            return f"""åŸºå› ç»„ {genome_id} å·²å­˜åœ¨

ç°æœ‰é…ç½®ï¼š
- ç‰©ç§ï¼š{existing.get('species', 'æœªçŸ¥')}
- ç‰ˆæœ¬ï¼š{existing.get('version', 'æœªçŸ¥')}
- FASTA URLï¼š{existing.get('fasta_url', 'æœªè®¾ç½®')}
- GTF URLï¼š{existing.get('gtf_url', 'æœªè®¾ç½®')}

å¦‚éœ€æ›´æ–°ï¼Œè¯·ä½¿ç”¨ä¸åŒçš„genome_idæˆ–å…ˆåˆ é™¤ç°æœ‰é…ç½®"""
        
        # æ·»åŠ æ–°é…ç½®
        genomes_data[genome_id] = new_genome_config
        
        # ä¿å­˜é…ç½®
        config = get_tools_config()
        config.path_manager.ensure_directory(config.settings.config_dir)
        with open(genomes_file, 'w', encoding='utf-8') as f:
            json.dump(genomes_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… åŸºå› ç»„é…ç½®æ·»åŠ æˆåŠŸ: {genome_id}")
        
        return f"""âœ… æˆåŠŸæ·»åŠ åŸºå› ç»„é…ç½®ï¼š{genome_id}

ğŸ“‹ åŸºå› ç»„ä¿¡æ¯ï¼š
- åŸºå› ç»„IDï¼š{genome_id}
- ç‰©ç§ï¼š{species}
- ç‰ˆæœ¬ï¼š{version}

ğŸ“ æœ¬åœ°è·¯å¾„é…ç½®ï¼š
- FASTAï¼š{fasta_path}
- GTFï¼š{gtf_path}

ğŸŒ ä¸‹è½½æºï¼š
- FASTA URLï¼š{fasta_url}
- GTF URLï¼š{gtf_url}

ğŸ’¡ æç¤ºï¼šæ–‡ä»¶å°†åœ¨é¦–æ¬¡è¿è¡Œåˆ†ææ—¶è‡ªåŠ¨ä¸‹è½½åˆ°æŒ‡å®šè·¯å¾„"""
        
    except Exception as e:
        print(f"âŒ add_genome_configå‡ºé”™: {str(e)}")
        return f"æ™ºèƒ½è§£æå¹¶æ·»åŠ åŸºå› ç»„æ—¶å‡ºé”™ï¼š{str(e)}"


def get_help(query: str = "") -> str:
    """è·å–ç³»ç»Ÿå¸®åŠ©ä¿¡æ¯"""
    return """
ğŸ¯ RNA-seqæ™ºèƒ½åˆ†æåŠ©æ‰‹ - Normalæ¨¡å¼ (é¡¹ç›®ä¿¡æ¯ä¸­å¿ƒ)

ğŸ“Š **æ ¸å¿ƒé¡¹ç›®å·¥å…·:**
â€¢ é¡¹ç›®æ¦‚è§ˆ - ä¸€é”®æŸ¥çœ‹é¡¹ç›®å®Œæ•´çŠ¶æ€å’Œå¥åº·åº¦
â€¢ å†å²åˆ†æè®°å½• - æµè§ˆå·²å®Œæˆçš„åˆ†æå’Œå¯å¤ç”¨é…ç½®

ğŸ“‹ **è¯¦ç»†ä¿¡æ¯æŸ¥è¯¢:**
â€¢ æŸ¥çœ‹FASTQæ–‡ä»¶ - è¯¦ç»†æ‰«ææ‰€æœ‰æµ‹åºæ•°æ®æ–‡ä»¶
â€¢ æŸ¥çœ‹åŸºå› ç»„ä¿¡æ¯ - æ˜¾ç¤ºå¯ç”¨å‚è€ƒåŸºå› ç»„çŠ¶æ€

ğŸ—„ï¸ **åŸºå› ç»„ç®¡ç†:**
â€¢ æ·»åŠ åŸºå› ç»„é…ç½® - æ™ºèƒ½è§£æå¹¶æ·»åŠ æ–°çš„å‚è€ƒåŸºå› ç»„

ğŸš€ **å¼€å§‹åˆ†æ:**
è¾“å…¥ "/plan" è¿›å…¥è®¡åˆ’æ¨¡å¼ï¼ŒPlanæ¨¡å¼å°†æ‰§è¡Œæ·±åº¦æ•°æ®æ£€æµ‹ã€ç³»ç»Ÿèµ„æºè¯„ä¼°å¹¶åˆ¶å®šæ™ºèƒ½åˆ†ææ–¹æ¡ˆ

ğŸ’¡ **ä½¿ç”¨å»ºè®®:**
1. é¦–æ¬¡ä½¿ç”¨å»ºè®®è¿è¡Œ "é¡¹ç›®æ¦‚è§ˆ" äº†è§£é¡¹ç›®çŠ¶æ€
2. å¦‚éœ€è¯¦ç»†ä¿¡æ¯ï¼Œä½¿ç”¨å…·ä½“çš„æŸ¥è¯¢å·¥å…·
3. é¡¹ç›®äº†è§£å®Œæˆåï¼Œä½¿ç”¨ "/plan" è¿›å…¥æ·±åº¦åˆ†æè§„åˆ’

ğŸ”„ **æ¨¡å¼åˆ†å·¥:**
- Normalæ¨¡å¼: å¿«é€Ÿä¿¡æ¯æŸ¥çœ‹å’Œé¡¹ç›®æ¦‚è§ˆ
- Planæ¨¡å¼: æ·±åº¦æ£€æµ‹ã€å°±ç»ªè¯„ä¼°å’Œåˆ†ææ–¹æ¡ˆåˆ¶å®š
""".strip()
