"""
RNA-seqæ™ºèƒ½åˆ†æåŠ©æ‰‹å·¥å…·æ¨¡å—
æä¾›æ•°æ®æ”¶é›†åŠŸèƒ½ï¼Œè¿”å›ç»“æ„åŒ–æ•°æ®ä¾›LLMæ™ºèƒ½å¤„ç†å’Œå±•ç¤º

é‡æ„åŸåˆ™:
- å·¥å…·ä¸“æ³¨çº¯æ•°æ®æ”¶é›†ï¼Œä¸åšæ ¼å¼åŒ–
- ä½¿ç”¨å®˜æ–¹ @tool è£…é¥°å™¨
- ç§»é™¤åŒæ¨¡å¼é€»è¾‘ï¼Œäº¤ç”±LLMå†³å®šå±•ç¤ºæ–¹å¼
- ç®€åŒ–ä»£ç ç»“æ„ï¼Œæé«˜ç»´æŠ¤æ€§
"""

import json
import re
import time
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# ä½¿ç”¨å®˜æ–¹å·¥å…·è£…é¥°å™¨
from langchain_core.tools import tool

# å¯¼å…¥é…ç½®æ¨¡å—
from .config import get_tools_config


# ==================== å·¥å…·å‡½æ•° (ä½¿ç”¨ @tool è£…é¥°å™¨) ====================

@tool
def scan_fastq_files() -> Dict[str, Any]:
    """æ‰«æFASTQæ–‡ä»¶ï¼Œä¼˜å…ˆåœ¨æ•°æ®ç›®å½•(data/fastq)ä¸‹æŸ¥æ‰¾ï¼Œå…¼å®¹å®¹å™¨æŒ‚è½½ç›®å½•ã€‚

    è¿”å›ï¼šæ–‡ä»¶åˆ—è¡¨ã€æ ·æœ¬ä¿¡æ¯å’ŒåŸºæœ¬ç»Ÿè®¡æ•°æ®ã€‚
    """
    config = get_tools_config()
    fastq_extensions = ["*.fastq", "*.fastq.gz", "*.fq", "*.fq.gz"]

    # å®šä¹‰è¦æ’é™¤çš„ç›®å½•ï¼ˆä¸­é—´æ–‡ä»¶å’Œç¼“å­˜ç›®å½•ï¼‰
    exclude_directories = {
        "work", "tmp", "temp", "results", "output",
        ".nextflow", "logs", "cache", "__pycache__"
    }

    # é€‰æ‹©æœç´¢æ ¹ç›®å½•ï¼šä¼˜å…ˆ data/fastqï¼Œå…¶æ¬¡ dataï¼Œæœ€åé¡¹ç›®æ ¹ç›®å½•
    search_roots = []
    try:
        if config.fastq_dir.exists():
            search_roots.append(config.fastq_dir)
        elif config.settings.data_dir.exists():
            search_roots.append(config.settings.data_dir)
        else:
            search_roots.append(config.project_root)
    except Exception:
        search_roots.append(config.project_root)

    # æ‰«ææ‰€æœ‰FASTQæ–‡ä»¶
    all_fastq_files = []
    for root in search_roots:
        for ext in fastq_extensions:
            for file_path in root.rglob(ext):
                if any(excluded in file_path.parts for excluded in exclude_directories):
                    continue
                all_fastq_files.append(file_path)
    
    # æ”¶é›†æ–‡ä»¶ä¿¡æ¯
    file_list = []
    for file_path in all_fastq_files:
        if not file_path.exists() or not file_path.is_file():
            continue
            
        try:
            file_size = file_path.stat().st_size
            
            # æå–æ ·æœ¬ä¿¡æ¯
            filename = file_path.name
            if "_1." in filename or "_R1" in filename:
                sample_name = filename.split("_1.")[0].split("_R1")[0]
                read_type = "R1"
            elif "_2." in filename or "_R2" in filename:
                sample_name = filename.split("_2.")[0].split("_R2")[0]
                read_type = "R2"
            else:
                sample_name = filename.split(".")[0]
                read_type = "single"
            
            file_info = {
                "filename": filename,
                "full_path": str(file_path),
                "directory": str(file_path.parent),
                "size_bytes": file_size,
                "size_mb": round(file_size / 1024 / 1024, 2),
                "extension": file_path.suffix,
                "sample_name": sample_name,
                "read_type": read_type
            }
            file_list.append(file_info)
        except Exception:
            continue
    
    # æŒ‰æ–‡ä»¶åæ’åº
    file_list.sort(key=lambda x: x["filename"])
    
    # æ ·æœ¬ç»Ÿè®¡
    samples = {}
    for file_info in file_list:
        sample_name = file_info["sample_name"]
        read_type = file_info["read_type"]
        
        if sample_name not in samples:
            samples[sample_name] = {"R1": None, "R2": None, "single": None}
        
        samples[sample_name][read_type] = file_info
    
    # ç¡®å®šæµ‹åºç±»å‹
    paired_count = sum(1 for s in samples.values() if s["R1"] and s["R2"])
    single_count = sum(1 for s in samples.values() if s["single"])
    
    if paired_count > 0 and single_count == 0:
        sequencing_type = "paired_end"
    elif single_count > 0 and paired_count == 0:
        sequencing_type = "single_end"
    else:
        sequencing_type = "mixed"
    
    return {
        "detection_status": "success",
        "search_roots": [str(p) for p in search_roots],
        "total_files": len(file_list),
        "total_samples": len(samples),
        "sequencing_type": sequencing_type,
        "paired_samples": paired_count,
        "single_samples": single_count,
        "total_size_mb": sum(f["size_mb"] for f in file_list),
        "files": file_list,
        "samples": samples,
        "scan_timestamp": time.time()
    }


@tool
def scan_system_resources() -> Dict[str, Any]:
    """æ£€æµ‹ç³»ç»Ÿç¡¬ä»¶èµ„æºï¼Œè¿”å›CPUã€å†…å­˜ã€ç£ç›˜å’Œè´Ÿè½½ä¿¡æ¯"""
    try:
        import psutil
        
        # CPUä¿¡æ¯
        cpu_count = psutil.cpu_count(logical=False) or 1
        cpu_freq = psutil.cpu_freq()
        
        # å†…å­˜ä¿¡æ¯
        memory = psutil.virtual_memory()
        memory_gb = memory.total / 1024**3
        available_gb = memory.available / 1024**3
        used_percent = memory.percent
        
        # ç£ç›˜ä¿¡æ¯
        disk = psutil.disk_usage('.')
        disk_total_gb = disk.total / 1024**3
        disk_free_gb = disk.free / 1024**3
        disk_used_percent = (disk.used / disk.total) * 100
        
        # ç³»ç»Ÿè´Ÿè½½
        load_info = {}
        try:
            load_avg = psutil.getloadavg()
            load_ratio = load_avg[0] / max(cpu_count, 1)
            load_info = {
                "load_1min": round(load_avg[0], 2),
                "load_5min": round(load_avg[1], 2),
                "load_15min": round(load_avg[2], 2),
                "load_ratio": round(load_ratio, 2)
            }
        except (AttributeError, OSError):
            load_info = {"error": "platform_not_supported"}
        
        return {
            "detection_status": "success",
            "cpu": {
                "physical_cores": cpu_count,
                "frequency_mhz": cpu_freq.current if cpu_freq else None
            },
            "memory": {
                "total_gb": round(memory_gb, 1),
                "available_gb": round(available_gb, 1),
                "used_percent": round(used_percent, 1),
                "total_bytes": memory.total,
                "available_bytes": memory.available
            },
            "disk": {
                "total_gb": round(disk_total_gb, 1),
                "free_gb": round(disk_free_gb, 1),
                "used_percent": round(disk_used_percent, 1),
                "total_bytes": disk.total,
                "free_bytes": disk.free
            },
            "load": load_info,
            "timestamp": time.time()
        }
        
    except ImportError:
        return {
            "detection_status": "missing_dependency",
            "error": "psutil not installed",
            "install_command": "uv add psutil"
        }
    except Exception as e:
        return {
            "detection_status": "error",
            "error": str(e)
        }


@tool
def scan_genome_files(genome_id: Optional[str] = None) -> Dict[str, Any]:
    """æ‰«æå¯ç”¨çš„å‚è€ƒåŸºå› ç»„é…ç½®ï¼Œè¿”å›åŸºå› ç»„åˆ—è¡¨å’Œæ–‡ä»¶çŠ¶æ€
    
    Args:
        genome_id: å¯é€‰çš„ç‰¹å®šåŸºå› ç»„IDï¼Œç”¨äºé‡ç‚¹å…³æ³¨
    """
    config = get_tools_config()
    genomes_file = config.genomes_config_path
    
    if not genomes_file.exists():
        result = {"detection_status": "no_config_file"}
    else:
        try:
            with open(genomes_file, 'r', encoding='utf-8') as f:
                genomes_data = json.load(f)
            
            if not genomes_data:
                result = {"detection_status": "empty_config"}
            else:
                # æ£€æŸ¥æ¯ä¸ªåŸºå› ç»„çš„æ–‡ä»¶çŠ¶æ€
                genome_status = {}
                for genome_id_key, info in genomes_data.items():
                    fasta_path = info.get('fasta_path', '')
                    gtf_path = info.get('gtf_path', '')
                    
                    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
                    fasta_exists = bool(fasta_path and Path(fasta_path).exists())
                    gtf_exists = bool(gtf_path and Path(gtf_path).exists())
                    
                    # æ£€æŸ¥ç´¢å¼•çŠ¶æ€
                    star_index_exists = False
                    hisat2_index_exists = False
                    
                    star_index_dir_str = ""
                    hisat2_index_dir_str = ""
                    if fasta_exists:
                        # STARç´¢å¼•æ£€æŸ¥
                        star_index_dir = config.get_star_index_dir(Path(fasta_path))
                        star_index_dir_str = str(star_index_dir)
                        if star_index_dir.exists():
                            star_index_files = list(star_index_dir.iterdir())
                            star_index_exists = len(star_index_files) > 0
                        
                        # HISAT2ç´¢å¼•æ£€æŸ¥
                        hisat2_index_dir = config.get_hisat2_index_dir(Path(fasta_path))
                        hisat2_index_dir_str = str(hisat2_index_dir)
                        if hisat2_index_dir.exists():
                            ht2_files = list(hisat2_index_dir.glob("*.ht2"))
                            hisat2_index_exists = len(ht2_files) > 0
                    
                    genome_status[genome_id_key] = {
                        "species": info.get('species', ''),
                        "version": info.get('version', ''),
                        "fasta_url": info.get('fasta_url', ''),
                        "gtf_url": info.get('gtf_url', ''),
                        "fasta_path": fasta_path,
                        "gtf_path": gtf_path,
                        "fasta_exists": fasta_exists,
                        "gtf_exists": gtf_exists,
                        "complete": fasta_exists and gtf_exists,
                        "star_index_exists": star_index_exists,
                        "star_index_dir": star_index_dir_str,
                        "hisat2_index_exists": hisat2_index_exists,
                        "hisat2_index_dir": hisat2_index_dir_str
                    }
                
                result = {
                    "detection_status": "success",
                    "total_genomes": len(genomes_data),
                    "available_genomes": len([g for g in genome_status.values() if g["complete"]]),
                    "genomes": genome_status,
                    "config_path": str(genomes_file)
                }
        except Exception as e:
            result = {
                "detection_status": "error",
                "error": str(e)
            }
    
    # å¦‚æœæŒ‡å®šäº†ç‰¹å®šåŸºå› ç»„ï¼Œæ·»åŠ æ ‡è®°
    if genome_id:
        result["requested_genome"] = genome_id
    
    return result


@tool
def get_project_overview() -> Dict[str, Any]:
    """è·å–é¡¹ç›®æ•´ä½“çŠ¶æ€æ¦‚è§ˆï¼ŒåŒ…æ‹¬æ•°æ®ã€åŸºå› ç»„ã€ç³»ç»Ÿèµ„æºå’Œåˆ†æå†å²"""
    # ä½¿ç”¨ BaseTool.invoke ä»¥é¿å…åœ¨å·¥å…·å†…éƒ¨ç›¸äº’è°ƒç”¨äº§ç”Ÿå¼ƒç”¨è­¦å‘Š
    return {
        "fastq_data": scan_fastq_files.invoke({}),
        "genome_status": scan_genome_files.invoke({}),
        "system_resources": scan_system_resources.invoke({}),
        "analysis_history": list_analysis_history.invoke({}),
        "overview_timestamp": time.time()
    }


@tool
def list_analysis_history() -> Dict[str, Any]:
    """è·å–å†å²åˆ†æè®°å½•ï¼Œè¿”å›åˆ†ææ—¶é—´ã€é…ç½®å’Œç»“æœä¿¡æ¯"""
    config = get_tools_config()
    reports_dir = config.reports_dir
    
    if not reports_dir.exists():
        return {
            "detection_status": "no_history",
            "total_analyses": 0,
            "analyses": []
        }
    
    # æ‰«ææ—¶é—´æˆ³æ ¼å¼çš„å½’æ¡£æ–‡ä»¶å¤¹
    analyses = []
    for item in reports_dir.iterdir():
        if item.is_dir() and not item.name.startswith('.') and item.name != "latest":
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ—¶é—´æˆ³æ ¼å¼ (YYYYMMDD_HHMMSS)
            if len(item.name) == 15 and item.name[8] == '_':
                try:
                    timestamp = datetime.strptime(item.name, "%Y%m%d_%H%M%S")
                    
                    # æ£€æŸ¥å½’æ¡£å†…å®¹
                    analysis_info = {
                        "timestamp_str": item.name,
                        "timestamp": timestamp.timestamp(),
                        "path": str(item),
                        "files": {}
                    }
                    
                    # æ£€æŸ¥å„ç±»æ–‡ä»¶
                    for file_name in ["analysis_report.json", "analysis_summary.md", 
                                    "runtime_config.json", "execution_log.txt"]:
                        file_path = item / file_name
                        if file_path.exists():
                            analysis_info["files"][file_name] = {
                                "exists": True,
                                "size_bytes": file_path.stat().st_size
                            }
                        else:
                            analysis_info["files"][file_name] = {"exists": False}
                    
                    # å°è¯•è¯»å–é…ç½®ä¿¡æ¯
                    runtime_config = item / "runtime_config.json"
                    if runtime_config.exists():
                        try:
                            with open(runtime_config, 'r', encoding='utf-8') as f:
                                config_data = json.load(f)
                            analysis_info["config"] = config_data.get("nextflow_params", {})
                        except Exception:
                            analysis_info["config"] = {}
                    
                    # è®¡ç®—æ€»å¤§å°
                    total_size = sum(f.stat().st_size for f in item.rglob("*") if f.is_file())
                    analysis_info["total_size_bytes"] = total_size
                    
                    analyses.append(analysis_info)
                except ValueError:
                    continue
    
    # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰
    analyses.sort(key=lambda x: x["timestamp"], reverse=True)
    
    return {
        "detection_status": "success",
        "total_analyses": len(analyses),
        "latest_analysis": analyses[0] if analyses else None,
        "analyses": analyses
    }


@tool
def check_tool_availability(tool_name: str) -> Dict[str, Any]:
    """æ£€æµ‹ç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·çš„å¯ç”¨æ€§
    
    Args:
        tool_name: å·¥å…·åç§° (fastp, star, hisat2, featurecounts)
    """
    tool_configs = {
        "fastp": ("qc_env", ["fastp", "--version"]),
        "star": ("align_env", ["STAR", "--version"]), 
        "hisat2": ("align_env", ["hisat2", "--version"]),
        "featurecounts": ("quant_env", ["featureCounts", "-v"])
    }
    
    if tool_name.lower() not in tool_configs:
        return {
            "tool_name": tool_name,
            "error": f"æœªçŸ¥å·¥å…·: {tool_name}",
            "available_tools": list(tool_configs.keys()),
            "available": False
        }
    
    env_name, cmd = tool_configs[tool_name.lower()]
    
    # æ‰§è¡Œå·¥å…·æ£€æµ‹
    detection_data = {
        "tool_name": tool_name,
        "environment": env_name,
        "command": cmd,
        "timestamp": time.time()
    }
    
    try:
        # æ‰§è¡Œç‰ˆæœ¬æ£€æµ‹å‘½ä»¤
        full_cmd = ['micromamba', 'run', '-n', env_name] + cmd
        result = subprocess.run(full_cmd, capture_output=True, text=True, timeout=15)
        
        detection_data.update({
            "command_executed": True,
            "return_code": result.returncode,
            "stdout": result.stdout,
            "stderr": result.stderr,
            "available": result.returncode == 0
        })
            
    except subprocess.TimeoutExpired:
        detection_data.update({
            "command_executed": False,
            "error": "timeout",
            "timeout_seconds": 15,
            "available": False
        })
    except FileNotFoundError:
        detection_data.update({
            "command_executed": False,
            "error": "micromamba_not_found",
            "available": False
        })
    except Exception as e:
        detection_data.update({
            "command_executed": False,
            "error": str(e),
            "available": False
        })
    
    return detection_data


@tool
def add_genome_config(user_input: str) -> Dict[str, Any]:
    """æ™ºèƒ½è§£æç”¨æˆ·è¾“å…¥å¹¶æ·»åŠ åŸºå› ç»„é…ç½®
    
    Args:
        user_input: åŒ…å«åŸºå› ç»„ä¿¡æ¯å’ŒURLçš„ç”¨æˆ·è¾“å…¥
    """
    try:
        if not user_input.strip():
            return {
                "success": False,
                "error": "è¯·æä¾›åŸºå› ç»„ä¿¡æ¯ï¼ŒåŒ…å«FASTAå’ŒGTFæ–‡ä»¶çš„URL"
            }
        
        # ä½¿ç”¨LLMè§£æç”¨æˆ·è¾“å…¥
        from .core import get_shared_llm
        
        llm = get_shared_llm()
        
        system_prompt = """ä½ æ˜¯åŸºå› ç»„ä¿¡æ¯è§£æä¸“å®¶ã€‚ç”¨æˆ·ä¼šæä¾›åŒ…å«FASTAå’ŒGTFæ–‡ä»¶URLçš„æ–‡æœ¬ï¼Œä½ éœ€è¦æå–ä¿¡æ¯å¹¶è¿”å›JSONæ ¼å¼ï¼š

{
  "genome_id": "åŸºå› ç»„æ ‡è¯†ç¬¦(å¦‚hg38,mm39,ce11)",
  "species": "ç‰©ç§åç§°(å¦‚human,mouse,caenorhabditis_elegans)", 
  "version": "ç‰ˆæœ¬å·(é€šå¸¸ä¸genome_idç›¸åŒ)",
  "fasta_url": "FASTAæ–‡ä»¶å®Œæ•´URL",
  "gtf_url": "GTFæ–‡ä»¶å®Œæ•´URL"
}

è¦æ±‚ï¼š
1. ä»URLä¸­æ™ºèƒ½è¯†åˆ«åŸºå› ç»„ç‰ˆæœ¬å’Œç‰©ç§
2. å¸¸è§æ˜ å°„ï¼šhg->human, mm->mouse, ce->caenorhabditis_elegans, dm->drosophila, rn->rat
3. å¦‚æœæ— æ³•ç¡®å®šï¼Œæ ¹æ®ç”Ÿç‰©ä¿¡æ¯å­¦å¸¸è¯†åˆç†æ¨æ–­
4. åªè¿”å›æœ‰æ•ˆçš„JSONï¼Œä¸è¦å…¶ä»–è§£é‡Š"""

        # è§£æç”¨æˆ·è¾“å…¥
        response = llm.invoke(f"System: {system_prompt}\n\nHuman: è¯·è§£æï¼š{user_input}")
        parsed_content = str(response.content).strip()
        
        # æå–JSONå†…å®¹
        try:
            genome_info = json.loads(parsed_content)
        except json.JSONDecodeError:
            json_match = re.search(r'\{.*\}', parsed_content, re.DOTALL)
            if not json_match:
                return {"success": False, "error": f"LLMè§£æå¤±è´¥ï¼š{parsed_content}"}
            genome_info = json.loads(json_match.group())
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        required_fields = ['genome_id', 'species', 'version', 'fasta_url', 'gtf_url']
        missing_fields = [field for field in required_fields if not genome_info.get(field)]
        if missing_fields:
            return {"success": False, "error": f"ç¼ºå°‘å­—æ®µï¼š{missing_fields}"}
        
        # æ„å»ºé…ç½®
        genome_id = genome_info['genome_id']
        species = genome_info['species']
        version = genome_info['version']
        
        fasta_path = f"genomes/{species}/{version}/{version}.fa"
        gtf_path = f"genomes/{species}/{version}/{version}.gtf"
        
        new_genome_config = {
            "species": species,
            "version": version,
            "fasta_path": fasta_path,
            "gtf_path": gtf_path,
            "fasta_url": genome_info['fasta_url'],
            "gtf_url": genome_info['gtf_url']
        }
        
        # è¯»å–ç°æœ‰é…ç½®å¹¶æ·»åŠ 
        config = get_tools_config()
        genomes_file = config.genomes_config_path
        
        if genomes_file.exists():
            with open(genomes_file, 'r', encoding='utf-8') as f:
                genomes_data = json.load(f)
        else:
            genomes_data = {}
        
        # æ£€æŸ¥é‡å¤
        if genome_id in genomes_data:
            return {
                "success": False,
                "error": f"åŸºå› ç»„ {genome_id} å·²å­˜åœ¨",
                "existing_config": genomes_data[genome_id]
            }
        
        # ä¿å­˜é…ç½®
        genomes_data[genome_id] = new_genome_config
        config.path_manager.ensure_directory(config.settings.config_dir)
        
        with open(genomes_file, 'w', encoding='utf-8') as f:
            json.dump(genomes_data, f, indent=2, ensure_ascii=False)
        
        return {
            "success": True,
            "genome_id": genome_id,
            "config": new_genome_config,
            "message": f"æˆåŠŸæ·»åŠ åŸºå› ç»„é…ç½®ï¼š{genome_id}"
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"æ·»åŠ åŸºå› ç»„é…ç½®æ—¶å‡ºé”™ï¼š{str(e)}"
        }


@tool
def get_help() -> Dict[str, Any]:
    """è·å–ç³»ç»Ÿå¸®åŠ©ä¿¡æ¯å’ŒåŠŸèƒ½è¯´æ˜"""
    return {
        "system_name": "RNA-seqæ™ºèƒ½åˆ†æåŠ©æ‰‹",
        "current_mode": "Normalæ¨¡å¼ (é¡¹ç›®ä¿¡æ¯ä¸­å¿ƒ)",
        "core_tools": [
            "é¡¹ç›®æ¦‚è§ˆ - ä¸€é”®æŸ¥çœ‹é¡¹ç›®å®Œæ•´çŠ¶æ€å’Œå¥åº·åº¦",
            "å†å²åˆ†æè®°å½• - æµè§ˆå·²å®Œæˆçš„åˆ†æå’Œå¯å¤ç”¨é…ç½®",
            "æŸ¥çœ‹FASTQæ–‡ä»¶ - è¯¦ç»†æ‰«ææ‰€æœ‰æµ‹åºæ•°æ®æ–‡ä»¶",
            "æŸ¥çœ‹åŸºå› ç»„ä¿¡æ¯ - æ˜¾ç¤ºå¯ç”¨å‚è€ƒåŸºå› ç»„çŠ¶æ€",
            "æ·»åŠ åŸºå› ç»„é…ç½® - æ™ºèƒ½è§£æå¹¶æ·»åŠ æ–°çš„å‚è€ƒåŸºå› ç»„"
        ],
        "next_steps": [
            "é¦–æ¬¡ä½¿ç”¨å»ºè®®è¿è¡Œ 'é¡¹ç›®æ¦‚è§ˆ' äº†è§£é¡¹ç›®çŠ¶æ€",
            "è¾“å…¥ '/plan' è¿›å…¥è®¡åˆ’æ¨¡å¼è¿›è¡Œæ·±åº¦åˆ†æè§„åˆ’"
        ],
        "mode_description": "Normalæ¨¡å¼ä¸“æ³¨å¿«é€Ÿä¿¡æ¯æŸ¥çœ‹å’Œé¡¹ç›®æ¦‚è§ˆï¼ŒPlanæ¨¡å¼è´Ÿè´£æ·±åº¦æ£€æµ‹å’Œåˆ†ææ–¹æ¡ˆåˆ¶å®š"
    }


# ==================== FastPä¸“ç”¨å·¥å…·å‡½æ•° ====================

@tool
def run_nextflow_fastp(fastp_params: Dict[str, Any], sample_info: Dict[str, Any]) -> Dict[str, Any]:
    """æ‰§è¡ŒNextflow FastPè´¨é‡æ§åˆ¶æµç¨‹
    
    Args:
        fastp_params: FastPå‚æ•°å­—å…¸ï¼Œä¾‹å¦‚ {"qualified_quality_phred": 25, "length_required": 50}
        sample_info: æ ·æœ¬ä¿¡æ¯ï¼ŒåŒ…å«sample_groupsç­‰
    
    Returns:
        æ‰§è¡Œç»“æœå­—å…¸ï¼ŒåŒ…å«çŠ¶æ€ã€è¾“å‡ºè·¯å¾„ã€æ‰§è¡Œæ—¥å¿—ç­‰
    """
    try:
        config = get_tools_config()
        
        # éªŒè¯å¿…éœ€å‚æ•°
        if not fastp_params:
            return {
                "success": False,
                "error": "FastPå‚æ•°ä¸èƒ½ä¸ºç©º",
                "execution_time": 0
            }
        
        if not sample_info.get("sample_groups"):
            return {
                "success": False, 
                "error": "æ ·æœ¬ä¿¡æ¯ç¼ºå¤±",
                "execution_time": 0
            }
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # ç»Ÿä¸€æ•°æ®æ ¹ç›®å½•æ¥æºï¼šå§‹ç»ˆä»¥ Settings().data_dir ä¸ºå‡†ï¼Œä¸ä» sample_info è¯»å–
        base_data_path = str(config.settings.data_dir)

        # ç»“æœç›®å½•ï¼ˆè¿è¡Œæ ¹ç›®å½•ï¼‰ï¼šä¼˜å…ˆä½¿ç”¨ sample_info æä¾›ï¼›å¦åˆ™æŒ‰æ—¶é—´æˆ³ç”Ÿæˆåˆ° data/results/<timestamp>
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_dir = sample_info.get("results_dir") or str(config.settings.data_dir / "results" / f"{timestamp}")

        # å·¥ä½œç›®å½•ï¼šç»Ÿä¸€åˆ° /data/work
        run_id = results_dir.split('/')[-1] if '/' in str(results_dir) else timestamp
        work_dir = Path(base_data_path) / "work" / f"fastp_{run_id}"
        work_dir.mkdir(parents=True, exist_ok=True)
        results_dir = Path(results_dir)
        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨ï¼Œé¿å… publishDir ç›®æ ‡ä¸å­˜åœ¨é€ æˆçš„å‘å¸ƒå¤±è´¥
        try:
            results_dir.mkdir(parents=True, exist_ok=True)
            (results_dir / "fastp").mkdir(parents=True, exist_ok=True)

        except Exception:
            pass
        print(f"ğŸ“ è¿è¡Œç›®å½•: base={base_data_path} work={work_dir} results={results_dir}")
        
        nextflow_params: Dict[str, Any] = {}
        for raw_key, value in (fastp_params or {}).items():
            if value is None:
                continue
            key = str(raw_key)
            if key.startswith("--"):
                key = key[2:]
            if key == "thread":
                key = "threads"
            nextflow_params[key] = value
        
        # æ·»åŠ æ ·æœ¬ç»„ä¿¡æ¯
        nextflow_params["sample_groups"] = sample_info["sample_groups"]
        
        # è®¾ç½®ç»“æœç›®å½•å’Œæ•°æ®è·¯å¾„
        nextflow_params["results_dir"] = str(results_dir)
        nextflow_params["data"] = base_data_path
        
        # åˆ›å»ºNextflowå‚æ•°æ–‡ä»¶ - ä¿å­˜åˆ°fastpå­ç›®å½•ä¸­
        fastp_dir = results_dir / "fastp"
        fastp_dir.mkdir(parents=True, exist_ok=True)
        params_file = fastp_dir / "fastp_params.json"
        with open(params_file, 'w', encoding='utf-8') as f:
            json.dump(nextflow_params, f, indent=2, ensure_ascii=False)
        
        # æ„å»ºNextflowå‘½ä»¤ï¼ˆå…¼å®¹Dockerä¸æœ¬åœ°è·¯å¾„ï¼‰
        # 1) ä¼˜å…ˆä½¿ç”¨src/nextflow/ç›®å½•ä¸‹çš„ fastp.nfï¼ˆæœ¬åœ°å¼€å‘ï¼‰
        # 2) Dockeré•œåƒä¸­ fastp.nf ä½äº /src/nextflow/ï¼ˆè§ Dockerfile COPYï¼‰
        nf_candidates = [
            config.settings.project_root / "src" / "nextflow" / "fastp.nf",
            Path("/src/nextflow/fastp.nf")
        ]
        nextflow_script = None
        for cand in nf_candidates:
            if cand.exists():
                nextflow_script = cand
                break
        if nextflow_script is None:
            return {
                "success": False,
                "error": "æœªæ‰¾åˆ° fastp.nf è„šæœ¬ï¼Œè¯·æ£€æŸ¥å®¹å™¨å†…æ˜¯å¦å­˜åœ¨ /src/nextflow/fastp.nf æˆ–æœ¬åœ°src/nextflowç›®å½•",
                "searched": [str(p) for p in nf_candidates]
            }

        cmd = [
            "nextflow", "run",
            str(nextflow_script),
            "-params-file", str(params_file),
            "-work-dir", str(work_dir),
            "--data", str(config.settings.data_dir)
        ]
        
        print(f"ğŸš€ æ‰§è¡ŒNextflow FastPæµæ°´çº¿...")
        print(f"   å‚æ•°æ–‡ä»¶: {params_file}")
        # æ­£ç¡®æ˜¾ç¤ºå¹¶ä½¿ç”¨æœ¬å‡½æ•°åˆ›å»ºçš„ Nextflow å·¥ä½œç›®å½•
        print(f"   å·¥ä½œç›®å½•: {work_dir}")
        print(f"   ç»“æœç›®å½•: {results_dir}")
        
        # æ‰§è¡ŒNextflowæµæ°´çº¿
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=1800,  # 30åˆ†é’Ÿè¶…æ—¶
            cwd=config.settings.project_root
        )
        
        execution_time = time.time() - start_time
        
        if result.returncode == 0:
            # è§£æè¾“å‡ºç»“æœ
            sample_groups = sample_info.get("sample_groups", [])
            sample_count = len(sample_groups)
            
            # åŸºäºçº¦å®šçš„å‘å¸ƒç›®å½•ç»“æ„ï¼Œæ„é€ æ¯ä¸ªæ ·æœ¬çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
            per_sample_outputs = []
            fastp_root = results_dir / "fastp"
            for item in sample_groups:
                sid = item.get("sample_id") or item.get("id")
                if not sid:
                    continue
                sdir = fastp_root / sid
                out = {
                    "sample_id": sid,
                    "html": str(sdir / f"{sid}.fastp.html"),
                    "json": str(sdir / f"{sid}.fastp.json"),
                }
                # åˆ¤æ–­å•åŒç«¯
                r2 = item.get("read2")
                if r2:
                    out.update({
                        "trimmed_r1": str(sdir / f"{sid}_1.trimmed.fastq.gz"),
                        "trimmed_r2": str(sdir / f"{sid}_2.trimmed.fastq.gz"),
                        "is_paired": True,
                    })
                else:
                    out.update({
                        "trimmed_single": str(sdir / f"{sid}.single.trimmed.fastq.gz"),
                        "is_paired": False,
                    })
                per_sample_outputs.append(out)
            
            payload = {
                "success": True,
                "message": f"FastPè´¨æ§å®Œæˆï¼Œå¤„ç†äº†{sample_count}ä¸ªæ ·æœ¬",
                "execution_time": execution_time,
                "results_dir": str(results_dir),
                # è¿”å›æ­£ç¡®çš„ Nextflow å·¥ä½œç›®å½•ï¼Œä¾¿äºç”¨æˆ·æ’æŸ¥
                "work_dir": str(work_dir),
                "params_file": str(params_file),
                "sample_count": sample_count,
                "per_sample_outputs": per_sample_outputs
            }
            # ä»…åœ¨è°ƒè¯•æ¨¡å¼è¿”å›è¯¦ç»†æ—¥å¿—
            try:
                if get_tools_config().settings.debug_mode:
                    payload.update({
                        "stdout": result.stdout,
                        "stderr": result.stderr,
                        "nextflow_params": nextflow_params
                    })
            except Exception:
                pass
            return payload
        else:
            payload = {
                "success": False,
                "error": f"Nextflowæ‰§è¡Œå¤±è´¥ (è¿”å›ç : {result.returncode})",
                "execution_time": execution_time,
            }
            try:
                if get_tools_config().settings.debug_mode:
                    payload.update({
                        "stdout": result.stdout,
                        "stderr": result.stderr,
                        "cmd": " ".join(cmd)
                    })
            except Exception:
                pass
            return payload
            
    except subprocess.TimeoutExpired:
        payload = {
            "success": False,
            "error": "Nextflowæ‰§è¡Œè¶…æ—¶ï¼ˆ30åˆ†é’Ÿï¼‰",
            "execution_time": time.time() - start_time
        }
        return payload
    except Exception as e:
        payload = {
            "success": False,
            "error": f"æ‰§è¡ŒFastPæµæ°´çº¿æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
            "execution_time": 0
        }
        return payload


@tool  
def parse_fastp_results(results_directory: str) -> Dict[str, Any]:
    """è§£æFastPç»“æœæ–‡ä»¶ï¼Œæå–å®¢è§‚è´¨é‡æŒ‡æ ‡ä¾›LLMåˆ†æ
    
    Args:
        results_directory: FastPç»“æœç›®å½•è·¯å¾„
    
    Returns:
        è§£æçš„è´¨é‡æŒ‡æ ‡å­—å…¸ï¼ŒåŒ…å«å„æ ·æœ¬çš„è´¨é‡ç»Ÿè®¡ã€è¿‡æ»¤ç‡ç­‰å®¢è§‚æ•°æ®
        æ³¨æ„ï¼šæ­¤å·¥å…·ä»…æä¾›å®¢è§‚æ•°æ®åˆ†æï¼Œä¸ç”Ÿæˆä¼˜åŒ–å»ºè®®ã€‚ä¼˜åŒ–å»ºè®®ç”±LLMåŸºäºè¿™äº›æ•°æ®æ™ºèƒ½ç”Ÿæˆã€‚
    """
    try:
        results_dir = Path(results_directory)
        if not results_dir.exists():
            return {
                "success": False,
                "error": f"ç»“æœç›®å½•ä¸å­˜åœ¨: {results_directory}"
            }
        
        # æŸ¥æ‰¾æ‰€æœ‰FastP JSONæŠ¥å‘Šæ–‡ä»¶
        json_files = list(results_dir.rglob("*.fastp.json"))
        
        if not json_files:
            return {
                "success": False,
                "error": "æœªæ‰¾åˆ°FastP JSONæŠ¥å‘Šæ–‡ä»¶"
            }
        
        sample_metrics = []
        overall_stats = {
            "total_reads_before": 0,
            "total_reads_after": 0,
            "total_bases_before": 0,
            "total_bases_after": 0,
            "q20_rates": [],
            "q30_rates": [],
            "gc_contents": []
        }
        
        # è§£ææ¯ä¸ªæ ·æœ¬çš„JSONæŠ¥å‘Š
        for json_file in json_files:
            try:
                with open(json_file, 'r') as f:
                    fastp_data = json.load(f)
                
                # æå–æ ·æœ¬IDï¼ˆä»æ–‡ä»¶åï¼‰
                sample_id = json_file.stem.replace('.fastp', '')
                
                # æå–è´¨é‡æŒ‡æ ‡
                summary = fastp_data.get("summary", {})
                before_filtering = summary.get("before_filtering", {})
                after_filtering = summary.get("after_filtering", {})
                
                # è¯»å–æ•°å’Œç¢±åŸºæ•°
                reads_before = before_filtering.get("total_reads", 0)
                reads_after = after_filtering.get("total_reads", 0)
                bases_before = before_filtering.get("total_bases", 0)
                bases_after = after_filtering.get("total_bases", 0)
                
                # è´¨é‡æŒ‡æ ‡
                q20_before = before_filtering.get("q20_rate", 0)
                q20_after = after_filtering.get("q20_rate", 0)
                q30_before = before_filtering.get("q30_rate", 0)
                q30_after = after_filtering.get("q30_rate", 0)
                
                # GCå«é‡
                gc_before = before_filtering.get("gc_content", 0)
                gc_after = after_filtering.get("gc_content", 0)
                
                # è¿‡æ»¤ç‡è®¡ç®—
                read_pass_rate = reads_after / reads_before if reads_before > 0 else 0
                base_pass_rate = bases_after / bases_before if bases_before > 0 else 0
                
                # å¹³å‡é•¿åº¦
                avg_length_before = bases_before / reads_before if reads_before > 0 else 0
                avg_length_after = bases_after / reads_after if reads_after > 0 else 0
                
                sample_metric = {
                    "sample_id": sample_id,
                    "json_file": str(json_file),
                    "reads_before": reads_before,
                    "reads_after": reads_after,
                    "bases_before": bases_before,
                    "bases_after": bases_after,
                    "read_pass_rate": round(read_pass_rate, 4),
                    "base_pass_rate": round(base_pass_rate, 4),
                    "q20_before": round(q20_before, 4),
                    "q20_after": round(q20_after, 4),
                    "q30_before": round(q30_before, 4),
                    "q30_after": round(q30_after, 4),
                    "gc_content_before": round(gc_before, 4),
                    "gc_content_after": round(gc_after, 4),
                    "avg_length_before": round(avg_length_before, 1),
                    "avg_length_after": round(avg_length_after, 1),
                    "quality_improvement": {
                        "q20_improvement": round(q20_after - q20_before, 4),
                        "q30_improvement": round(q30_after - q30_before, 4)
                    }
                }
                
                sample_metrics.append(sample_metric)
                
                # ç´¯ç§¯æ€»ä½“ç»Ÿè®¡
                overall_stats["total_reads_before"] += reads_before
                overall_stats["total_reads_after"] += reads_after
                overall_stats["total_bases_before"] += bases_before
                overall_stats["total_bases_after"] += bases_after
                overall_stats["q20_rates"].append(q20_after)
                overall_stats["q30_rates"].append(q30_after)
                overall_stats["gc_contents"].append(gc_after)
                
            except Exception as e:
                sample_metrics.append({
                    "sample_id": json_file.stem.replace('.fastp', ''),
                    "json_file": str(json_file),
                    "error": f"è§£æå¤±è´¥: {str(e)}"
                })
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        total_samples = len([m for m in sample_metrics if "error" not in m])
        if total_samples > 0:
            overall_read_pass_rate = overall_stats["total_reads_after"] / overall_stats["total_reads_before"]
            overall_base_pass_rate = overall_stats["total_bases_after"] / overall_stats["total_bases_before"]
            avg_q20_rate = sum(overall_stats["q20_rates"]) / len(overall_stats["q20_rates"])
            avg_q30_rate = sum(overall_stats["q30_rates"]) / len(overall_stats["q30_rates"])
            avg_gc_content = sum(overall_stats["gc_contents"]) / len(overall_stats["gc_contents"])
        else:
            overall_read_pass_rate = 0
            overall_base_pass_rate = 0
            avg_q20_rate = 0
            avg_q30_rate = 0
            avg_gc_content = 0
        
        # è´¨é‡è¯„ä¼°ï¼ˆä»…æä¾›å®¢è§‚æŒ‡æ ‡ï¼Œä¸ç”Ÿæˆä¼˜åŒ–å»ºè®®ï¼‰
        quality_assessment = {
            "overall_quality": "good" if avg_q30_rate > 0.85 else "moderate" if avg_q30_rate > 0.7 else "poor",
            "pass_rate_status": "good" if overall_read_pass_rate > 0.8 else "moderate" if overall_read_pass_rate > 0.6 else "poor"
        }
        
        return {
            "success": True,
            "total_samples": total_samples,
            "results_directory": results_directory,
            "sample_metrics": sample_metrics,
            "overall_statistics": {
                "total_reads_before": overall_stats["total_reads_before"],
                "total_reads_after": overall_stats["total_reads_after"],
                "total_bases_before": overall_stats["total_bases_before"],
                "total_bases_after": overall_stats["total_bases_after"],
                "overall_read_pass_rate": round(overall_read_pass_rate, 4),
                "overall_base_pass_rate": round(overall_base_pass_rate, 4),
                "average_q20_rate": round(avg_q20_rate, 4),
                "average_q30_rate": round(avg_q30_rate, 4),
                "average_gc_content": round(avg_gc_content, 4)
            },
            "quality_assessment": quality_assessment
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"è§£æFastPç»“æœå¤±è´¥: {str(e)}"
        }


# ==================== STARå·¥å…·å‡½æ•° ====================

@tool
def download_genome_assets(genome_id: str, force: bool = False) -> Dict[str, Any]:
    """ä¸‹è½½æŒ‡å®šåŸºå› ç»„çš„FASTAå’ŒGTFæ–‡ä»¶
    
    Args:
        genome_id: åŸºå› ç»„æ ‡è¯†ï¼ˆå¦‚"hg38", "mm39"ï¼‰ï¼Œç”¨äºåœ¨genomes.jsonä¸­æŸ¥è¯¢ä¸‹è½½ä¿¡æ¯
        force: è‹¥ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ˜¯å¦å¼ºåˆ¶è¦†ç›–ä¸‹è½½ï¼ˆé»˜è®¤å¦ï¼‰
    
    Returns:
        Dict: ä¸‹è½½ç»“æœä¿¡æ¯
        {
            "success": bool,
            "fasta_path": str,      # ä¸‹è½½åçš„FASTAæ–‡ä»¶è·¯å¾„
            "gtf_path": str,        # ä¸‹è½½åçš„GTFæ–‡ä»¶è·¯å¾„
            "downloaded": List[str], # å®é™…ä¸‹è½½çš„æ–‡ä»¶åˆ—è¡¨
            "skipped": List[str],   # è·³è¿‡çš„æ–‡ä»¶åˆ—è¡¨
            "errors": List[str]     # é”™è¯¯ä¿¡æ¯åˆ—è¡¨
        }
    """
    try:
        tools_config = get_tools_config()
        
        # è¯»å–åŸºå› ç»„é…ç½®
        genomes_config_path = tools_config.genomes_config_path
        if not genomes_config_path.exists():
            return {
                "success": False,
                "error": f"åŸºå› ç»„é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {genomes_config_path}"
            }
        
        with open(genomes_config_path, 'r', encoding='utf-8') as f:
            genomes_config = json.load(f)
        
        if genome_id not in genomes_config:
            return {
                "success": False,
                "error": f"åŸºå› ç»„ '{genome_id}' åœ¨é…ç½®æ–‡ä»¶ä¸­ä¸å­˜åœ¨"
            }
        
        config = genomes_config[genome_id]
        fasta_url = config.get("fasta_url")
        gtf_url = config.get("gtf_url")
        fasta_relative = config.get("fasta_path")
        gtf_relative = config.get("gtf_path")
        
        if not all([fasta_url, gtf_url, fasta_relative, gtf_relative]):
            return {
                "success": False,
                "error": f"åŸºå› ç»„ '{genome_id}' é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘å¿…è¦çš„URLæˆ–è·¯å¾„ä¿¡æ¯"
            }
        
        # æ„å»ºç»å¯¹è·¯å¾„
        project_root = tools_config.settings.project_root
        fasta_path = project_root / fasta_relative
        gtf_path = project_root / gtf_relative
        
        downloaded = []
        skipped = []
        errors = []
        
        # åˆ›å»ºç›®å½•
        fasta_path.parent.mkdir(parents=True, exist_ok=True)
        gtf_path.parent.mkdir(parents=True, exist_ok=True)
        
        def download_file(url: str, target_path: Path, file_type: str) -> bool:
            """ä¸‹è½½å•ä¸ªæ–‡ä»¶çš„è¾…åŠ©å‡½æ•°"""
            try:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                if target_path.exists() and target_path.stat().st_size > 0 and not force:
                    skipped.append(f"{file_type}: {target_path}")
                    return True
                
                # ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶
                temp_path = target_path.with_suffix(target_path.suffix + '.part')
                
                # æ„å»ºcurlä¸‹è½½å‘½ä»¤
                cmd = [
                    "curl", "-L", "-fS", "--retry", "5", 
                    "--retry-delay", "5", "--retry-connrefused",
                    "-C", "-",  # æ–­ç‚¹ç»­ä¼ 
                    "-o", str(temp_path),
                    url
                ]
                
                print(f"ä¸‹è½½ {file_type}...")
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)  # 1å°æ—¶è¶…æ—¶
                
                if result.returncode != 0:
                    errors.append(f"{file_type}ä¸‹è½½å¤±è´¥: {result.stderr}")
                    return False
                
                # è§£å‹æ–‡ä»¶ï¼ˆå¦‚æœæ˜¯.gzæ ¼å¼ï¼‰
                if temp_path.suffix == '.gz':
                    # ä½¿ç”¨pigzä¼˜å…ˆï¼Œfallbackåˆ°gzip
                    decompress_cmd = ["pigz", "-d", "-c", str(temp_path)]
                    decompress_result = subprocess.run(decompress_cmd, capture_output=True, timeout=1800)
                    
                    if decompress_result.returncode != 0:
                        # fallbackåˆ°gzip
                        decompress_cmd = ["gzip", "-d", "-c", str(temp_path)]
                        decompress_result = subprocess.run(decompress_cmd, capture_output=True, timeout=1800)
                        
                        if decompress_result.returncode != 0:
                            errors.append(f"{file_type}è§£å‹å¤±è´¥")
                            return False
                    
                    # å†™å…¥è§£å‹å†…å®¹åˆ°æœ€ç»ˆæ–‡ä»¶
                    with open(target_path, 'wb') as f:
                        f.write(decompress_result.stdout)
                    
                    # åˆ é™¤ä¸´æ—¶å‹ç¼©æ–‡ä»¶
                    temp_path.unlink()
                else:
                    # ç›´æ¥é‡å‘½å
                    temp_path.rename(target_path)
                
                downloaded.append(f"{file_type}: {target_path}")
                return True
                
            except subprocess.TimeoutExpired:
                errors.append(f"{file_type}ä¸‹è½½è¶…æ—¶")
                return False
            except Exception as e:
                errors.append(f"{file_type}ä¸‹è½½å¼‚å¸¸: {str(e)}")
                return False
        
        # å¹¶è¡Œä¸‹è½½FASTAå’ŒGTFï¼ˆä½¿ç”¨ç®€å•çš„ä¸²è¡Œå®ç°ï¼Œå¯ä»¥åç»­ä¼˜åŒ–ä¸ºçœŸæ­£çš„å¹¶è¡Œï¼‰
        fasta_success = download_file(fasta_url, fasta_path, "FASTA")
        gtf_success = download_file(gtf_url, gtf_path, "GTF")
        
        success = fasta_success and gtf_success
        
        return {
            "success": success,
            "fasta_path": str(fasta_path),
            "gtf_path": str(gtf_path),
            "downloaded": downloaded,
            "skipped": skipped,
            "errors": errors
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"ä¸‹è½½åŸºå› ç»„èµ„æºå¤±è´¥: {str(e)}"
        }


@tool
def build_star_index(
    genome_id: str,
    sjdb_overhang: Optional[int] = None,
    runThreadN: Optional[int] = None,
    force_rebuild: bool = False,
    results_dir: Optional[str] = None,
) -> Dict[str, Any]:
    """æ„å»ºSTARåŸºå› ç»„ç´¢å¼•ï¼ˆä½¿ç”¨ build_index.nfï¼‰
    
    ä¸»è¦åœ¨RNA-seqåˆ†ææµç¨‹ä¸­è‡ªåŠ¨è°ƒç”¨ï¼Œå½“ç³»ç»Ÿæ£€æµ‹åˆ°ç¼ºå°‘STARç´¢å¼•æ—¶è§¦å‘ã€‚
    å‚æ•°æ–‡ä»¶ç»Ÿä¸€ä¿å­˜åˆ°results/starç›®å½•ï¼Œä¸å…¶ä»–STARç›¸å…³æ–‡ä»¶æ”¾åœ¨ä¸€èµ·ã€‚

    Args:
        genome_id: åŸºå› ç»„æ ‡è¯†ï¼Œç”¨äºå®šä½FASTAå’ŒGTFæ–‡ä»¶
        sjdb_overhang: å‰ªæ¥ä½ç‚¹overhangï¼ˆé»˜è®¤ç”¨ DEFAULT_STAR_PARAMS æˆ– 100ï¼‰
        runThreadN: çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ç”¨ DEFAULT_STAR_PARAMS æˆ– 4ï¼‰
        force_rebuild: è‹¥ç´¢å¼•ç›®å½•å·²å­˜åœ¨æ˜¯å¦å¼ºåˆ¶é‡å»º
        results_dir: åˆ†æç»“æœç›®å½•ï¼Œå‚æ•°æ–‡ä»¶ä¿å­˜åˆ°results/star/å­ç›®å½•

    Returns:
        Dict: æ‰§è¡Œç»“æœï¼ˆåŒ…å« index_dir/stdout/stderr/skipped ç­‰ï¼‰
    """
    try:
        from .config.default_tool_params import DEFAULT_STAR_PARAMS
        tools_config = get_tools_config()

        # è¯»å–åŸºå› ç»„é…ç½®
        genomes_config_path = tools_config.genomes_config_path
        if not genomes_config_path.exists():
            return {"success": False, "error": f"åŸºå› ç»„é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {genomes_config_path}"}

        with open(genomes_config_path, 'r', encoding='utf-8') as f:
            genomes_config = json.load(f)
        if genome_id not in genomes_config:
            return {"success": False, "error": f"åŸºå› ç»„ '{genome_id}' åœ¨é…ç½®æ–‡ä»¶ä¸­ä¸å­˜åœ¨"}

        cfg = genomes_config[genome_id]
        fasta_rel = cfg.get("fasta_path")
        gtf_rel = cfg.get("gtf_path")
        if not fasta_rel or not gtf_rel:
            return {"success": False, "error": f"åŸºå› ç»„ '{genome_id}' ç¼ºå°‘ fasta_path æˆ– gtf_path"}

        project_root = tools_config.settings.project_root
        fasta_path = project_root / fasta_rel
        gtf_path = project_root / gtf_rel
        if not fasta_path.exists():
            return {"success": False, "error": f"FASTAæ–‡ä»¶ä¸å­˜åœ¨: {fasta_path}"}
        if not gtf_path.exists():
            return {"success": False, "error": f"GTFæ–‡ä»¶ä¸å­˜åœ¨: {gtf_path}"}

        # ç›®æ ‡ç´¢å¼•ç›®å½•
        index_dir = tools_config.get_star_index_dir(fasta_path)

        # å­˜åœ¨å³è·³è¿‡
        if index_dir.exists() and not force_rebuild:
            key_files = ["SA", "SAindex", "Genome"]
            if all((index_dir / f).exists() for f in key_files):
                return {
                    "success": True,
                    "index_dir": str(index_dir),
                    "skipped": True,
                    "message": "STARç´¢å¼•å·²å­˜åœ¨ï¼Œè·³è¿‡æ„å»º",
                }

        # æ„é€  Nextflow å‚æ•°
        sjdb = (
            int(sjdb_overhang)
            if sjdb_overhang is not None
            else int(DEFAULT_STAR_PARAMS.get("sjdbOverhang") or 100)
        )
        threads = (
            int(runThreadN)
            if runThreadN is not None
            else int(DEFAULT_STAR_PARAMS.get("runThreadN", 4))
        )

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        temp_dir = Path(tools_config.settings.data_dir) / "tmp" / f"nextflow_star_index_{timestamp}"
        temp_dir.mkdir(parents=True, exist_ok=True)

        nf_params = {
            "genome_fasta": str(fasta_path),
            "genome_gtf": str(gtf_path),
            "star_index_dir": str(index_dir),
            "sjdb_overhang": sjdb,
            "runThreadN": threads,
            "limitGenomeGenerateRAM": 32000000000,
        }

        # åˆ›å»ºå‚æ•°æ–‡ä»¶ - ç»Ÿä¸€ä¿å­˜åˆ°results/starç›®å½•ï¼ˆä¸star_paramsåœ¨ä¸€èµ·ï¼‰
        if results_dir:
            results_path = Path(results_dir)
            star_subdir = results_path / "star"
            star_subdir.mkdir(parents=True, exist_ok=True)
            params_file = star_subdir / "build_index_params.json"
        else:
            # ç½•è§çš„ç‹¬ç«‹ä½¿ç”¨æƒ…å†µï¼Œä»ç„¶ä¿å­˜åˆ°åŸºå› ç»„starç›®å½•
            star_dir = index_dir.parent / "star"
            star_dir.mkdir(parents=True, exist_ok=True)
            params_file = star_dir / f"build_index_params_{timestamp}.json"
        with open(params_file, "w", encoding="utf-8") as f:
            json.dump(nf_params, f, indent=2, ensure_ascii=False)

        # å®šä½ build_index.nf
        nf_candidates = [
            tools_config.settings.project_root / "src" / "nextflow" / "build_index.nf",
            Path("/src/nextflow/build_index.nf"),
        ]
        nextflow_script = None
        for cand in nf_candidates:
            if cand.exists():
                nextflow_script = cand
                break
        if nextflow_script is None:
            return {
                "success": False,
                "error": "æœªæ‰¾åˆ° build_index.nf è„šæœ¬ï¼Œè¯·æ£€æŸ¥ /src/nextflow/build_index.nf è·¯å¾„",
                "searched": [str(p) for p in nf_candidates],
            }

        # è¿è¡Œ Nextflow
        # ç»Ÿä¸€ Nextflow å·¥ä½œç›®å½•åˆ° /data/work
        work_root = tools_config.settings.data_dir / "work"
        work_dir = work_root / f"star_index_{timestamp}"
        work_dir.mkdir(parents=True, exist_ok=True)
        cmd = [
            "nextflow",
            "run",
            str(nextflow_script),
            "-params-file",
            str(params_file),
            "-work-dir",
            str(work_dir),
        ]

        print("ğŸš€ æ„å»ºSTARç´¢å¼• (Nextflow)â€¦")
        print(f"   å‚æ•°æ–‡ä»¶: {params_file}")
        print(f"   ç›®æ ‡ç›®å½•: {index_dir}")
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=7200,
            cwd=tools_config.settings.project_root,
        )

        payload = {
            "success": result.returncode == 0,
            "index_dir": str(index_dir),
            "skipped": False,
            "params_file": str(params_file),
        }
        try:
            if get_tools_config().settings.debug_mode:
                payload.update({
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                    "cmd": " ".join(cmd)
                })
        except Exception:
            pass
        return payload

    except subprocess.TimeoutExpired:
        return {"success": False, "error": "STARç´¢å¼•æ„å»ºè¶…æ—¶"}
    except Exception as e:
        return {"success": False, "error": f"æ„å»ºSTARç´¢å¼•å¤±è´¥: {str(e)}"}


@tool
def run_nextflow_star(
    star_params: Dict[str, Any],
    fastp_results: Dict[str, Any],
    genome_info: Dict[str, Any],
    results_timestamp: Optional[str] = None
) -> Dict[str, Any]:
    """æ‰§è¡Œ STAR æ¯”å¯¹ï¼ˆç²¾ç®€ç‰ˆï¼‰

    çº¦æŸï¼ˆä¸è·¯å¾„å¥‘çº¦ä¸€è‡´ï¼‰:
    - ä»…åœ¨ fastp_results.success ä¸ºçœŸä¸”åŒ…å« per_sample_outputs æ—¶æ”¾è¡Œ
    - ç»Ÿä¸€å¤ç”¨ FastP çš„ results_dir ä½œä¸ºè¿è¡Œæ ¹ç›®å½•
    - STAR ç´¢å¼•ä¼˜å…ˆä½¿ç”¨ genome_info.star_index_dirï¼›å¦åˆ™ç”± genome_info.fasta_path æ¨å¯¼
    - sample_inputs ä»…æ¥æºäº fastp_results.per_sample_outputsï¼ˆä¸å†æ‰«æç›®å½•ï¼‰
    - per_sample_outputs è·¯å¾„ä¸ star.nf äº§å‡ºä¸€è‡´ï¼ˆæ ·æœ¬å­ç›®å½• + é»˜è®¤æ–‡ä»¶åï¼‰
    """
    try:
        tools_config = get_tools_config()

        # 1) æ ¡éªŒ FastP ç»“æœä¸è¿è¡Œæ ¹ç›®å½•
        if not (fastp_results and fastp_results.get("success")):
            return {"success": False, "error": "FastPç»“æœæ— æ•ˆï¼Œæ— æ³•æ‰§è¡ŒSTARæ¯”å¯¹"}

        fastp_results_dir = fastp_results.get("results_dir")
        if not fastp_results_dir:
            return {"success": False, "error": "FastPç»“æœç¼ºå°‘results_dir"}

        per_sample = fastp_results.get("per_sample_outputs") or []
        if not per_sample:
            return {"success": False, "error": "FastPç»“æœç¼ºå°‘per_sample_outputs"}

        # 2) è¿è¡Œæ ¹ç›®å½•ä¸å·¥ä½œç›®å½•
        timestamp = results_timestamp or datetime.now().strftime("%Y%m%d_%H%M%S")
        results_dir = Path(fastp_results_dir)
        run_id = results_dir.name or timestamp
        work_dir = tools_config.settings.data_dir / "work" / f"star_{run_id}"
        results_dir.mkdir(parents=True, exist_ok=True)
        work_dir.mkdir(parents=True, exist_ok=True)

        # 3) è§£æ STAR ç´¢å¼•ç›®å½•
        def _resolve(p: str) -> Path:
            pp = Path(p)
            return pp if pp.is_absolute() else (tools_config.settings.project_root / pp)

        star_index_dir = ""
        if isinstance(genome_info, dict):
            star_index_dir = genome_info.get("star_index_dir") or genome_info.get("index_dir") or ""
            if not star_index_dir:
                fasta_path = genome_info.get("fasta_path") or genome_info.get("fasta")
                if fasta_path:
                    star_index_dir = str(tools_config.get_star_index_dir(_resolve(fasta_path)))

        if not star_index_dir:
            return {"success": False, "error": "ç¼ºå°‘STARç´¢å¼•ç›®å½•ï¼ˆgenome_info.star_index_dir æˆ– fasta_path å¿…é¡»æä¾›ï¼‰"}

        star_index_path = _resolve(star_index_dir)
        if not star_index_path.exists():
            return {"success": False, "error": f"STARç´¢å¼•ä¸å­˜åœ¨: {star_index_path}"}

        # 4) æ„é€  sample_inputsï¼ˆä»…ä½¿ç”¨ FastP è¿”å›çš„ç»“æ„ï¼‰
        sample_inputs: List[Dict[str, Any]] = []
        for i, fp in enumerate(per_sample):
            sid = fp.get("sample_id", f"sample_{i+1}")
            r1 = fp.get("trimmed_single") or fp.get("trimmed_r1")
            r2 = fp.get("trimmed_r2")
            if not r1:
                continue
            sample_inputs.append({
                "sample_id": sid,
                "is_paired": bool(r2),
                "read1": r1,
                **({"read2": r2} if r2 else {})
            })
        if not sample_inputs:
            return {"success": False, "error": "æœªä»FastPç»“æœæ„é€ åˆ°ä»»ä½•æ ·æœ¬è¾“å…¥"}

        # 5) ç»„è£… Nextflow å‚æ•°
        cleaned_params: Dict[str, Any] = {}
        for k, v in (star_params or {}).items():
            if v is None or k in {"star_cpus", "outFileNamePrefix"}:
                continue
            cleaned_params[k.lstrip('-')] = v

        nf_params = {
            "sample_inputs": json.dumps(sample_inputs, ensure_ascii=False),
            "star_index": str(star_index_path),
            "results_dir": str(results_dir),
            "work_dir": str(work_dir),
            **cleaned_params,
        }

        # ä¿å­˜å‚æ•°æ–‡ä»¶åˆ°starå­ç›®å½•
        star_dir = results_dir / "star"
        star_dir.mkdir(parents=True, exist_ok=True)
        params_file = star_dir / "star_params.json"
        with open(params_file, "w", encoding="utf-8") as f:
            json.dump(nf_params, f, indent=2, ensure_ascii=False)

        # 6) å®šä½å¹¶æ‰§è¡Œ Nextflow
        nf_candidates = [
            tools_config.settings.project_root / "src" / "nextflow" / "star.nf",
            Path("/src/nextflow/star.nf"),
        ]
        nextflow_script = next((p for p in nf_candidates if p.exists()), None)
        if nextflow_script is None:
            return {"success": False, "error": "æœªæ‰¾åˆ° star.nf è„šæœ¬ï¼Œè¯·æ£€æŸ¥ /src/nextflow/star.nf è·¯å¾„", "searched": [str(p) for p in nf_candidates]}

        print(f"æ‰§è¡ŒSTARæ¯”å¯¹ - å‚æ•°æ–‡ä»¶: {params_file}")
        print(f"STARç´¢å¼•: {nf_params['star_index']}")
        cmd = [
            "nextflow", "run", str(nextflow_script),
            "-params-file", str(params_file),
            "-work-dir", str(work_dir),
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=7200, cwd=tools_config.settings.project_root)

        # 7) ç»„è£…æ¯æ ·æœ¬è¾“å‡ºè·¯å¾„ï¼ˆä¸ star.nf publishDir å¯¹é½ï¼‰
        star_out = results_dir / "star"
        per_sample_outputs: List[Dict[str, Any]] = []
        for item in sample_inputs:
            sid = item["sample_id"]
            sdir = star_out / sid
            entry = {
                "sample_id": sid,
                "aligned_bam": str(sdir / f"{sid}.Aligned.sortedByCoord.out.bam"),
                "log_final": str(sdir / f"{sid}.Log.final.out"),
                "log_out": str(sdir / f"{sid}.Log.out"),
                "log_progress": str(sdir / f"{sid}.Log.progress.out"),
                "splice_junctions": str(sdir / f"{sid}.SJ.out.tab"),
            }
            qm = str(nf_params.get("quantMode", ""))
            if "TranscriptomeSAM" in qm:
                entry["transcriptome_bam"] = str(sdir / f"{sid}.Aligned.toTranscriptome.out.bam")
            if "GeneCounts" in qm:
                entry["gene_counts"] = str(sdir / f"{sid}.ReadsPerGene.out.tab")
            per_sample_outputs.append(entry)

        payload = {
            "success": result.returncode == 0,
            "results_dir": str(results_dir),
            "work_dir": str(work_dir),
            "params_file": str(params_file),
            "sample_count": len(sample_inputs),
            "per_sample_outputs": per_sample_outputs,
        }
        if get_tools_config().settings.debug_mode:
            payload.update({"stdout": result.stdout, "stderr": result.stderr, "cmd": " ".join(cmd)})
        return payload

    except Exception as e:
        return {"success": False, "error": f"æ‰§è¡ŒSTARæ¯”å¯¹å¤±è´¥: {str(e)}"}


@tool
def parse_star_metrics(results_directory: str) -> Dict[str, Any]:
    """è§£æSTARæ¯”å¯¹ç»“æœæ–‡ä»¶ï¼Œæå–æ¯”å¯¹æŒ‡æ ‡
    
    Args:
        results_directory: STARç»“æœç›®å½•è·¯å¾„
    
    Returns:
        Dict: è§£æåçš„æ¯”å¯¹æŒ‡æ ‡æ•°æ®
        {
            "success": bool,
            "total_samples": int,
            "results_directory": str,
            "per_sample_metrics": List[Dict],    # æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†æŒ‡æ ‡
            "overall_statistics": Dict,          # æ€»ä½“ç»Ÿè®¡ä¿¡æ¯
            "quality_assessment": Dict           # è´¨é‡è¯„ä¼°
        }
    """
    try:
        results_path = Path(results_directory)
        if not results_path.exists():
            return {
                "success": False,
                "error": f"STARç»“æœç›®å½•ä¸å­˜åœ¨: {results_directory}"
            }
        
        sample_metrics = []
        overall_stats = {
            "total_input_reads": 0,
            "total_mapped_reads": 0,
            "total_uniquely_mapped": 0,
            "total_multi_mapped": 0,
            "mapping_rates": [],
            "unique_mapping_rates": [],
            "multi_mapping_rates": [],
            "mismatch_rates": []
        }
        
        # æŸ¥æ‰¾STARè¾“å‡ºç›®å½•
        star_dir = results_path / "star"
        if not star_dir.exists():
            return {
                "success": False,
                "error": f"STARè¾“å‡ºç›®å½•ä¸å­˜åœ¨: {star_dir}"
            }
        
        # éå†æ ·æœ¬ç›®å½•ï¼ŒæŸ¥æ‰¾Log.final.outæ–‡ä»¶
        for sample_dir in star_dir.iterdir():
            if not sample_dir.is_dir():
                continue
                
            # ä¸ star.nf ä¸­ outFileNamePrefix ä¿æŒä¸€è‡´ï¼š{sample_id}/{sample_id}.*
            log_final_file = sample_dir / f"{sample_dir.name}.Log.final.out"
            if not log_final_file.exists():
                sample_metrics.append({
                    "sample_id": sample_dir.name,
                    "error": "Log.final.outæ–‡ä»¶ä¸å­˜åœ¨"
                })
                continue
            
            try:
                # è§£æLog.final.outæ–‡ä»¶
                with open(log_final_file, 'r', encoding='utf-8') as f:
                    log_content = f.read()
                
                # æå–å…³é”®æŒ‡æ ‡ï¼ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼‰
                input_reads = _extract_metric(log_content, r"Number of input reads.*?(\d+)")
                uniquely_mapped = _extract_metric(log_content, r"Uniquely mapped reads number.*?(\d+)")
                multi_mapped = _extract_metric(log_content, r"Number of reads mapped to multiple loci.*?(\d+)")
                unmapped = _extract_metric(log_content, r"Number of reads unmapped.*?(\d+)")
                
                # è®¡ç®—æ¯”ç‡
                if input_reads > 0:
                    mapping_rate = (uniquely_mapped + multi_mapped) / input_reads
                    unique_mapping_rate = uniquely_mapped / input_reads
                    multi_mapping_rate = multi_mapped / input_reads
                else:
                    mapping_rate = unique_mapping_rate = multi_mapping_rate = 0
                
                # æå–mismatchç‡
                mismatch_rate = _extract_metric(log_content, r"Mismatch rate per base.*?([\d.]+)%") / 100
                
                sample_metric = {
                    "sample_id": sample_dir.name,
                    "input_reads": input_reads,
                    "uniquely_mapped": uniquely_mapped,
                    "multi_mapped": multi_mapped,
                    "unmapped": unmapped,
                    "mapping_rate": round(mapping_rate, 4),
                    "unique_mapping_rate": round(unique_mapping_rate, 4),
                    "multi_mapping_rate": round(multi_mapping_rate, 4),
                    "mismatch_rate": round(mismatch_rate, 4),
                    "log_file": str(log_final_file)
                }
                
                sample_metrics.append(sample_metric)
                
                # ç´¯åŠ åˆ°æ€»ä½“ç»Ÿè®¡
                overall_stats["total_input_reads"] += input_reads
                overall_stats["total_mapped_reads"] += (uniquely_mapped + multi_mapped)
                overall_stats["total_uniquely_mapped"] += uniquely_mapped
                overall_stats["total_multi_mapped"] += multi_mapped
                overall_stats["mapping_rates"].append(mapping_rate)
                overall_stats["unique_mapping_rates"].append(unique_mapping_rate)
                overall_stats["multi_mapping_rates"].append(multi_mapping_rate)
                overall_stats["mismatch_rates"].append(mismatch_rate)
                
            except Exception as e:
                sample_metrics.append({
                    "sample_id": sample_dir.name,
                    "log_file": str(log_final_file),
                    "error": f"è§£æå¤±è´¥: {str(e)}"
                })
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        total_samples = len([m for m in sample_metrics if "error" not in m])
        if total_samples > 0:
            overall_mapping_rate = overall_stats["total_mapped_reads"] / overall_stats["total_input_reads"]
            overall_unique_rate = overall_stats["total_uniquely_mapped"] / overall_stats["total_input_reads"]
            overall_multi_rate = overall_stats["total_multi_mapped"] / overall_stats["total_input_reads"]
            avg_mismatch_rate = sum(overall_stats["mismatch_rates"]) / len(overall_stats["mismatch_rates"])
        else:
            overall_mapping_rate = overall_unique_rate = overall_multi_rate = avg_mismatch_rate = 0
        
        # è´¨é‡è¯„ä¼°
        quality_assessment = {
            "overall_quality": "good" if overall_mapping_rate > 0.85 else "moderate" if overall_mapping_rate > 0.7 else "poor",
            "unique_mapping_status": "good" if overall_unique_rate > 0.8 else "moderate" if overall_unique_rate > 0.6 else "poor",
            "multi_mapping_status": "good" if overall_multi_rate < 0.2 else "moderate" if overall_multi_rate < 0.3 else "high"
        }
        
        return {
            "success": True,
            "total_samples": total_samples,
            "results_directory": results_directory,
            "sample_metrics": sample_metrics,
            "overall_statistics": {
                "total_input_reads": overall_stats["total_input_reads"],
                "total_mapped_reads": overall_stats["total_mapped_reads"],
                "total_uniquely_mapped": overall_stats["total_uniquely_mapped"],
                "total_multi_mapped": overall_stats["total_multi_mapped"],
                "overall_mapping_rate": round(overall_mapping_rate, 4),
                "overall_unique_mapping_rate": round(overall_unique_rate, 4),
                "overall_multi_mapping_rate": round(overall_multi_rate, 4),
                "average_mismatch_rate": round(avg_mismatch_rate, 4)
            },
            "quality_assessment": quality_assessment
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"è§£æSTARç»“æœå¤±è´¥: {str(e)}"
        }

def _extract_metric(text: str, pattern: str) -> float:
    """ä»æ–‡æœ¬ä¸­æå–æ•°å€¼æŒ‡æ ‡çš„è¾…åŠ©å‡½æ•°"""
    import re
    match = re.search(pattern, text)
    if match:
        try:
            return float(match.group(1))
        except ValueError:
            return 0.0
    return 0.0


@tool
def run_nextflow_hisat2(
    hisat2_params: Dict[str, Any],
    fastp_results: Dict[str, Any],
    genome_info: Dict[str, Any],
    results_timestamp: Optional[str] = None
) -> Dict[str, Any]:
    """æ‰§è¡Œ HISAT2 æ¯”å¯¹ï¼ˆç²¾ç®€ç‰ˆï¼Œä¸ run_nextflow_star ç­‰ä»·ï¼‰

    çº¦æŸï¼ˆä¸è·¯å¾„å¥‘çº¦ä¸€è‡´ï¼‰:
    - ä»…åœ¨ fastp_results.success ä¸ºçœŸä¸”åŒ…å« per_sample_outputs æ—¶æ”¾è¡Œ
    - ç»Ÿä¸€å¤ç”¨ FastP çš„ results_dir ä½œä¸ºè¿è¡Œæ ¹ç›®å½•
    - HISAT2 ç´¢å¼•ä¼˜å…ˆä½¿ç”¨ genome_info.hisat2_index_dirï¼›å¦åˆ™ç”± genome_info.fasta_path æ¨å¯¼
    - sample_inputs ä»…æ¥æºäº fastp_results.per_sample_outputsï¼ˆä¸å†æ‰«æç›®å½•ï¼‰
    - per_sample_outputs è·¯å¾„ä¸ hisat2.nf äº§å‡ºä¸€è‡´ï¼ˆæ ·æœ¬å­ç›®å½• + é»˜è®¤æ–‡ä»¶åï¼‰
    """
    try:
        tools_config = get_tools_config()

        # 1) æ ¡éªŒ FastP ç»“æœä¸è¿è¡Œæ ¹ç›®å½•
        if not (fastp_results and fastp_results.get("success")):
            return {"success": False, "error": "FastPç»“æœæ— æ•ˆï¼Œæ— æ³•æ‰§è¡ŒHISAT2æ¯”å¯¹"}

        fastp_results_dir = fastp_results.get("results_dir")
        if not fastp_results_dir:
            return {"success": False, "error": "FastPç»“æœç¼ºå°‘results_dir"}

        per_sample = fastp_results.get("per_sample_outputs") or []
        if not per_sample:
            return {"success": False, "error": "FastPç»“æœç¼ºå°‘per_sample_outputs"}

        # 2) è¿è¡Œæ ¹ç›®å½•ä¸å·¥ä½œç›®å½•
        timestamp = results_timestamp or datetime.now().strftime("%Y%m%d_%H%M%S")
        results_dir = Path(fastp_results_dir)
        run_id = results_dir.name or timestamp
        work_dir = tools_config.settings.data_dir / "work" / f"hisat2_{run_id}"
        results_dir.mkdir(parents=True, exist_ok=True)
        work_dir.mkdir(parents=True, exist_ok=True)

        # 3) è§£æ HISAT2 ç´¢å¼•ç›®å½•
        def _resolve(p: str) -> Path:
            pp = Path(p)
            return pp if pp.is_absolute() else (tools_config.settings.project_root / pp)

        hisat2_index_prefix = ""
        if isinstance(genome_info, dict):
            hisat2_index_dir = genome_info.get("hisat2_index_dir") or genome_info.get("index_dir") or ""
            if hisat2_index_dir:
                hisat2_index_prefix = str(_resolve(hisat2_index_dir) / "genome")
            else:
                fasta_path = genome_info.get("fasta_path") or genome_info.get("fasta")
                if fasta_path:
                    hisat2_index_prefix = str(tools_config.get_hisat2_index_dir(_resolve(fasta_path)) / "genome")

        if not hisat2_index_prefix:
            return {"success": False, "error": "ç¼ºå°‘HISAT2ç´¢å¼•ç›®å½•ï¼ˆgenome_info.hisat2_index_dir æˆ– fasta_path å¿…é¡»æä¾›ï¼‰"}

        # æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        index_files = list(Path(hisat2_index_prefix).parent.glob(f"{Path(hisat2_index_prefix).name}.*.ht2"))
        if not index_files:
            return {"success": False, "error": f"HISAT2ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {hisat2_index_prefix}.*.ht2"}

        # 4) æ„é€  sample_inputsï¼ˆä»…ä½¿ç”¨ FastP è¿”å›çš„ç»“æ„ï¼‰
        sample_inputs: List[Dict[str, Any]] = []
        for i, fp in enumerate(per_sample):
            sid = fp.get("sample_id", f"sample_{i+1}")
            r1 = fp.get("trimmed_single") or fp.get("trimmed_r1")
            r2 = fp.get("trimmed_r2")
            if not r1:
                continue
            sample_inputs.append({
                "sample_id": sid,
                "is_paired": bool(r2),
                "read1": r1,
                **({"read2": r2} if r2 else {})
            })
        if not sample_inputs:
            return {"success": False, "error": "æœªä»FastPç»“æœæ„é€ åˆ°ä»»ä½•æ ·æœ¬è¾“å…¥"}

        # 5) ç»„è£… Nextflow å‚æ•°
        cleaned_params: Dict[str, Any] = {}
        for k, v in (hisat2_params or {}).items():
            if v is None or k in {"hisat2_cpus", "threads", "p"}:
                continue
            cleaned_params[k.lstrip('-')] = v

        nf_params = {
            "sample_inputs": json.dumps(sample_inputs, ensure_ascii=False),
            "hisat2_index": hisat2_index_prefix,
            "results_dir": str(results_dir),
            "work_dir": str(work_dir),
            **cleaned_params,
        }

        # ä¿å­˜å‚æ•°æ–‡ä»¶åˆ°hisat2å­ç›®å½•
        hisat2_dir = results_dir / "hisat2"
        hisat2_dir.mkdir(parents=True, exist_ok=True)
        params_file = hisat2_dir / "hisat2_params.json"
        with open(params_file, "w", encoding="utf-8") as f:
            json.dump(nf_params, f, indent=2, ensure_ascii=False)

        # 6) å®šä½å¹¶æ‰§è¡Œ Nextflow
        nf_candidates = [
            tools_config.settings.project_root / "src" / "nextflow" / "hisat2.nf",
            Path("/src/nextflow/hisat2.nf"),
        ]
        nextflow_script = next((p for p in nf_candidates if p.exists()), None)
        if nextflow_script is None:
            return {"success": False, "error": "æœªæ‰¾åˆ° hisat2.nf è„šæœ¬ï¼Œè¯·æ£€æŸ¥ /src/nextflow/hisat2.nf è·¯å¾„", "searched": [str(p) for p in nf_candidates]}

        print(f"æ‰§è¡ŒHISAT2æ¯”å¯¹ - å‚æ•°æ–‡ä»¶: {params_file}")
        print(f"HISAT2ç´¢å¼•: {nf_params['hisat2_index']}")
        cmd = [
            "nextflow", "run", str(nextflow_script),
            "-params-file", str(params_file),
            "-work-dir", str(work_dir),
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=7200, cwd=tools_config.settings.project_root)

        # 7) ç»„è£…æ¯æ ·æœ¬è¾“å‡ºè·¯å¾„ï¼ˆä¸ hisat2.nf publishDir å¯¹é½ï¼‰
        hisat2_out = results_dir / "hisat2"
        per_sample_outputs: List[Dict[str, Any]] = []
        for item in sample_inputs:
            sid = item["sample_id"]
            sdir = hisat2_out / sid
            entry = {
                "sample_id": sid,
                "aligned_bam": str(sdir / f"{sid}.hisat2.bam"),
                "align_summary": str(sdir / f"{sid}.align_summary.txt"),
                "bam_index": str(sdir / f"{sid}.hisat2.bam.bai"),
            }
            per_sample_outputs.append(entry)

        payload = {
            "success": result.returncode == 0,
            "results_dir": str(results_dir),
            "work_dir": str(work_dir),
            "params_file": str(params_file),
            "sample_count": len(sample_inputs),
            "per_sample_outputs": per_sample_outputs,
        }
        if get_tools_config().settings.debug_mode:
            payload.update({"stdout": result.stdout, "stderr": result.stderr, "cmd": " ".join(cmd)})
        return payload

    except Exception as e:
        return {"success": False, "error": f"æ‰§è¡ŒHISAT2æ¯”å¯¹å¤±è´¥: {str(e)}"}


@tool
def build_hisat2_index(
    genome_id: str,
    p: Optional[int] = None,
    force_rebuild: bool = False,
    results_dir: Optional[str] = None,
) -> Dict[str, Any]:
    """æ„å»º HISAT2 ç´¢å¼•ï¼ˆç­‰ä»· build_star_indexï¼‰"""
    try:
        tools_config = get_tools_config()

        # 1) ä»åŸºå› ç»„é…ç½®è·å–åŸºå› ç»„ä¿¡æ¯
        genome_configs = tools_config.get_genome_configs()
        if genome_id not in genome_configs:
            return {"success": False, "error": f"åŸºå› ç»„ID '{genome_id}' æœªæ‰¾åˆ°ï¼Œå¯ç”¨ID: {list(genome_configs.keys())}"}
        
        genome_config = genome_configs[genome_id]
        fasta_path = genome_config.get("fasta_path")
        gtf_path = genome_config.get("gtf_path", "")  # GTFå¯é€‰
        
        if not fasta_path:
            return {"success": False, "error": f"åŸºå› ç»„ '{genome_id}' é…ç½®ç¼ºå°‘fasta_path"}

        # 2) è§£æè·¯å¾„
        def _resolve(p: str) -> Path:
            pp = Path(p)
            return pp if pp.is_absolute() else (tools_config.settings.project_root / pp)

        fasta_file = _resolve(fasta_path)
        if not fasta_file.exists():
            return {"success": False, "error": f"FASTAæ–‡ä»¶ä¸å­˜åœ¨: {fasta_file}"}

        # 3) ç¡®å®šç´¢å¼•ç›®å½•
        index_dir = tools_config.get_hisat2_index_dir(fasta_file)
        index_dir.mkdir(parents=True, exist_ok=True)

        # 4) æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»º
        index_files = list(index_dir.glob("genome.*.ht2"))
        if index_files and not force_rebuild:
            return {
                "success": True,
                "hisat2_index_dir": str(index_dir),
                "status": "å·²å­˜åœ¨",
                "index_files": [str(f) for f in index_files],
                "message": f"HISAT2ç´¢å¼•å·²å­˜åœ¨ï¼Œè·³è¿‡æ„å»º: {index_dir}"
            }

        # 5) æ‰§è¡Œç´¢å¼•æ„å»º
        work_dir_name = f"hisat2_index_{genome_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        work_dir = tools_config.settings.data_dir / "work" / work_dir_name
        work_dir.mkdir(parents=True, exist_ok=True)

        nf_params = {
            "genome_fasta": str(fasta_file),
            "genome_gtf": str(_resolve(gtf_path)) if gtf_path else "",
            "hisat2_index_dir": str(index_dir),
            "index_basename": "genome",
            "p": p or 4,
        }

        params_file = work_dir / "build_hisat2_index_params.json"
        with open(params_file, "w", encoding="utf-8") as f:
            json.dump(nf_params, f, indent=2, ensure_ascii=False)

        # 6) å®šä½å¹¶æ‰§è¡Œ Nextflow
        nf_candidates = [
            tools_config.settings.project_root / "src" / "nextflow" / "build_hisat2_index.nf",
            Path("/src/nextflow/build_hisat2_index.nf"),
        ]
        nextflow_script = next((p for p in nf_candidates if p.exists()), None)
        if nextflow_script is None:
            return {"success": False, "error": "æœªæ‰¾åˆ° build_hisat2_index.nf è„šæœ¬"}

        print(f"æ„å»ºHISAT2ç´¢å¼• - åŸºå› ç»„: {genome_id}")
        print(f"FASTA: {fasta_file}")
        print(f"ç´¢å¼•ç›®å½•: {index_dir}")
        
        cmd = [
            "nextflow", "run", str(nextflow_script),
            "-params-file", str(params_file),
            "-work-dir", str(work_dir),
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=14400, cwd=tools_config.settings.project_root)

        # 7) æ£€æŸ¥ç»“æœ
        final_index_files = list(index_dir.glob("genome.*.ht2"))
        success = result.returncode == 0 and len(final_index_files) > 0

        payload = {
            "success": success,
            "hisat2_index_dir": str(index_dir),
            "genome_id": genome_id,
            "index_files": [str(f) for f in final_index_files],
            "work_dir": str(work_dir),
            "params_file": str(params_file),
        }

        if success:
            payload["status"] = "æ„å»ºæˆåŠŸ"
            payload["message"] = f"HISAT2ç´¢å¼•æ„å»ºå®Œæˆ: {index_dir}"
        else:
            payload["status"] = "æ„å»ºå¤±è´¥"
            payload["error"] = f"HISAT2ç´¢å¼•æ„å»ºå¤±è´¥ï¼Œreturncode: {result.returncode}"

        if get_tools_config().settings.debug_mode:
            payload.update({"stdout": result.stdout, "stderr": result.stderr, "cmd": " ".join(cmd)})

        return payload

    except Exception as e:
        return {"success": False, "error": f"æ„å»ºHISAT2ç´¢å¼•å¤±è´¥: {str(e)}"}


@tool
def parse_hisat2_metrics(results_directory: str) -> Dict[str, Any]:
    """è§£æHISAT2æ¯”å¯¹ç»“æœæ–‡ä»¶ï¼Œæå–æ¯”å¯¹æŒ‡æ ‡
    
    Args:
        results_directory: HISAT2ç»“æœç›®å½•è·¯å¾„
        
    Returns:
        Dict: åŒ…å«æ ·æœ¬æŒ‡æ ‡å’Œæ•´ä½“ç»Ÿè®¡çš„ç»“æœ
    """
    try:
        results_path = Path(results_directory)
        
        if not results_path.exists():
            return {"success": False, "error": f"ç»“æœç›®å½•ä¸å­˜åœ¨: {results_directory}"}
        
        # æŸ¥æ‰¾HISAT2ç»“æœå­ç›®å½•
        hisat2_dir = results_path / "hisat2"
        if not hisat2_dir.exists():
            return {"success": False, "error": f"HISAT2å­ç›®å½•ä¸å­˜åœ¨: {hisat2_dir}"}
        
        sample_metrics = []
        overall_stats = {
            "total_input_reads": 0,
            "total_mapped_reads": 0,
            "total_uniquely_mapped": 0,
            "total_multi_mapped": 0,
            "mapping_rates": [],
            "unique_mapping_rates": [],
            "multi_mapping_rates": []
        }
        
        # æ‰«ææ ·æœ¬ç›®å½•
        sample_dirs = [d for d in hisat2_dir.iterdir() if d.is_dir()]
        total_samples = len(sample_dirs)
        
        if total_samples == 0:
            return {"success": False, "error": "æœªæ‰¾åˆ°ä»»ä½•æ ·æœ¬ç›®å½•"}
        
        for sample_dir in sample_dirs:
            sample_id = sample_dir.name
            summary_file = sample_dir / f"{sample_id}.align_summary.txt"
            
            if not summary_file.exists():
                continue
            
            # è§£æHISAT2æ¯”å¯¹æ‘˜è¦
            with open(summary_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # HISAT2è¾“å‡ºæ ¼å¼è§£æ
            metrics = _parse_hisat2_summary(content, sample_id)
            if metrics:
                sample_metrics.append(metrics)
                
                # ç´¯ç§¯æ•´ä½“ç»Ÿè®¡
                overall_stats["total_input_reads"] += metrics["input_reads"]
                overall_stats["total_mapped_reads"] += metrics["mapped_reads"]
                overall_stats["total_uniquely_mapped"] += metrics.get("uniquely_mapped", 0)
                overall_stats["total_multi_mapped"] += metrics.get("multi_mapped", 0)
                overall_stats["mapping_rates"].append(metrics["mapping_rate"])
                overall_stats["unique_mapping_rates"].append(metrics.get("unique_mapping_rate", 0))
                overall_stats["multi_mapping_rates"].append(metrics.get("multi_mapping_rate", 0))
        
        # è®¡ç®—æ•´ä½“æ¯”ç‡
        if overall_stats["total_input_reads"] > 0:
            overall_mapping_rate = overall_stats["total_mapped_reads"] / overall_stats["total_input_reads"]
            overall_unique_rate = overall_stats["total_uniquely_mapped"] / overall_stats["total_input_reads"]
            overall_multi_rate = overall_stats["total_multi_mapped"] / overall_stats["total_input_reads"]
        else:
            overall_mapping_rate = overall_unique_rate = overall_multi_rate = 0
        
        # è´¨é‡è¯„ä¼°
        quality_assessment = {
            "overall_quality": "good" if overall_mapping_rate > 0.85 else "moderate" if overall_mapping_rate > 0.7 else "poor",
            "unique_mapping_status": "good" if overall_unique_rate > 0.8 else "moderate" if overall_unique_rate > 0.6 else "poor",
            "multi_mapping_status": "good" if overall_multi_rate < 0.2 else "moderate" if overall_multi_rate < 0.3 else "high"
        }
        
        return {
            "success": True,
            "total_samples": total_samples,
            "results_directory": results_directory,
            "sample_metrics": sample_metrics,
            "overall_statistics": {
                "total_input_reads": overall_stats["total_input_reads"],
                "total_mapped_reads": overall_stats["total_mapped_reads"],
                "total_uniquely_mapped": overall_stats["total_uniquely_mapped"],
                "total_multi_mapped": overall_stats["total_multi_mapped"],
                "overall_mapping_rate": round(overall_mapping_rate, 4),
                "overall_unique_mapping_rate": round(overall_unique_rate, 4),
                "overall_multi_mapping_rate": round(overall_multi_rate, 4),
            },
            "quality_assessment": quality_assessment
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"è§£æHISAT2ç»“æœå¤±è´¥: {str(e)}"
        }


def _parse_hisat2_summary(content: str, sample_id: str) -> Dict[str, Any]:
    """è§£æHISAT2æ¯”å¯¹æ‘˜è¦å†…å®¹"""
    try:
        lines = content.strip().split('\n')
        metrics = {"sample_id": sample_id}
        
        for line in lines:
            line = line.strip()
            
            # HISAT2è¾“å‡ºæ ¼å¼ç¤ºä¾‹:
            # "25000000 reads; of these:"
            # "  23500000 (94.00%) aligned concordantly exactly 1 time"
            # "  1200000 (4.80%) aligned concordantly >1 times"
            # "  300000 (1.20%) aligned concordantly 0 times"
            
            # æå–æ€»readæ•°
            if " reads; of these:" in line:
                total_reads = int(line.split()[0])
                metrics["input_reads"] = total_reads
            
            # æå–æ¯”å¯¹ç»Ÿè®¡ï¼ˆåŒç«¯æµ‹åºï¼‰
            elif "aligned concordantly exactly 1 time" in line:
                unique_count = _extract_reads_count(line)
                metrics["uniquely_mapped"] = unique_count
            elif "aligned concordantly >1 times" in line:
                multi_count = _extract_reads_count(line)
                metrics["multi_mapped"] = multi_count
            elif "aligned concordantly 0 times" in line:
                unaligned_count = _extract_reads_count(line)
                # å¯¹äºå•ç«¯æµ‹åºçš„æƒ…å†µ
            elif "aligned exactly 1 time" in line and "concordantly" not in line:
                unique_count = _extract_reads_count(line)
                metrics["uniquely_mapped"] = unique_count
            elif "aligned >1 times" in line and "concordantly" not in line:
                multi_count = _extract_reads_count(line)  
                metrics["multi_mapped"] = multi_count
        
        # è®¡ç®—æ´¾ç”ŸæŒ‡æ ‡
        if "input_reads" in metrics:
            total_reads = metrics["input_reads"]
            uniquely_mapped = metrics.get("uniquely_mapped", 0)
            multi_mapped = metrics.get("multi_mapped", 0)
            mapped_reads = uniquely_mapped + multi_mapped
            
            metrics["mapped_reads"] = mapped_reads
            metrics["unmapped_reads"] = total_reads - mapped_reads
            
            if total_reads > 0:
                metrics["mapping_rate"] = round(mapped_reads / total_reads, 4)
                metrics["unique_mapping_rate"] = round(uniquely_mapped / total_reads, 4)
                metrics["multi_mapping_rate"] = round(multi_mapped / total_reads, 4)
                metrics["unmapped_rate"] = round((total_reads - mapped_reads) / total_reads, 4)
            else:
                metrics.update({
                    "mapping_rate": 0.0,
                    "unique_mapping_rate": 0.0,
                    "multi_mapping_rate": 0.0,
                    "unmapped_rate": 0.0
                })
        
        return metrics if "input_reads" in metrics else None
        
    except Exception as e:
        print(f"è§£æHISAT2æ‘˜è¦å¤±è´¥ (æ ·æœ¬ {sample_id}): {e}")
        return None


def _extract_reads_count(line: str) -> int:
    """ä»HISAT2è¾“å‡ºè¡Œä¸­æå–readsæ•°é‡"""
    import re
    # åŒ¹é…æ•°å­—æ¨¡å¼ï¼Œä¾‹å¦‚: "23500000 (94.00%) aligned..."
    match = re.search(r'(\d+)\s+\([\d.]+%\)', line.strip())
    if match:
        return int(match.group(1))
    return 0


# ==================== FeatureCounts ä¸“ç”¨å·¥å…·å‡½æ•° ====================

@tool
def run_nextflow_featurecounts(
    featurecounts_params: Dict[str, Any],
    star_results: Dict[str, Any],
    genome_info: Dict[str, Any],
    results_timestamp: Optional[str] = None,
    base_results_dir: Optional[str] = None,
    hisat2_results: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """æ‰§è¡ŒNextflow FeatureCountså®šé‡æµç¨‹

    Args:
        featurecounts_params: FeatureCountså‚æ•°å­—å…¸ï¼ˆå¯åŒ…å« -T/-s/-p/-M ç­‰é£æ ¼é”®ï¼‰
        star_results: STARèŠ‚ç‚¹ç»“æœï¼ˆå¯ä¸ºç©ºï¼‰ï¼Œéœ€åŒ…å« per_sample_outputs ä¸­çš„ BAM è·¯å¾„
        genome_info: åŸºå› ç»„ä¿¡æ¯ï¼ˆéœ€æä¾› GTF è·¯å¾„ï¼Œå¦‚ gtf_pathï¼‰
        results_timestamp: å¯é€‰çš„æ—¶é—´æˆ³ï¼Œä¼˜å…ˆç”¨äºç»“æœç›®å½•
        base_results_dir: å¯é€‰çš„åŸºåº•ç»“æœç›®å½•ï¼ˆæ¥è‡ªDetectèŠ‚ç‚¹ï¼‰
        hisat2_results: HISAT2èŠ‚ç‚¹ç»“æœï¼ˆå¯ä¸ºç©ºï¼‰ï¼Œä¸ star_results äºŒé€‰ä¸€

    Returns:
        æ‰§è¡Œç»“æœå­—å…¸ï¼ŒåŒ…å«çŠ¶æ€ã€è¾“å‡ºè·¯å¾„ã€æ ·æœ¬è¾“å‡ºç­‰
    """
    try:
        tools_config = get_tools_config()

        # ä»…æ”¯æŒå®¹å™¨è·¯å¾„çš„å½’ä¸€åŒ–ï¼ˆä¸åšæœ¬åœ°æ˜ å°„ï¼‰
        def _to_container_path(path_str: str) -> str:
            """å°†è¾“å…¥è§„èŒƒåŒ–ä¸ºå®¹å™¨å†…è·¯å¾„ï¼š
            - å…è®¸ '/data/...', '/config/...', '/work/...'
            - 'genomes/...' å°†æ˜ å°„ä¸º '/data/genomes/...'
            - å…¶ä»–ç›¸å¯¹æˆ–æœ¬åœ°è·¯å¾„ä¸€å¾‹åŸæ ·è¿”å›ï¼ˆç”±å­˜åœ¨æ€§æ ¡éªŒæŠ¥é”™ï¼‰
            """
            s = str(path_str or '').strip()
            if not s:
                return s
            if s.startswith('/data/') or s.startswith('/config/') or s.startswith('/work/'):
                return s
            if s.startswith('genomes/'):
                return '/data/' + s
            return s

        # æ ¡éªŒä¾èµ–è¾“å…¥
        # é€‰æ‹©å¯ç”¨çš„æ¯”å¯¹ç»“æœï¼ˆSTAR/HISAT2ï¼‰
        align_results = None
        if star_results and star_results.get("success"):
            align_results = star_results
        elif hisat2_results and hisat2_results.get("success"):
            align_results = hisat2_results
        else:
            return {"success": False, "error": "æ¯”å¯¹ç»“æœæ— æ•ˆï¼ˆSTAR/HISAT2ï¼‰ï¼Œæ— æ³•æ‰§è¡ŒFeatureCounts"}

        per_sample = align_results.get("per_sample_outputs") or []
        if not per_sample:
            return {"success": False, "error": "æ¯”å¯¹ç»“æœç¼ºå°‘ per_sample_outputs ä¿¡æ¯"}

        # ç¯å¢ƒæ£€æŸ¥ï¼šå…è®¸æœ¬åœ°ä¸å®¹å™¨ç¯å¢ƒï¼Œè·¯å¾„è§„èŒƒåŒ–åœ¨ä¸‹æ–¹å¤„ç†

        # è§£æå¹¶å½’ä¸€åŒ– GTF æ³¨é‡Šæ–‡ä»¶è·¯å¾„ï¼ˆå®¹å™¨å†…ï¼‰
        gtf_file_raw = (
            genome_info.get("gtf_path")
            or genome_info.get("gtf")
            or genome_info.get("annotation_gtf")
            or ""
        )
        if not gtf_file_raw:
            return {"success": False, "error": "genome_info æœªæä¾› GTF æ³¨é‡Šæ–‡ä»¶è·¯å¾„ (gtf_path)"}
        gtf_file = _to_container_path(gtf_file_raw)
        if not Path(gtf_file).exists():
            return {
                "success": False,
                "error": f"GTFæ–‡ä»¶ä¸å­˜åœ¨: {gtf_file}",
            }

        # è¿è¡Œæ ¹ç›®å½•ï¼ˆresults_dirï¼‰ï¼šå¤ç”¨æ¯”å¯¹æ­¥éª¤çš„ results_dirï¼Œä¿æŒåŒä¸€è¿è¡Œæ ¹ç›®å½•
        timestamp = results_timestamp or datetime.now().strftime("%Y%m%d_%H%M%S")
        run_root = Path(align_results.get("results_dir") or base_results_dir or tools_config.results_dir / f"{timestamp}")
        # å®¹å™¨è·¯å¾„è§„èŒƒï¼ˆå¦‚éœ€ï¼‰
        run_root = Path(_to_container_path(str(run_root)))
        results_dir = run_root
        # ç»Ÿä¸€ Nextflow å·¥ä½œç›®å½•åˆ° /data/workï¼Œä½¿ç”¨è¿è¡ŒIDåŒºåˆ†
        run_id = results_dir.name or timestamp
        work_dir = tools_config.settings.data_dir / "work" / f"featurecounts_{run_id}"
        results_dir.mkdir(parents=True, exist_ok=True)
        work_dir.mkdir(parents=True, exist_ok=True)
        (results_dir / "featurecounts").mkdir(parents=True, exist_ok=True)

        # æ„å»º Nextflow å‚æ•°ï¼ˆä¸ featurecounts.nf å¯¹é½ï¼‰
        # å°† STAR è¾“å‡ºçš„ BAM åˆ—è¡¨è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ï¼ˆNextflow ç«¯ä¼š echo åå†è§£æï¼‰
        bam_entries = []
        for item in per_sample:
            sid = item.get("sample_id") or "sample"
            bam = item.get("aligned_bam") or item.get("bam")
            if not bam:
                continue
            bam_norm = _to_container_path(bam)
            if not Path(bam_norm).exists():
                return {
                    "success": False,
                    "error": f"BAMæ–‡ä»¶ä¸å­˜åœ¨: {bam_norm}",
                }
            bam_entries.append({"sample_id": sid, "bam_file": bam_norm})
        if not bam_entries:
            return {"success": False, "error": "æœªä»STARç»“æœä¸­æ”¶é›†åˆ°ä»»ä½•BAMè·¯å¾„"}

        # å‚æ•°æ˜ å°„ï¼šPythoné£æ ¼/çŸ­æ——æ ‡ â†’ Nextflow params åç§°
        mapped: Dict[str, Any] = {}
        p = featurecounts_params or {}

        def pick_bool(key: str) -> Optional[bool]:
            v = p.get(key)
            if isinstance(v, bool):
                return v
            return None

        def pick_int(key: str) -> Optional[int]:
            v = p.get(key)
            try:
                return int(v) if v is not None else None
            except Exception:
                return None

        # çº¿ç¨‹/é“¾ç‰¹å¼‚æ€§/ç‰¹å¾/å±æ€§/è´¨é‡é˜ˆ
        mapped["threads"] = pick_int("-T") or p.get("threads") or 4
        mapped["strand_specificity"] = pick_int("-s") or p.get("strand_specificity") or 0
        mapped["feature_type"] = p.get("-t") or p.get("feature_type") or "exon"
        mapped["attribute_type"] = p.get("-g") or p.get("attribute_type") or "gene_id"
        mapped["min_mapping_quality"] = pick_int("-Q") or p.get("min_mapping_quality") or 10

        # å¸ƒå°”å¼€å…³ - ä¿®æ”¹count_reads_pairsé»˜è®¤å€¼ä¸ºfalse
        mapped["count_reads_pairs"] = pick_bool("-p") if pick_bool("-p") is not None else (p.get("count_reads_pairs") if isinstance(p.get("count_reads_pairs"), bool) else False)
        mapped["count_multi_mapping_reads"] = pick_bool("-M") if pick_bool("-M") is not None else bool(p.get("count_multi_mapping_reads", False))
        mapped["ignore_duplicates"] = bool(p.get("--ignoreDup", False) or p.get("ignore_duplicates", False))
        mapped["require_both_ends_mapped"] = bool(p.get("-B", False) or p.get("require_both_ends_mapped", False))
        mapped["exclude_chimeric"] = bool(p.get("-C", False) or p.get("exclude_chimeric", False))

        # ç»„è£… Nextflow å‚æ•°æ–‡ä»¶
        nf_params = {
            "input_bam_list": json.dumps(bam_entries, ensure_ascii=False),
            "gtf_file": gtf_file,
            "results_dir": str(results_dir),
            "work_dir": str(work_dir),
            **mapped,
        }

        # ä¿å­˜å‚æ•°æ–‡ä»¶åˆ°featurecountså­ç›®å½•
        fc_dir = results_dir / "featurecounts"
        fc_dir.mkdir(parents=True, exist_ok=True)
        params_file = fc_dir / "featurecounts_params.json"
        with open(params_file, "w", encoding="utf-8") as f:
            json.dump(nf_params, f, indent=2, ensure_ascii=False)

        # å®šä½ Nextflow è„šæœ¬
        nf_candidates = [
            tools_config.settings.project_root / "src" / "nextflow" / "featurecounts.nf",
            Path("/src/nextflow/featurecounts.nf"),
        ]
        nextflow_script = None
        for cand in nf_candidates:
            if cand.exists():
                nextflow_script = cand
                break
        if nextflow_script is None:
            return {
                "success": False,
                "error": "æœªæ‰¾åˆ° featurecounts.nf è„šæœ¬ï¼Œè¯·æ£€æŸ¥ /src/nextflow/featurecounts.nf è·¯å¾„",
                "searched": [str(p) for p in nf_candidates],
            }

        # æ‰§è¡Œ Nextflow
        cmd = [
            "nextflow",
            "run",
            str(nextflow_script),
            "-params-file",
            str(params_file),
            "-work-dir",
            str(work_dir),
        ]

        print("ğŸš€ æ‰§è¡ŒNextflow FeatureCountsæµæ°´çº¿â€¦")
        print(f"   å‚æ•°æ–‡ä»¶: {params_file}")
        print(f"   å·¥ä½œç›®å½•: {work_dir}")
        print(f"   ç»“æœç›®å½•: {results_dir}")

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=1800,
            cwd=tools_config.settings.project_root,
        )

        # æ„å»ºè¾“å‡ºç»“æ„ - é€‚é…æ–°çš„æ‰¹é‡è¾“å‡ºæ ¼å¼
        sample_count = len(bam_entries)
        per_sample_outputs: List[Dict[str, Any]] = []
        fc_root = results_dir / "featurecounts"
        
        # æ–°çš„featurecounts.nfè„šæœ¬ç”Ÿæˆæ‰¹é‡æ–‡ä»¶ï¼Œä¸å†æœ‰æ¯ä¸ªæ ·æœ¬çš„å•ç‹¬ç›®å½•
        # ä¸»è¦è¾“å‡ºæ–‡ä»¶ï¼š
        # - all_samples.featureCounts (å®Œæ•´è®¡æ•°çŸ©é˜µ)
        # - all_samples.featureCounts.summary (ç»Ÿè®¡æ‘˜è¦)
        # - merged_counts_matrix.txt (å…¼å®¹æ ¼å¼çš„çŸ©é˜µ)
        
        # ä¸ºå…¼å®¹æ€§ç”Ÿæˆper_sample_outputsç»“æ„ï¼ŒæŒ‡å‘æ‰¹é‡æ–‡ä»¶
        for entry in bam_entries:
            sid = entry["sample_id"]
            sample_output = {
                "sample_id": sid,
                "counts_file": str(fc_root / "all_samples.featureCounts"),  # æŒ‡å‘æ‰¹é‡æ–‡ä»¶
                "summary_file": str(fc_root / "all_samples.featureCounts.summary"),  # æŒ‡å‘æ‰¹é‡æ–‡ä»¶
            }
            per_sample_outputs.append(sample_output)

        payload = {
            "success": result.returncode == 0,
            "message": f"FeatureCountså®šé‡å®Œæˆï¼Œå¤„ç†äº†{sample_count}ä¸ªæ ·æœ¬" if result.returncode == 0 else "FeatureCountsæ‰§è¡Œå¤±è´¥",
            "results_dir": str(results_dir),
            "work_dir": str(work_dir),
            "params_file": str(params_file),
            "sample_count": sample_count,
            "per_sample_outputs": per_sample_outputs,
            "matrix_path": str(fc_root / "merged_counts_matrix.txt"),
            "nextflow_params": nf_params,
        }
        try:
            if get_tools_config().settings.debug_mode:
                payload.update({
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                    "cmd": " ".join(cmd),
                })
            else:
                # éè°ƒè¯•æ¨¡å¼å»æ‰ nextflow_params ä»¥å‡å°è´Ÿè½½
                payload.pop("nextflow_params", None)
        except Exception:
            pass
        return payload

    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "error": "Nextflowæ‰§è¡Œè¶…æ—¶ï¼ˆ30åˆ†é’Ÿï¼‰",
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"æ‰§è¡ŒFeatureCountsæµæ°´çº¿å¤±è´¥: {str(e)}",
        }


@tool
def parse_featurecounts_metrics(results_directory: str) -> Dict[str, Any]:
    """è§£æFeatureCountså®šé‡ç»“æœï¼Œè¾“å‡ºæ ·æœ¬çº§ä¸æ€»ä½“æŒ‡æ ‡
    
    Args:
        results_directory: FeatureCountsç»“æœç›®å½•ï¼ˆåŒ…å« featurecounts å­ç›®å½•ï¼‰
    
    Returns:
        è§£æåçš„æŒ‡æ ‡ï¼ˆassignment ratesã€æœªåˆ†é…åŸå› ç­‰ï¼‰
    """
    try:
        results_path = Path(results_directory)
        if not results_path.exists():
            return {"success": False, "error": f"ç»“æœç›®å½•ä¸å­˜åœ¨: {results_directory}"}
        
        fc_dir = results_path / "featurecounts"
        if not fc_dir.exists():
            return {"success": False, "error": f"ç¼ºå°‘ç‰¹å¾è®¡æ•°ç›®å½•: {fc_dir}"}
        
        # æŸ¥æ‰¾æ‰¹é‡è¾“å‡ºçš„æ±‡æ€»æ–‡ä»¶
        summary_file = fc_dir / "all_samples.featureCounts.summary"
        counts_file = fc_dir / "all_samples.featureCounts"
        
        if not summary_file.exists():
            return {"success": False, "error": f"æœªæ‰¾åˆ°FeatureCountsæ±‡æ€»æ–‡ä»¶: {summary_file}"}
        
        if not counts_file.exists():
            return {"success": False, "error": f"æœªæ‰¾åˆ°FeatureCountsè®¡æ•°æ–‡ä»¶: {counts_file}"}
        
        # è§£ææ‰¹é‡æ±‡æ€»æ–‡ä»¶
        sample_metrics: List[Dict[str, Any]] = []
        totals = {
            "assigned": 0,
            "nofeatures": 0,
            "multimapping": 0,
            "ambiguous": 0,
            "mappingquality": 0,
            "other": 0,
            "total": 0,
        }
        
        try:
            # è§„èŒƒåŒ–æ ·æœ¬IDçš„å†…éƒ¨å·¥å…·ï¼š
            # - å…¼å®¹åˆ—åä¸ºBAMæ–‡ä»¶è·¯å¾„/æ–‡ä»¶å/å¸¦STARåç¼€çš„å¤šç§æƒ…å†µ
            # - ç›®æ ‡ï¼šä¸æ ·æœ¬IDï¼ˆå¦‚ SRRxxxxã€æ ·æœ¬ç›®å½•åï¼‰å¯¹é½
            def _normalize_sample_id(name: str) -> str:
                s = str(name or "").strip()
                if not s:
                    return s
                # å»é™¤å¯èƒ½çš„è·¯å¾„å‰ç¼€ï¼ˆåŒæ—¶æ”¯æŒ / ä¸ \\ åˆ†éš”ç¬¦ï¼‰
                if "/" in s:
                    s = s.split("/")[-1]
                if "\\" in s:
                    s = s.split("\\")[-1]
                # å»é™¤å¸¸è§æ‰©å±•å
                for ext in [".bam", ".cram", ".sam", ".txt"]:
                    if s.endswith(ext):
                        s = s[: -len(ext)]
                        break
                # å»é™¤å¸¸è§åç¼€ï¼ˆSTAR/HISAT2 çš„å‘½ååç¼€ï¼Œç‚¹/ä¸‹åˆ’çº¿å˜ä½“ï¼‰
                star_suffixes = [
                    ".Aligned.sortedByCoord.out",
                    ".Aligned.out",
                    ".Aligned",
                    "_Aligned.sortedByCoord.out",
                    "_Aligned.out",
                    "_Aligned",
                    ".hisat2",  # æ¥è‡ª HISAT2 çš„å¸¸è§åç¼€ï¼ˆåœ¨ç§»é™¤ .bam åå¯èƒ½æ®‹ç•™ï¼‰
                    "_hisat2",
                ]
                for suf in star_suffixes:
                    if s.endswith(suf):
                        s = s[: -len(suf)]
                        break
                return s

            with open(summary_file, "r", encoding="utf-8", errors="ignore") as f:
                lines = f.readlines()
                
                if len(lines) < 2:
                    return {"success": False, "error": "æ±‡æ€»æ–‡ä»¶æ ¼å¼é”™è¯¯"}
                
                # è§£ææ ‡é¢˜è¡Œè·å–æ ·æœ¬åç§°ï¼ˆç”¨äºå›é€€ï¼‰
                header = lines[0].strip().split('\t')
                if len(header) < 2:
                    return {"success": False, "error": "æ±‡æ€»æ–‡ä»¶æ ‡é¢˜è¡Œæ ¼å¼é”™è¯¯"}

                # ä¼˜å…ˆä»å‚æ•°æ–‡ä»¶è¯»å–æ ·æœ¬IDé¡ºåºï¼ˆä¸æ‰§è¡Œè¾“å…¥ä¸€è‡´ï¼‰
                sample_ids: List[str] = []
                try:
                    params_path = results_path / "featurecounts" / "featurecounts_params.json"
                    if params_path.exists():
                        with open(params_path, "r", encoding="utf-8") as pf:
                            pf_json = json.load(pf)
                        input_bam_list = pf_json.get("input_bam_list")
                        if isinstance(input_bam_list, str):
                            input_bam_list = json.loads(input_bam_list)
                        if isinstance(input_bam_list, list):
                            for ent in input_bam_list:
                                sid = ent.get("sample_id") or _normalize_sample_id(ent.get("bam_file", ""))
                                sample_ids.append(_normalize_sample_id(sid))
                except Exception:
                    # è‹¥è¯»å–æˆ–è§£æå¤±è´¥ï¼Œå¿½ç•¥å¹¶å›é€€åˆ°header
                    sample_ids = []

                # æ ¡éªŒæ ·æœ¬æ•°æ˜¯å¦ä¸æ±‡æ€»åˆ—æ•°ä¸€è‡´ï¼›å¦åˆ™å›é€€åˆ°headeråˆ—å
                if not sample_ids or len(sample_ids) != (len(header) - 1):
                    sample_names = header[1:]  # ç¬¬ä¸€åˆ—æ˜¯Statusï¼Œåé¢æ˜¯æ ·æœ¬åï¼ˆé€šå¸¸ä¸ºè¾“å…¥BAMçš„æ–‡ä»¶åï¼‰
                    sample_ids = [_normalize_sample_id(nm) for nm in sample_names]

                # åˆå§‹åŒ–æ¯ä¸ªæ ·æœ¬çš„æŒ‡æ ‡
                for sid in sample_ids:
                    
                    sample_metrics.append({
                        "sample_id": sid,
                        "assigned": 0,
                        "unassigned_unmapped": 0,
                        "unassigned_mappingquality": 0,
                        "unassigned_nofeatures": 0,
                        "unassigned_ambiguity": 0,
                        "total_reads": 0
                    })
                
                # è§£ææ¯ä¸€è¡Œç»Ÿè®¡æ•°æ®
                for line in lines[1:]:
                    parts = line.strip().split('\t')
                    if len(parts) < 2:
                        continue
                        
                    status = parts[0]
                    values = [int(v) for v in parts[1:]]
                    
                    # æ›´æ–°æ¯ä¸ªæ ·æœ¬çš„æŒ‡æ ‡
                    for i, value in enumerate(values):
                        if i < len(sample_metrics):
                            if status == "Assigned":
                                sample_metrics[i]["assigned"] = value
                                totals["assigned"] += value
                            elif status == "Unassigned_Unmapped":
                                sample_metrics[i]["unassigned_unmapped"] = value
                            elif status == "Unassigned_MappingQuality":
                                sample_metrics[i]["unassigned_mappingquality"] = value
                                totals["mappingquality"] += value
                            elif status == "Unassigned_NoFeatures":
                                sample_metrics[i]["unassigned_nofeatures"] = value
                                totals["nofeatures"] += value
                            elif status == "Unassigned_Ambiguity":
                                sample_metrics[i]["unassigned_ambiguity"] = value
                                totals["ambiguous"] += value
                
                # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æ€»è¯»æ•°å’Œåˆ†é…ç‡
                for sample_metric in sample_metrics:
                    sample_metric["total_reads"] = (
                        sample_metric["assigned"] +
                        sample_metric["unassigned_unmapped"] +
                        sample_metric["unassigned_mappingquality"] +
                        sample_metric["unassigned_nofeatures"] +
                        sample_metric["unassigned_ambiguity"]
                    )
                    
                    if sample_metric["total_reads"] > 0:
                        sample_metric["assignment_rate"] = round(
                            sample_metric["assigned"] / sample_metric["total_reads"], 4
                        )
                    else:
                        sample_metric["assignment_rate"] = 0.0
                    
                    totals["total"] += sample_metric["total_reads"]
        
        except Exception as e:
            return {"success": False, "error": f"è§£ææ±‡æ€»æ–‡ä»¶å¤±è´¥: {str(e)}"}
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_assignment_rate = totals["assigned"] / totals["total"] if totals["total"] > 0 else 0.0
        
        # è¯»å–è®¡æ•°çŸ©é˜µè·å–åŸºå› æ•°é‡
        gene_count = 0
        try:
            with open(counts_file, "r", encoding="utf-8") as f:
                # è·³è¿‡æ³¨é‡Šè¡Œ
                for line in f:
                    if not line.startswith('#'):
                        gene_count += 1
                gene_count -= 1  # å‡å»æ ‡é¢˜è¡Œ
        except Exception:
            gene_count = 0
        
        # é‡è¦æ–‡ä»¶è·¯å¾„ï¼ˆè‹¥å­˜åœ¨åˆå¹¶çŸ©é˜µï¼Œåˆ™ä¼˜å…ˆæä¾›ï¼‰
        matrix_file = fc_dir / "merged_counts_matrix.txt"

        return {
            "success": True,
            "results_directory": results_directory,
            "summary_file": str(summary_file),
            "counts_file": str(counts_file),
            "matrix_path": str(matrix_file) if matrix_file.exists() else str(counts_file),
            "sample_count": len(sample_metrics),
            "gene_count": gene_count,
            "sample_metrics": sample_metrics,
            "overall_statistics": {
                "total_reads": totals["total"],
                "total_assigned": totals["assigned"],
                "total_unassigned_nofeatures": totals["nofeatures"],
                "total_unassigned_ambiguity": totals["ambiguous"],
                "total_unassigned_mappingquality": totals["mappingquality"],
                "overall_assignment_rate": round(total_assignment_rate, 4)
            }
        }
    
    except Exception as e:
        return {
            "success": False,
            "error": f"è§£æFeatureCountsç»“æœå¤±è´¥: {str(e)}"
        }


# ==================== Analysis æŠ¥å‘Šç”Ÿæˆå·¥å…·å‡½æ•° ====================

@tool
def write_analysis_markdown(
    analysis_report: Dict[str, Any],
    output_dir: Optional[str] = None,
    filename: Optional[str] = None,
    append_llm_section: bool = True
) -> Dict[str, Any]:
    """å°†ç»“æ„åŒ–çš„ analysis_report æ¸²æŸ“ä¸ºå¯è¯»çš„ Markdown æ‘˜è¦æ–‡ä»¶
    
    Args:
        analysis_report: æŒ‰è®¾è®¡æ–‡æ¡£ç¬¬4èŠ‚JSONç»“æ„çº¦å®šç”Ÿæˆçš„åˆ†ææŠ¥å‘Š
        output_dir: ç›®æ ‡ç›®å½•ï¼Œè‹¥ä¸ºç©ºåˆ™ä½¿ç”¨ context.results_dir/reports/timestamp
        filename: æ–‡ä»¶åï¼Œé»˜è®¤ analysis_summary.md
        append_llm_section: æ˜¯å¦è¿½åŠ LLMæ´å¯Ÿåˆ°æ–‡æœ«
    
    Returns:
        Dict: åŒ…å«æˆåŠŸçŠ¶æ€ã€æ–‡ä»¶è·¯å¾„ã€å­—èŠ‚æ•°ç­‰ä¿¡æ¯
    """
    try:
        from .config import get_tools_config
        tools_config = get_tools_config()
        
        # å‚æ•°éªŒè¯
        if not analysis_report or not isinstance(analysis_report, dict):
            return {
                "success": False,
                "error": "analysis_report å‚æ•°æ— æ•ˆæˆ–ä¸ºç©º"
            }
        
        # ç¡®å®šè¾“å‡ºç›®å½•å’Œæ–‡ä»¶å
        filename = filename or "analysis_summary.md"
        
        if output_dir:
            target_dir = Path(output_dir)
        else:
            # ä»reportä¸­è·å–ç›®å½•ä¿¡æ¯
            context = analysis_report.get("context", {})
            results_dir = context.get("results_dir")
            timestamp = context.get("timestamp")
            
            if results_dir and timestamp:
                target_dir = Path(results_dir) / "reports" / timestamp
            else:
                # fallbackåˆ°é»˜è®¤æŠ¥å‘Šç›®å½•
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                target_dir = tools_config.reports_dir / timestamp
        
        # åˆ›å»ºç›®å½•
        target_dir.mkdir(parents=True, exist_ok=True)
        output_path = target_dir / filename
        
        # æ¸²æŸ“Markdownå†…å®¹
        markdown_content = _render_analysis_markdown(analysis_report, append_llm_section)
        
        # å†™å…¥æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        file_size = output_path.stat().st_size
        line_count = markdown_content.count('\n') + 1
        
        return {
            "success": True,
            "path": str(output_path.absolute()),
            "bytes": file_size,
            "lines": line_count,
            "used_output_dir": str(target_dir),
            "used_filename": filename
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"ç”ŸæˆMarkdownæŠ¥å‘Šå¤±è´¥: {str(e)}"
        }


def _render_analysis_markdown(report: Dict[str, Any], append_llm: bool = True) -> str:
    """æ¸²æŸ“åˆ†ææŠ¥å‘Šä¸ºMarkdownæ ¼å¼çš„è¾…åŠ©å‡½æ•°"""
    
    def _safe_get(obj: Dict, *keys, default="-"):
        """å®‰å…¨è·å–åµŒå¥—å­—å…¸å€¼"""
        for key in keys:
            if isinstance(obj, dict) and key in obj:
                obj = obj[key]
            else:
                return default
        return obj if obj is not None else default
    
    def _format_percent(value, default="-"):
        """æ ¼å¼åŒ–ç™¾åˆ†æ¯”"""
        if value is None or value == default:
            return default
        try:
            return f"{float(value) * 100:.1f}%" if 0 <= float(value) <= 1 else f"{float(value):.1f}%"
        except (ValueError, TypeError):
            return default
    
    def _format_number(value, default="-"):
        """æ ¼å¼åŒ–æ•°å­—"""
        if value is None:
            return default
        try:
            if isinstance(value, float):
                return f"{value:,.1f}" if value >= 1000 else f"{value:.3f}"
            else:
                return f"{int(value):,}"
        except (ValueError, TypeError):
            return default
    
    # è·å–æŠ¥å‘Šå„éƒ¨åˆ†
    pipeline = report.get("pipeline", {})
    context = report.get("context", {})
    metrics = report.get("metrics", {})
    per_sample = report.get("per_sample", [])
    summary = report.get("summary", {})
    files = report.get("files", {})
    recommendations = report.get("recommendations", [])
    llm_output = report.get("llm", report.get("llm_output", {}))
    
    # å¼€å§‹æ„å»ºMarkdown
    lines = []
    
    # 1) æ ‡é¢˜ä¸åŸºæœ¬ä¿¡æ¯
    steps_str = " â†’ ".join(pipeline.get("steps", []))
    lines.extend([
        f"# RNA-seq åˆ†ææŠ¥å‘Š",
        "",
        f"**åˆ†ææµç¨‹**: {steps_str}",
        f"**ç‰©ç§**: {pipeline.get('species', 'æœªçŸ¥')}",
        f"**åŸºå› ç»„ç‰ˆæœ¬**: {pipeline.get('genome_version', 'æœªçŸ¥')}",
        f"**æ ·æœ¬æ•°é‡**: {context.get('sample_count', 0)}",
        f"**åˆ†ææ—¶é—´**: {context.get('timestamp', 'æœªçŸ¥')}",
        f"**ç»“æœç›®å½•**: `{context.get('results_dir', 'æœªçŸ¥')}`",
        ""
    ])
    
    # 2) æ€»ä½“ç»“è®ºä¸å…³é”®å‘ç°
    status = summary.get("status", "UNKNOWN")
    status_emoji = {"PASS": "âœ…", "WARN": "âš ï¸", "FAIL": "âŒ"}.get(status, "â“")
    
    samples_info = summary.get("samples", {})
    lines.extend([
        f"## {status_emoji} æ€»ä½“ç»“è®º: {status}",
        "",
        f"**æ ·æœ¬ç»Ÿè®¡**:",
        f"- é€šè¿‡: {samples_info.get('pass', 0)} ä¸ª",
        f"- è­¦å‘Š: {samples_info.get('warn', 0)} ä¸ª", 
        f"- å¤±è´¥: {samples_info.get('fail', 0)} ä¸ª",
        ""
    ])
    
    key_findings = summary.get("key_findings", [])
    if key_findings:
        lines.extend([
            f"**å…³é”®å‘ç°**:",
            *[f"- {finding}" for finding in key_findings],
            ""
        ])
    
    # 3) å„æ­¥éª¤æ€»ä½“æŒ‡æ ‡
    lines.append("## ğŸ“Š æµç¨‹æ­¥éª¤æŒ‡æ ‡")
    lines.append("")
    
    # FastP æŒ‡æ ‡
    fastp_metrics = metrics.get("fastp", {}).get("overall", {})
    if fastp_metrics:
        lines.extend([
            "### FastP è´¨é‡æ§åˆ¶",
            f"- å¹³å‡Q30è´¨é‡: {_format_percent(fastp_metrics.get('average_q30_rate'))}",
            f"- è¯»æ•°é€šè¿‡ç‡: {_format_percent(fastp_metrics.get('overall_read_pass_rate'))}",
            f"- ç¢±åŸºé€šè¿‡ç‡: {_format_percent(fastp_metrics.get('overall_base_pass_rate'))}",
            f"- æ€»readså¤„ç†: {_format_number(fastp_metrics.get('total_reads_before'))} â†’ {_format_number(fastp_metrics.get('total_reads_after'))}",
            ""
        ])
    
    # STAR æŒ‡æ ‡
    star_metrics = metrics.get("star", {}).get("overall", {})
    if star_metrics:
        lines.extend([
            "### STAR åºåˆ—æ¯”å¯¹",
            f"- æ€»ä½“æ¯”å¯¹ç‡: {_format_percent(star_metrics.get('overall_mapping_rate'))}",
            f"- å”¯ä¸€æ¯”å¯¹ç‡: {_format_percent(star_metrics.get('overall_unique_mapping_rate'))}",
            f"- å¤šé‡æ¯”å¯¹ç‡: {_format_percent(star_metrics.get('overall_multi_mapping_rate'))}",
            f"- å¹³å‡é”™é…ç‡: {_format_percent(star_metrics.get('average_mismatch_rate'))}",
            ""
        ])
    
    # FeatureCounts æŒ‡æ ‡  
    fc_metrics = metrics.get("featurecounts", {}).get("overall", {})
    if fc_metrics:
        lines.extend([
            "### FeatureCounts åŸºå› å®šé‡",
            f"- æ•´ä½“åˆ†é…ç‡: {_format_percent(fc_metrics.get('overall_assignment_rate'))}",
            f"- æ€»åˆ†é…reads: {_format_number(fc_metrics.get('total_assigned'))}",
            f"- æœªåˆ†é…(æ— ç‰¹å¾): {_format_number(fc_metrics.get('total_unassigned_nofeatures'))}",
            f"- æœªåˆ†é…(æ­§ä¹‰): {_format_number(fc_metrics.get('total_unassigned_ambiguity'))}",
            ""
        ])
    
    # 4) æ ·æœ¬çº§æ‘˜è¦ - æŒ‰å¥åº·åº¦æ’åº
    if per_sample:
        lines.extend([
            "## ğŸ”¬ æ ·æœ¬è¯¦æƒ…",
            "",
            "| æ ·æœ¬ID | å¥åº·åº¦ | Q30 | æ¯”å¯¹ç‡ | åˆ†é…ç‡ | å¤‡æ³¨ |",
            "|--------|--------|-----|--------|--------|------|"
        ])
        
        # æŒ‰å¥åº·åº¦æ’åº (FAIL > WARN > PASS)
        health_order = {"FAIL": 0, "WARN": 1, "PASS": 2}
        sorted_samples = sorted(per_sample, key=lambda x: health_order.get(x.get("health", "UNKNOWN"), 3))
        
        for sample in sorted_samples:
            sid = sample.get("sample_id", "")
            health = sample.get("health", "")
            health_emoji = {"PASS": "âœ…", "WARN": "âš ï¸", "FAIL": "âŒ"}.get(health, "â“")
            
            # æå–å…³é”®æŒ‡æ ‡
            fastp_data = sample.get("fastp", {})
            star_data = sample.get("star", {})
            fc_data = sample.get("featurecounts", {})
            
            q30 = _format_percent(fastp_data.get("q30_after"))
            mapping = _format_percent(star_data.get("mapping_rate"))
            assignment = _format_percent(fc_data.get("assignment_rate"))
            
            notes = sample.get("notes", [])
            notes_str = ", ".join(notes[:2]) if notes else "-"
            if len(notes) > 2:
                notes_str += "..."
                
            lines.append(f"| {sid} | {health_emoji} {health} | {q30} | {mapping} | {assignment} | {notes_str} |")
        
        lines.append("")
    
    # 5) é‡è¦æ–‡ä»¶
    matrix_path = files.get("matrix_path") or _safe_get(fc_metrics, "matrix_path")
    report_json = files.get("report_json")
    
    if matrix_path or report_json:
        lines.extend([
            "## ğŸ“ é‡è¦æ–‡ä»¶",
            ""
        ])
        if matrix_path:
            lines.append(f"- **è®¡æ•°çŸ©é˜µ**: `{matrix_path}`")
        if report_json:
            lines.append(f"- **è¯¦ç»†æŠ¥å‘Š**: `{report_json}`")
        lines.append("")
    
    # 6) å»ºè®®ä¸åç»­æ­¥éª¤
    if recommendations:
        lines.extend([
            "## ğŸ’¡ å»ºè®®ä¸åç»­æ­¥éª¤",
            ""
        ])
        
        for rec in recommendations:
            rec_type = rec.get("type", "")
            title = rec.get("title", "")
            detail = rec.get("detail", "")
            
            if rec_type == "action":
                lines.append(f"### ğŸ”§ {title}")
            elif rec_type == "next":
                lines.append(f"### ğŸ“ˆ {title}")
            else:
                lines.append(f"### {title}")
            
            lines.extend([f"{detail}", ""])
    
    # 7) LLM æ´å¯Ÿ (å¯é€‰)
    if append_llm and llm_output:
        lines.extend([
            "## ğŸ¤– æ™ºèƒ½åˆ†ææ´å¯Ÿ",
            ""
        ])
        
        global_summary = llm_output.get("global_summary")
        if global_summary:
            lines.extend([
                "### æ€»ä½“è¯„ä¼°",
                global_summary,
                ""
            ])
        
        llm_findings = llm_output.get("key_findings", [])
        if llm_findings:
            lines.extend([
                "### å…³é”®å‘ç°",
                *[f"- {finding}" for finding in llm_findings],
                ""
            ])
        
        per_sample_flags = llm_output.get("per_sample_flags", [])
        if per_sample_flags:
            lines.extend([
                "### æ ·æœ¬çº§é—®é¢˜",
                ""
            ])
            for flag in per_sample_flags:
                sid = flag.get("sample_id", "")
                issues = flag.get("issues", [])
                severity = flag.get("severity", "")
                if issues:
                    lines.append(f"**{sid}** ({severity}): " + ", ".join(issues))
            lines.append("")
        
        risks = llm_output.get("risks", [])
        if risks:
            lines.extend([
                "### âš ï¸ æ½œåœ¨é£é™©",
                *[f"- {risk}" for risk in risks],
                ""
            ])
        
        # å¦‚æœæœ‰é¢å¤–çš„LLMç”Ÿæˆçš„Markdownç‰‡æ®µ
        report_md = llm_output.get("report_md")
        if report_md:
            lines.extend([
                "### è¡¥å……åˆ†æ",
                report_md,
                ""
            ])
    
    # ç”Ÿæˆæ—¶é—´æˆ³
    lines.extend([
        "---",
        f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"
    ])
    
    return "\n".join(lines)
