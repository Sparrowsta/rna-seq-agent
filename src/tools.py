"""
RNA-seqæ™ºèƒ½åˆ†æåŠ©æ‰‹å·¥å…·æ¨¡å—
æä¾›æ•°æ®æ”¶é›†åŠŸèƒ½ï¼Œè¿”å›ç»“æ„åŒ–æ•°æ®ä¾›LLMæ™ºèƒ½å¤„ç†å’Œå±•ç¤º

é‡æ„åŸåˆ™:
- å·¥å…·ä¸“æ³¨çº¯æ•°æ®æ”¶é›†ï¼Œä¸åšæ ¼å¼åŒ–
- ä½¿ç”¨å®˜æ–¹ @tool è£…é¥°å™¨
- ç§»é™¤åŒæ¨¡å¼é€»è¾‘ï¼Œäº¤ç”±LLMå†³å®šå±•ç¤ºæ–¹å¼
- ç®€åŒ–ä»£ç ç»“æ„ï¼Œæé«˜ç»´æŠ¤æ€§
"""

import json
import re
import time
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# ä½¿ç”¨å®˜æ–¹å·¥å…·è£…é¥°å™¨
from langchain_core.tools import tool

# å¯¼å…¥é…ç½®æ¨¡å—
from .config import get_tools_config


# ==================== å·¥å…·å‡½æ•° (ä½¿ç”¨ @tool è£…é¥°å™¨) ====================

@tool
def scan_fastq_files() -> Dict[str, Any]:
    """æ‰«æFASTQæ–‡ä»¶ï¼Œä¼˜å…ˆåœ¨æ•°æ®ç›®å½•(data/fastq)ä¸‹æŸ¥æ‰¾ï¼Œå…¼å®¹å®¹å™¨æŒ‚è½½ç›®å½•ã€‚

    è¿”å›ï¼šæ–‡ä»¶åˆ—è¡¨ã€æ ·æœ¬ä¿¡æ¯å’ŒåŸºæœ¬ç»Ÿè®¡æ•°æ®ã€‚
    """
    config = get_tools_config()
    fastq_extensions = ["*.fastq", "*.fastq.gz", "*.fq", "*.fq.gz"]

    # å®šä¹‰è¦æ’é™¤çš„ç›®å½•ï¼ˆä¸­é—´æ–‡ä»¶å’Œç¼“å­˜ç›®å½•ï¼‰
    exclude_directories = {
        "work", "tmp", "temp", "results", "output",
        ".nextflow", "logs", "cache", "__pycache__"
    }

    # é€‰æ‹©æœç´¢æ ¹ç›®å½•ï¼šä¼˜å…ˆ data/fastqï¼Œå…¶æ¬¡ dataï¼Œæœ€åé¡¹ç›®æ ¹ç›®å½•
    search_roots = []
    try:
        if config.fastq_dir.exists():
            search_roots.append(config.fastq_dir)
        elif config.settings.data_dir.exists():
            search_roots.append(config.settings.data_dir)
        else:
            search_roots.append(config.project_root)
    except Exception:
        search_roots.append(config.project_root)

    # æ‰«ææ‰€æœ‰FASTQæ–‡ä»¶
    all_fastq_files = []
    for root in search_roots:
        for ext in fastq_extensions:
            for file_path in root.rglob(ext):
                if any(excluded in file_path.parts for excluded in exclude_directories):
                    continue
                all_fastq_files.append(file_path)
    
    # æ”¶é›†æ–‡ä»¶ä¿¡æ¯
    file_list = []
    for file_path in all_fastq_files:
        if not file_path.exists() or not file_path.is_file():
            continue
            
        try:
            file_size = file_path.stat().st_size
            
            # æå–æ ·æœ¬ä¿¡æ¯
            filename = file_path.name
            if "_1." in filename or "_R1" in filename:
                sample_name = filename.split("_1.")[0].split("_R1")[0]
                read_type = "R1"
            elif "_2." in filename or "_R2" in filename:
                sample_name = filename.split("_2.")[0].split("_R2")[0]
                read_type = "R2"
            else:
                sample_name = filename.split(".")[0]
                read_type = "single"
            
            file_info = {
                "filename": filename,
                "full_path": str(file_path),
                "directory": str(file_path.parent),
                "size_bytes": file_size,
                "size_mb": round(file_size / 1024 / 1024, 2),
                "extension": file_path.suffix,
                "sample_name": sample_name,
                "read_type": read_type
            }
            file_list.append(file_info)
        except Exception:
            continue
    
    # æŒ‰æ–‡ä»¶åæ’åº
    file_list.sort(key=lambda x: x["filename"])
    
    # æ ·æœ¬ç»Ÿè®¡
    samples = {}
    for file_info in file_list:
        sample_name = file_info["sample_name"]
        read_type = file_info["read_type"]
        
        if sample_name not in samples:
            samples[sample_name] = {"R1": None, "R2": None, "single": None}
        
        samples[sample_name][read_type] = file_info
    
    # ç¡®å®šæµ‹åºç±»å‹
    paired_count = sum(1 for s in samples.values() if s["R1"] and s["R2"])
    single_count = sum(1 for s in samples.values() if s["single"])
    
    if paired_count > 0 and single_count == 0:
        sequencing_type = "paired_end"
    elif single_count > 0 and paired_count == 0:
        sequencing_type = "single_end"
    else:
        sequencing_type = "mixed"
    
    return {
        "detection_status": "success",
        "search_roots": [str(p) for p in search_roots],
        "total_files": len(file_list),
        "total_samples": len(samples),
        "sequencing_type": sequencing_type,
        "paired_samples": paired_count,
        "single_samples": single_count,
        "total_size_mb": sum(f["size_mb"] for f in file_list),
        "files": file_list,
        "samples": samples,
        "scan_timestamp": time.time()
    }


@tool
def scan_system_resources() -> Dict[str, Any]:
    """æ£€æµ‹ç³»ç»Ÿç¡¬ä»¶èµ„æºï¼Œè¿”å›CPUã€å†…å­˜ã€ç£ç›˜å’Œè´Ÿè½½ä¿¡æ¯"""
    try:
        import psutil
        
        # CPUä¿¡æ¯
        cpu_count = psutil.cpu_count(logical=False) or 1
        cpu_freq = psutil.cpu_freq()
        
        # å†…å­˜ä¿¡æ¯
        memory = psutil.virtual_memory()
        memory_gb = memory.total / 1024**3
        available_gb = memory.available / 1024**3
        used_percent = memory.percent
        
        # ç£ç›˜ä¿¡æ¯
        disk = psutil.disk_usage('.')
        disk_total_gb = disk.total / 1024**3
        disk_free_gb = disk.free / 1024**3
        disk_used_percent = (disk.used / disk.total) * 100
        
        # ç³»ç»Ÿè´Ÿè½½
        load_info = {}
        try:
            load_avg = psutil.getloadavg()
            load_ratio = load_avg[0] / max(cpu_count, 1)
            load_info = {
                "load_1min": round(load_avg[0], 2),
                "load_5min": round(load_avg[1], 2),
                "load_15min": round(load_avg[2], 2),
                "load_ratio": round(load_ratio, 2)
            }
        except (AttributeError, OSError):
            load_info = {"error": "platform_not_supported"}
        
        return {
            "detection_status": "success",
            "cpu": {
                "physical_cores": cpu_count,
                "frequency_mhz": cpu_freq.current if cpu_freq else None
            },
            "memory": {
                "total_gb": round(memory_gb, 1),
                "available_gb": round(available_gb, 1),
                "used_percent": round(used_percent, 1),
                "total_bytes": memory.total,
                "available_bytes": memory.available
            },
            "disk": {
                "total_gb": round(disk_total_gb, 1),
                "free_gb": round(disk_free_gb, 1),
                "used_percent": round(disk_used_percent, 1),
                "total_bytes": disk.total,
                "free_bytes": disk.free
            },
            "load": load_info,
            "timestamp": time.time()
        }
        
    except ImportError:
        return {
            "detection_status": "missing_dependency",
            "error": "psutil not installed",
            "install_command": "uv add psutil"
        }
    except Exception as e:
        return {
            "detection_status": "error",
            "error": str(e)
        }


@tool
def scan_genome_files(genome_id: Optional[str] = None) -> Dict[str, Any]:
    """æ‰«æå¯ç”¨çš„å‚è€ƒåŸºå› ç»„é…ç½®ï¼Œè¿”å›åŸºå› ç»„åˆ—è¡¨å’Œæ–‡ä»¶çŠ¶æ€
    
    Args:
        genome_id: å¯é€‰çš„ç‰¹å®šåŸºå› ç»„IDï¼Œç”¨äºé‡ç‚¹å…³æ³¨
    """
    config = get_tools_config()
    genomes_file = config.genomes_config_path
    
    if not genomes_file.exists():
        result = {"detection_status": "no_config_file"}
    else:
        try:
            with open(genomes_file, 'r', encoding='utf-8') as f:
                genomes_data = json.load(f)
            
            if not genomes_data:
                result = {"detection_status": "empty_config"}
            else:
                # æ£€æŸ¥æ¯ä¸ªåŸºå› ç»„çš„æ–‡ä»¶çŠ¶æ€
                genome_status = {}
                for genome_id_key, info in genomes_data.items():
                    fasta_path = info.get('fasta_path', '')
                    gtf_path = info.get('gtf_path', '')
                    
                    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
                    fasta_exists = bool(fasta_path and Path(fasta_path).exists())
                    gtf_exists = bool(gtf_path and Path(gtf_path).exists())
                    
                    # æ£€æŸ¥ç´¢å¼•çŠ¶æ€
                    star_index_exists = False
                    hisat2_index_exists = False
                    
                    if fasta_exists:
                        # STARç´¢å¼•æ£€æŸ¥
                        star_index_dir = config.get_star_index_dir(Path(fasta_path))
                        if star_index_dir.exists():
                            star_index_files = list(star_index_dir.iterdir())
                            star_index_exists = len(star_index_files) > 0
                        
                        # HISAT2ç´¢å¼•æ£€æŸ¥
                        hisat2_index_dir = config.get_hisat2_index_dir(Path(fasta_path))
                        if hisat2_index_dir.exists():
                            ht2_files = list(hisat2_index_dir.glob("*.ht2"))
                            hisat2_index_exists = len(ht2_files) > 0
                    
                    genome_status[genome_id_key] = {
                        "species": info.get('species', ''),
                        "version": info.get('version', ''),
                        "fasta_url": info.get('fasta_url', ''),
                        "gtf_url": info.get('gtf_url', ''),
                        "fasta_path": fasta_path,
                        "gtf_path": gtf_path,
                        "fasta_exists": fasta_exists,
                        "gtf_exists": gtf_exists,
                        "complete": fasta_exists and gtf_exists,
                        "star_index_exists": star_index_exists,
                        "hisat2_index_exists": hisat2_index_exists
                    }
                
                result = {
                    "detection_status": "success",
                    "total_genomes": len(genomes_data),
                    "available_genomes": len([g for g in genome_status.values() if g["complete"]]),
                    "genomes": genome_status,
                    "config_path": str(genomes_file)
                }
        except Exception as e:
            result = {
                "detection_status": "error",
                "error": str(e)
            }
    
    # å¦‚æœæŒ‡å®šäº†ç‰¹å®šåŸºå› ç»„ï¼Œæ·»åŠ æ ‡è®°
    if genome_id:
        result["requested_genome"] = genome_id
    
    return result


@tool
def get_project_overview() -> Dict[str, Any]:
    """è·å–é¡¹ç›®æ•´ä½“çŠ¶æ€æ¦‚è§ˆï¼ŒåŒ…æ‹¬æ•°æ®ã€åŸºå› ç»„ã€ç³»ç»Ÿèµ„æºå’Œåˆ†æå†å²"""
    # ä½¿ç”¨ BaseTool.invoke ä»¥é¿å…åœ¨å·¥å…·å†…éƒ¨ç›¸äº’è°ƒç”¨äº§ç”Ÿå¼ƒç”¨è­¦å‘Š
    return {
        "fastq_data": scan_fastq_files.invoke({}),
        "genome_status": scan_genome_files.invoke({}),
        "system_resources": scan_system_resources.invoke({}),
        "analysis_history": list_analysis_history.invoke({}),
        "overview_timestamp": time.time()
    }


@tool
def list_analysis_history() -> Dict[str, Any]:
    """è·å–å†å²åˆ†æè®°å½•ï¼Œè¿”å›åˆ†ææ—¶é—´ã€é…ç½®å’Œç»“æœä¿¡æ¯"""
    config = get_tools_config()
    reports_dir = config.reports_dir
    
    if not reports_dir.exists():
        return {
            "detection_status": "no_history",
            "total_analyses": 0,
            "analyses": []
        }
    
    # æ‰«ææ—¶é—´æˆ³æ ¼å¼çš„å½’æ¡£æ–‡ä»¶å¤¹
    analyses = []
    for item in reports_dir.iterdir():
        if item.is_dir() and not item.name.startswith('.') and item.name != "latest":
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ—¶é—´æˆ³æ ¼å¼ (YYYYMMDD_HHMMSS)
            if len(item.name) == 15 and item.name[8] == '_':
                try:
                    timestamp = datetime.strptime(item.name, "%Y%m%d_%H%M%S")
                    
                    # æ£€æŸ¥å½’æ¡£å†…å®¹
                    analysis_info = {
                        "timestamp_str": item.name,
                        "timestamp": timestamp.timestamp(),
                        "path": str(item),
                        "files": {}
                    }
                    
                    # æ£€æŸ¥å„ç±»æ–‡ä»¶
                    for file_name in ["analysis_report.json", "analysis_summary.md", 
                                    "runtime_config.json", "execution_log.txt"]:
                        file_path = item / file_name
                        if file_path.exists():
                            analysis_info["files"][file_name] = {
                                "exists": True,
                                "size_bytes": file_path.stat().st_size
                            }
                        else:
                            analysis_info["files"][file_name] = {"exists": False}
                    
                    # å°è¯•è¯»å–é…ç½®ä¿¡æ¯
                    runtime_config = item / "runtime_config.json"
                    if runtime_config.exists():
                        try:
                            with open(runtime_config, 'r', encoding='utf-8') as f:
                                config_data = json.load(f)
                            analysis_info["config"] = config_data.get("nextflow_params", {})
                        except Exception:
                            analysis_info["config"] = {}
                    
                    # è®¡ç®—æ€»å¤§å°
                    total_size = sum(f.stat().st_size for f in item.rglob("*") if f.is_file())
                    analysis_info["total_size_bytes"] = total_size
                    
                    analyses.append(analysis_info)
                except ValueError:
                    continue
    
    # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰
    analyses.sort(key=lambda x: x["timestamp"], reverse=True)
    
    return {
        "detection_status": "success",
        "total_analyses": len(analyses),
        "latest_analysis": analyses[0] if analyses else None,
        "analyses": analyses
    }


@tool
def check_tool_availability(tool_name: str) -> Dict[str, Any]:
    """æ£€æµ‹ç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·çš„å¯ç”¨æ€§
    
    Args:
        tool_name: å·¥å…·åç§° (fastp, star, hisat2, featurecounts)
    """
    tool_configs = {
        "fastp": ("qc_env", ["fastp", "--version"]),
        "star": ("align_env", ["STAR", "--version"]), 
        "hisat2": ("align_env", ["hisat2", "--version"]),
        "featurecounts": ("quant_env", ["featureCounts", "-v"])
    }
    
    if tool_name.lower() not in tool_configs:
        return {
            "tool_name": tool_name,
            "error": f"æœªçŸ¥å·¥å…·: {tool_name}",
            "available_tools": list(tool_configs.keys()),
            "available": False
        }
    
    env_name, cmd = tool_configs[tool_name.lower()]
    
    # æ‰§è¡Œå·¥å…·æ£€æµ‹
    detection_data = {
        "tool_name": tool_name,
        "environment": env_name,
        "command": cmd,
        "timestamp": time.time()
    }
    
    try:
        # æ‰§è¡Œç‰ˆæœ¬æ£€æµ‹å‘½ä»¤
        full_cmd = ['micromamba', 'run', '-n', env_name] + cmd
        result = subprocess.run(full_cmd, capture_output=True, text=True, timeout=15)
        
        detection_data.update({
            "command_executed": True,
            "return_code": result.returncode,
            "stdout": result.stdout,
            "stderr": result.stderr,
            "available": result.returncode == 0
        })
            
    except subprocess.TimeoutExpired:
        detection_data.update({
            "command_executed": False,
            "error": "timeout",
            "timeout_seconds": 15,
            "available": False
        })
    except FileNotFoundError:
        detection_data.update({
            "command_executed": False,
            "error": "micromamba_not_found",
            "available": False
        })
    except Exception as e:
        detection_data.update({
            "command_executed": False,
            "error": str(e),
            "available": False
        })
    
    return detection_data


@tool
def add_genome_config(user_input: str) -> Dict[str, Any]:
    """æ™ºèƒ½è§£æç”¨æˆ·è¾“å…¥å¹¶æ·»åŠ åŸºå› ç»„é…ç½®
    
    Args:
        user_input: åŒ…å«åŸºå› ç»„ä¿¡æ¯å’ŒURLçš„ç”¨æˆ·è¾“å…¥
    """
    try:
        if not user_input.strip():
            return {
                "success": False,
                "error": "è¯·æä¾›åŸºå› ç»„ä¿¡æ¯ï¼ŒåŒ…å«FASTAå’ŒGTFæ–‡ä»¶çš„URL"
            }
        
        # ä½¿ç”¨LLMè§£æç”¨æˆ·è¾“å…¥
        from .core import get_shared_llm
        
        llm = get_shared_llm()
        
        system_prompt = """ä½ æ˜¯åŸºå› ç»„ä¿¡æ¯è§£æä¸“å®¶ã€‚ç”¨æˆ·ä¼šæä¾›åŒ…å«FASTAå’ŒGTFæ–‡ä»¶URLçš„æ–‡æœ¬ï¼Œä½ éœ€è¦æå–ä¿¡æ¯å¹¶è¿”å›JSONæ ¼å¼ï¼š

{
  "genome_id": "åŸºå› ç»„æ ‡è¯†ç¬¦(å¦‚hg38,mm39,ce11)",
  "species": "ç‰©ç§åç§°(å¦‚human,mouse,caenorhabditis_elegans)", 
  "version": "ç‰ˆæœ¬å·(é€šå¸¸ä¸genome_idç›¸åŒ)",
  "fasta_url": "FASTAæ–‡ä»¶å®Œæ•´URL",
  "gtf_url": "GTFæ–‡ä»¶å®Œæ•´URL"
}

è¦æ±‚ï¼š
1. ä»URLä¸­æ™ºèƒ½è¯†åˆ«åŸºå› ç»„ç‰ˆæœ¬å’Œç‰©ç§
2. å¸¸è§æ˜ å°„ï¼šhg->human, mm->mouse, ce->caenorhabditis_elegans, dm->drosophila, rn->rat
3. å¦‚æœæ— æ³•ç¡®å®šï¼Œæ ¹æ®ç”Ÿç‰©ä¿¡æ¯å­¦å¸¸è¯†åˆç†æ¨æ–­
4. åªè¿”å›æœ‰æ•ˆçš„JSONï¼Œä¸è¦å…¶ä»–è§£é‡Š"""

        # è§£æç”¨æˆ·è¾“å…¥
        response = llm.invoke(f"System: {system_prompt}\n\nHuman: è¯·è§£æï¼š{user_input}")
        parsed_content = str(response.content).strip()
        
        # æå–JSONå†…å®¹
        try:
            genome_info = json.loads(parsed_content)
        except json.JSONDecodeError:
            json_match = re.search(r'\{.*\}', parsed_content, re.DOTALL)
            if not json_match:
                return {"success": False, "error": f"LLMè§£æå¤±è´¥ï¼š{parsed_content}"}
            genome_info = json.loads(json_match.group())
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        required_fields = ['genome_id', 'species', 'version', 'fasta_url', 'gtf_url']
        missing_fields = [field for field in required_fields if not genome_info.get(field)]
        if missing_fields:
            return {"success": False, "error": f"ç¼ºå°‘å­—æ®µï¼š{missing_fields}"}
        
        # æ„å»ºé…ç½®
        genome_id = genome_info['genome_id']
        species = genome_info['species']
        version = genome_info['version']
        
        fasta_path = f"genomes/{species}/{version}/{version}.fa"
        gtf_path = f"genomes/{species}/{version}/{version}.gtf"
        
        new_genome_config = {
            "species": species,
            "version": version,
            "fasta_path": fasta_path,
            "gtf_path": gtf_path,
            "fasta_url": genome_info['fasta_url'],
            "gtf_url": genome_info['gtf_url']
        }
        
        # è¯»å–ç°æœ‰é…ç½®å¹¶æ·»åŠ 
        config = get_tools_config()
        genomes_file = config.genomes_config_path
        
        if genomes_file.exists():
            with open(genomes_file, 'r', encoding='utf-8') as f:
                genomes_data = json.load(f)
        else:
            genomes_data = {}
        
        # æ£€æŸ¥é‡å¤
        if genome_id in genomes_data:
            return {
                "success": False,
                "error": f"åŸºå› ç»„ {genome_id} å·²å­˜åœ¨",
                "existing_config": genomes_data[genome_id]
            }
        
        # ä¿å­˜é…ç½®
        genomes_data[genome_id] = new_genome_config
        config.path_manager.ensure_directory(config.settings.config_dir)
        
        with open(genomes_file, 'w', encoding='utf-8') as f:
            json.dump(genomes_data, f, indent=2, ensure_ascii=False)
        
        return {
            "success": True,
            "genome_id": genome_id,
            "config": new_genome_config,
            "message": f"æˆåŠŸæ·»åŠ åŸºå› ç»„é…ç½®ï¼š{genome_id}"
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"æ·»åŠ åŸºå› ç»„é…ç½®æ—¶å‡ºé”™ï¼š{str(e)}"
        }


@tool
def get_help() -> Dict[str, Any]:
    """è·å–ç³»ç»Ÿå¸®åŠ©ä¿¡æ¯å’ŒåŠŸèƒ½è¯´æ˜"""
    return {
        "system_name": "RNA-seqæ™ºèƒ½åˆ†æåŠ©æ‰‹",
        "current_mode": "Normalæ¨¡å¼ (é¡¹ç›®ä¿¡æ¯ä¸­å¿ƒ)",
        "core_tools": [
            "é¡¹ç›®æ¦‚è§ˆ - ä¸€é”®æŸ¥çœ‹é¡¹ç›®å®Œæ•´çŠ¶æ€å’Œå¥åº·åº¦",
            "å†å²åˆ†æè®°å½• - æµè§ˆå·²å®Œæˆçš„åˆ†æå’Œå¯å¤ç”¨é…ç½®",
            "æŸ¥çœ‹FASTQæ–‡ä»¶ - è¯¦ç»†æ‰«ææ‰€æœ‰æµ‹åºæ•°æ®æ–‡ä»¶",
            "æŸ¥çœ‹åŸºå› ç»„ä¿¡æ¯ - æ˜¾ç¤ºå¯ç”¨å‚è€ƒåŸºå› ç»„çŠ¶æ€",
            "æ·»åŠ åŸºå› ç»„é…ç½® - æ™ºèƒ½è§£æå¹¶æ·»åŠ æ–°çš„å‚è€ƒåŸºå› ç»„"
        ],
        "next_steps": [
            "é¦–æ¬¡ä½¿ç”¨å»ºè®®è¿è¡Œ 'é¡¹ç›®æ¦‚è§ˆ' äº†è§£é¡¹ç›®çŠ¶æ€",
            "è¾“å…¥ '/plan' è¿›å…¥è®¡åˆ’æ¨¡å¼è¿›è¡Œæ·±åº¦åˆ†æè§„åˆ’"
        ],
        "mode_description": "Normalæ¨¡å¼ä¸“æ³¨å¿«é€Ÿä¿¡æ¯æŸ¥çœ‹å’Œé¡¹ç›®æ¦‚è§ˆï¼ŒPlanæ¨¡å¼è´Ÿè´£æ·±åº¦æ£€æµ‹å’Œåˆ†ææ–¹æ¡ˆåˆ¶å®š"
    }


# ==================== FastPä¸“ç”¨å·¥å…·å‡½æ•° ====================

@tool
def run_nextflow_fastp(fastp_params: Dict[str, Any], sample_info: Dict[str, Any]) -> Dict[str, Any]:
    """æ‰§è¡ŒNextflow FastPè´¨é‡æ§åˆ¶æµç¨‹
    
    Args:
        fastp_params: FastPå‚æ•°å­—å…¸ï¼Œä¾‹å¦‚ {"qualified_quality_phred": 25, "length_required": 50}
        sample_info: æ ·æœ¬ä¿¡æ¯ï¼ŒåŒ…å«sample_groupsç­‰
    
    Returns:
        æ‰§è¡Œç»“æœå­—å…¸ï¼ŒåŒ…å«çŠ¶æ€ã€è¾“å‡ºè·¯å¾„ã€æ‰§è¡Œæ—¥å¿—ç­‰
    """
    try:
        config = get_tools_config()
        
        # éªŒè¯å¿…éœ€å‚æ•°
        if not fastp_params:
            return {
                "success": False,
                "error": "FastPå‚æ•°ä¸èƒ½ä¸ºç©º",
                "execution_time": 0
            }
        
        if not sample_info.get("sample_groups"):
            return {
                "success": False, 
                "error": "æ ·æœ¬ä¿¡æ¯ç¼ºå¤±",
                "execution_time": 0
            }
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # ç»Ÿä¸€æ•°æ®æ ¹ç›®å½•æ¥æºï¼šå§‹ç»ˆä»¥ Settings().data_dir ä¸ºå‡†ï¼Œä¸ä» sample_info è¯»å–
        base_data_path = str(config.settings.data_dir)

        # ç»“æœç›®å½•ï¼šä¼˜å…ˆä½¿ç”¨ sample_info æä¾›ï¼›å¦åˆ™æŒ‰æ—¶é—´æˆ³ç”Ÿæˆåˆ° data/results ä¸‹
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_dir = sample_info.get("results_dir") or str(config.settings.data_dir / "results" / f"fastp_{timestamp}")

        # å·¥ä½œç›®å½•ï¼šå›ºå®šæ”¾åˆ° data/tmp ä¸‹ï¼ˆåŒ…å«æ—¶é—´æˆ³/ç»“æœåï¼‰
        run_id = results_dir.split('/')[-1] if '/' in str(results_dir) else timestamp
        temp_dir = Path(base_data_path) / "tmp" / f"nextflow_fastp_{run_id}"
        temp_dir.mkdir(parents=True, exist_ok=True)
        results_dir = Path(results_dir)
        print(f"ğŸ“ è¿è¡Œç›®å½•: base={base_data_path} work={temp_dir} results={results_dir}")
        
        nextflow_params: Dict[str, Any] = {}
        for raw_key, value in (fastp_params or {}).items():
            if value is None:
                continue
            key = str(raw_key)
            if key.startswith("--"):
                key = key[2:]
            if key == "thread":
                key = "threads"
            nextflow_params[key] = value
        
        # æ·»åŠ æ ·æœ¬ç»„ä¿¡æ¯
        nextflow_params["sample_groups"] = sample_info["sample_groups"]
        
        # è®¾ç½®ç»“æœç›®å½•å’Œæ•°æ®è·¯å¾„
        nextflow_params["results_dir"] = str(results_dir)
        nextflow_params["data"] = base_data_path
        
        # åˆ›å»ºNextflowå‚æ•°æ–‡ä»¶
        params_file = temp_dir / "fastp_params.json"
        with open(params_file, 'w', encoding='utf-8') as f:
            json.dump(nextflow_params, f, indent=2, ensure_ascii=False)
        
        # æ„å»ºNextflowå‘½ä»¤ï¼ˆå…¼å®¹Dockerä¸æœ¬åœ°è·¯å¾„ï¼‰
        # 1) ä¼˜å…ˆä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ fastp.nfï¼ˆæœ¬åœ°å¼€å‘ï¼‰
        # 2) Dockeré•œåƒä¸­ fastp.nf ä½äºæ ¹è·¯å¾„ '/'ï¼ˆè§ Dockerfile COPY fastp.nf /ï¼‰
        nf_candidates = [
            config.settings.project_root / "fastp.nf",
            Path("/fastp.nf")
        ]
        nextflow_script = None
        for cand in nf_candidates:
            if cand.exists():
                nextflow_script = cand
                break
        if nextflow_script is None:
            return {
                "success": False,
                "error": "æœªæ‰¾åˆ° fastp.nf è„šæœ¬ï¼Œè¯·æ£€æŸ¥å®¹å™¨å†…æ˜¯å¦å­˜åœ¨ /fastp.nf æˆ–æœ¬åœ°é¡¹ç›®æ ¹ç›®å½•",
                "searched": [str(p) for p in nf_candidates]
            }

        cmd = [
            "nextflow", "run",
            str(nextflow_script),
            "-params-file", str(params_file),
            "-work-dir", str(temp_dir / "work"),
            "--data", str(config.settings.data_dir)
        ]
        
        print(f"ğŸš€ æ‰§è¡ŒNextflow FastPæµæ°´çº¿...")
        print(f"   å‚æ•°æ–‡ä»¶: {params_file}")
        print(f"   å·¥ä½œç›®å½•: {temp_dir / 'work'}")
        print(f"   ç»“æœç›®å½•: {results_dir}")
        
        # æ‰§è¡ŒNextflowæµæ°´çº¿
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=1800,  # 30åˆ†é’Ÿè¶…æ—¶
            cwd=config.settings.project_root
        )
        
        execution_time = time.time() - start_time
        
        if result.returncode == 0:
            # è§£æè¾“å‡ºç»“æœ
            sample_count = len(sample_info["sample_groups"])
            
            return {
                "success": True,
                "message": f"FastPè´¨æ§å®Œæˆï¼Œå¤„ç†äº†{sample_count}ä¸ªæ ·æœ¬",
                "execution_time": execution_time,
                "results_dir": str(results_dir),
                "work_dir": str(temp_dir / "work"),
                "params_file": str(params_file),
                "sample_count": sample_count,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "nextflow_params": nextflow_params
            }
        else:
            return {
                "success": False,
                "error": f"Nextflowæ‰§è¡Œå¤±è´¥ (è¿”å›ç : {result.returncode})",
                "execution_time": execution_time,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "cmd": " ".join(cmd)
            }
            
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "error": "Nextflowæ‰§è¡Œè¶…æ—¶ï¼ˆ30åˆ†é’Ÿï¼‰",
            "execution_time": time.time() - start_time
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"æ‰§è¡ŒFastPæµæ°´çº¿æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
            "execution_time": 0
        }


@tool  
def parse_fastp_results(results_directory: str) -> Dict[str, Any]:
    """è§£æFastPç»“æœæ–‡ä»¶ï¼Œæå–å®¢è§‚è´¨é‡æŒ‡æ ‡ä¾›LLMåˆ†æ
    
    Args:
        results_directory: FastPç»“æœç›®å½•è·¯å¾„
    
    Returns:
        è§£æçš„è´¨é‡æŒ‡æ ‡å­—å…¸ï¼ŒåŒ…å«å„æ ·æœ¬çš„è´¨é‡ç»Ÿè®¡ã€è¿‡æ»¤ç‡ç­‰å®¢è§‚æ•°æ®
        æ³¨æ„ï¼šæ­¤å·¥å…·ä»…æä¾›å®¢è§‚æ•°æ®åˆ†æï¼Œä¸ç”Ÿæˆä¼˜åŒ–å»ºè®®ã€‚ä¼˜åŒ–å»ºè®®ç”±LLMåŸºäºè¿™äº›æ•°æ®æ™ºèƒ½ç”Ÿæˆã€‚
    """
    try:
        results_dir = Path(results_directory)
        if not results_dir.exists():
            return {
                "success": False,
                "error": f"ç»“æœç›®å½•ä¸å­˜åœ¨: {results_directory}"
            }
        
        # æŸ¥æ‰¾æ‰€æœ‰FastP JSONæŠ¥å‘Šæ–‡ä»¶
        json_files = list(results_dir.rglob("*.fastp.json"))
        
        if not json_files:
            return {
                "success": False,
                "error": "æœªæ‰¾åˆ°FastP JSONæŠ¥å‘Šæ–‡ä»¶"
            }
        
        sample_metrics = []
        overall_stats = {
            "total_reads_before": 0,
            "total_reads_after": 0,
            "total_bases_before": 0,
            "total_bases_after": 0,
            "q20_rates": [],
            "q30_rates": [],
            "gc_contents": []
        }
        
        # è§£ææ¯ä¸ªæ ·æœ¬çš„JSONæŠ¥å‘Š
        for json_file in json_files:
            try:
                with open(json_file, 'r') as f:
                    fastp_data = json.load(f)
                
                # æå–æ ·æœ¬IDï¼ˆä»æ–‡ä»¶åï¼‰
                sample_id = json_file.stem.replace('.fastp', '')
                
                # æå–è´¨é‡æŒ‡æ ‡
                summary = fastp_data.get("summary", {})
                before_filtering = summary.get("before_filtering", {})
                after_filtering = summary.get("after_filtering", {})
                
                # è¯»å–æ•°å’Œç¢±åŸºæ•°
                reads_before = before_filtering.get("total_reads", 0)
                reads_after = after_filtering.get("total_reads", 0)
                bases_before = before_filtering.get("total_bases", 0)
                bases_after = after_filtering.get("total_bases", 0)
                
                # è´¨é‡æŒ‡æ ‡
                q20_before = before_filtering.get("q20_rate", 0)
                q20_after = after_filtering.get("q20_rate", 0)
                q30_before = before_filtering.get("q30_rate", 0)
                q30_after = after_filtering.get("q30_rate", 0)
                
                # GCå«é‡
                gc_before = before_filtering.get("gc_content", 0)
                gc_after = after_filtering.get("gc_content", 0)
                
                # è¿‡æ»¤ç‡è®¡ç®—
                read_pass_rate = reads_after / reads_before if reads_before > 0 else 0
                base_pass_rate = bases_after / bases_before if bases_before > 0 else 0
                
                # å¹³å‡é•¿åº¦
                avg_length_before = bases_before / reads_before if reads_before > 0 else 0
                avg_length_after = bases_after / reads_after if reads_after > 0 else 0
                
                sample_metric = {
                    "sample_id": sample_id,
                    "json_file": str(json_file),
                    "reads_before": reads_before,
                    "reads_after": reads_after,
                    "bases_before": bases_before,
                    "bases_after": bases_after,
                    "read_pass_rate": round(read_pass_rate, 4),
                    "base_pass_rate": round(base_pass_rate, 4),
                    "q20_before": round(q20_before, 4),
                    "q20_after": round(q20_after, 4),
                    "q30_before": round(q30_before, 4),
                    "q30_after": round(q30_after, 4),
                    "gc_content_before": round(gc_before, 4),
                    "gc_content_after": round(gc_after, 4),
                    "avg_length_before": round(avg_length_before, 1),
                    "avg_length_after": round(avg_length_after, 1),
                    "quality_improvement": {
                        "q20_improvement": round(q20_after - q20_before, 4),
                        "q30_improvement": round(q30_after - q30_before, 4)
                    }
                }
                
                sample_metrics.append(sample_metric)
                
                # ç´¯ç§¯æ€»ä½“ç»Ÿè®¡
                overall_stats["total_reads_before"] += reads_before
                overall_stats["total_reads_after"] += reads_after
                overall_stats["total_bases_before"] += bases_before
                overall_stats["total_bases_after"] += bases_after
                overall_stats["q20_rates"].append(q20_after)
                overall_stats["q30_rates"].append(q30_after)
                overall_stats["gc_contents"].append(gc_after)
                
            except Exception as e:
                sample_metrics.append({
                    "sample_id": json_file.stem.replace('.fastp', ''),
                    "json_file": str(json_file),
                    "error": f"è§£æå¤±è´¥: {str(e)}"
                })
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        total_samples = len([m for m in sample_metrics if "error" not in m])
        if total_samples > 0:
            overall_read_pass_rate = overall_stats["total_reads_after"] / overall_stats["total_reads_before"]
            overall_base_pass_rate = overall_stats["total_bases_after"] / overall_stats["total_bases_before"]
            avg_q20_rate = sum(overall_stats["q20_rates"]) / len(overall_stats["q20_rates"])
            avg_q30_rate = sum(overall_stats["q30_rates"]) / len(overall_stats["q30_rates"])
            avg_gc_content = sum(overall_stats["gc_contents"]) / len(overall_stats["gc_contents"])
        else:
            overall_read_pass_rate = 0
            overall_base_pass_rate = 0
            avg_q20_rate = 0
            avg_q30_rate = 0
            avg_gc_content = 0
        
        # è´¨é‡è¯„ä¼°ï¼ˆä»…æä¾›å®¢è§‚æŒ‡æ ‡ï¼Œä¸ç”Ÿæˆä¼˜åŒ–å»ºè®®ï¼‰
        quality_assessment = {
            "overall_quality": "good" if avg_q30_rate > 0.85 else "moderate" if avg_q30_rate > 0.7 else "poor",
            "pass_rate_status": "good" if overall_read_pass_rate > 0.8 else "moderate" if overall_read_pass_rate > 0.6 else "poor"
        }
        
        return {
            "success": True,
            "total_samples": total_samples,
            "results_directory": results_directory,
            "sample_metrics": sample_metrics,
            "overall_statistics": {
                "total_reads_before": overall_stats["total_reads_before"],
                "total_reads_after": overall_stats["total_reads_after"],
                "total_bases_before": overall_stats["total_bases_before"],
                "total_bases_after": overall_stats["total_bases_after"],
                "overall_read_pass_rate": round(overall_read_pass_rate, 4),
                "overall_base_pass_rate": round(overall_base_pass_rate, 4),
                "average_q20_rate": round(avg_q20_rate, 4),
                "average_q30_rate": round(avg_q30_rate, 4),
                "average_gc_content": round(avg_gc_content, 4)
            },
            "quality_assessment": quality_assessment
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"è§£æFastPç»“æœå¤±è´¥: {str(e)}"
        }
