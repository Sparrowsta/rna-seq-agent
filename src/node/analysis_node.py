"""
AnalysisèŠ‚ç‚¹  - åŸºäºè®¾è®¡æ–‡æ¡£çš„ç»¼åˆåˆ†æå®ç°

åŠŸèƒ½ï¼š
- è§£æ FastP/STAR/FeatureCounts ä¸‰æ­¥ç»“æœ
- è®¡ç®—æ ·æœ¬å¥åº·åº¦å’Œæ€»ä½“ç»“è®º
- ç”Ÿæˆç»“æ„åŒ– JSON æŠ¥å‘Šå’Œ Markdown æ‘˜è¦
- é›†æˆ LLM æ™ºèƒ½åˆ†æ
- æ”¯æŒå†å²å½’æ¡£å’ŒæŠ¥å‘Šè½ç›˜
"""

import json
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

from ..state import AgentState, LLMAnalysisModel
from ..prompts import ANALYSIS_LLM_SYSTEM_PROMPT
from ..tools import (
    parse_fastp_results, 
    parse_star_metrics, 
    parse_featurecounts_metrics,
    write_analysis_markdown
)
from ..core import get_shared_llm
from ..logging_bootstrap import get_logger, log_llm_preview

logger = get_logger("rna.analysis")


async def analysis_node(state: AgentState) -> Dict[str, Any]:
    """
    AnalysisèŠ‚ç‚¹  - ç»¼åˆåˆ†æå®ç°
    
    æ‰§è¡Œæµç¨‹ï¼š
    1. å‰ç½®æ ¡éªŒ - æ£€æŸ¥æ‰€æœ‰æ­¥éª¤ç»“æœ
    2. è§£æä¸‰æ­¥æŒ‡æ ‡ - è°ƒç”¨è§£æå™¨å·¥å…·
    3. æ ·æœ¬IDå½’ä¸€åŒ–å’Œå¯¹é½
    4. æŒ‡æ ‡åˆå¹¶ä¸å¥åº·åº¦è¯„ä¼°
    5. LLMæ™ºèƒ½æ€»ç»“ï¼ˆå¯é€‰ï¼‰
    6. æŠ¥å‘Šè½ç›˜ - JSONå’ŒMarkdown
    7. çŠ¶æ€å›å¡«ä¸æ¸…ç†
    """
    logger.info("ç»¼åˆåˆ†æèŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")
    
    try:
        # 1. å‰ç½®æ ¡éªŒ
        validation_result = _validate_input_results(state)
        if not validation_result["success"]:
            return _create_error_response(validation_result["error"])
        
        results_dir = validation_result["results_dir"]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        logger.info(f"åˆ†æç»“æœç›®å½•: {results_dir}")
        
        # 2. è§£æä¸‰æ­¥æŒ‡æ ‡
        logger.info("è§£æ FastP/STAR/FeatureCounts ç»“æœ...")
        parsing_result = _parse_pipeline_metrics(results_dir)
        
        if not parsing_result["success"]:
            return _create_error_response(f"æŒ‡æ ‡è§£æå¤±è´¥: {parsing_result['error']}")
        
        fastp_data = parsing_result["fastp"]
        star_data = parsing_result["star"] 
        featurecounts_data = parsing_result["featurecounts"]
        parsing_status = parsing_result.get("parsing_status", {})
        parsing_errors = parsing_result.get("parsing_errors", {})
        
        # 3. æ ·æœ¬IDå½’ä¸€åŒ–å’Œå¯¹é½
        logger.debug("æ ·æœ¬IDå½’ä¸€åŒ–å’ŒæŒ‡æ ‡å¯¹é½...")
        alignment_result = _align_sample_metrics(
            state.nextflow_config.get("sample_groups", []),
            fastp_data, star_data, featurecounts_data
        )
        
        # 4. æŒ‡æ ‡åˆå¹¶ä¸å¥åº·åº¦è¯„ä¼°
        logger.debug("è®¡ç®—æ ·æœ¬å¥åº·åº¦å’Œæ€»ä½“ç»“è®º...")
        assessment_result = _assess_sample_health(alignment_result)
        
        # 5. æ„å»ºåŸºç¡€æŠ¥å‘Šç»“æ„
        base_report = _build_base_report(
            state, results_dir, timestamp, 
            fastp_data, star_data, featurecounts_data,
            alignment_result, assessment_result
        )
        
        # æ·»åŠ è§£æçŠ¶æ€åˆ°æŠ¥å‘Šä¸­ï¼Œå¢å¼ºå¯è§‚æµ‹æ€§
        if parsing_errors:
            base_report.setdefault("files", {})["parsing_errors"] = parsing_errors
            failed_steps = [step for step, success in parsing_status.items() if not success]
            if failed_steps:
                base_report.setdefault("summary", {}).setdefault("key_findings", []).append(
                    f"âš ï¸ è§£æå¤±è´¥çš„æ­¥éª¤: {', '.join(failed_steps)}")
                logger.warning(f"éƒ¨åˆ†è§£æå¤±è´¥: {failed_steps}")
        
        # 6. LLMæ™ºèƒ½æ€»ç»“ï¼ˆå¯é€‰ä½†æ¨èï¼‰
        logger.info("æ‰§è¡ŒLLMæ™ºèƒ½åˆ†æ...")
        llm_result = await _execute_llm_analysis(base_report)
        
        # å°†LLMç»“æœåˆå¹¶åˆ°æŠ¥å‘Šä¸­ï¼Œå¹¶å¢å¼ºå¯è§‚æµ‹æ€§
        if llm_result["success"]:
            base_report["llm"] = llm_result["analysis"]
            logger.info("LLMæ™ºèƒ½åˆ†æå®Œæˆ")
        else:
            base_report["llm_error"] = llm_result["error"]
            # åœ¨key_findingsä¸­æ·»åŠ LLMé™çº§æç¤ºï¼Œæå‡ç”¨æˆ·å¯è§æ€§
            base_report.setdefault("summary", {}).setdefault("key_findings", []).insert(0, 
                f"ğŸ¤– LLMæ™ºèƒ½åˆ†æä¸å¯ç”¨ï¼ˆé™çº§ï¼‰ï¼š{llm_result['error']}")
            logger.warning(f"LLMåˆ†æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åˆ†çº§ç»“æœ: {llm_result['error']}")
        
        # 7. æŠ¥å‘Šè½ç›˜
        logger.info("ç”Ÿæˆå¹¶ä¿å­˜åˆ†ææŠ¥å‘Š...")
        report_result = _save_reports(base_report, results_dir, timestamp)
        
        if not report_result["success"]:
            logger.error(f"æŠ¥å‘Šä¿å­˜å¤±è´¥: {report_result['error']}")
        
        # 8. çŠ¶æ€å›å¡«ä¸æ¸…ç†
        logger.debug("æ›´æ–°çŠ¶æ€å¹¶æ¸…ç†...")
        return _create_success_response(base_report, report_result)
        
    except Exception as e:
        logger.error(f"Analysis Node æ‰§è¡Œå¼‚å¸¸: {str(e)}", exc_info=True)
        return _create_error_response(f"åˆ†æèŠ‚ç‚¹æ‰§è¡Œå¼‚å¸¸: {str(e)}")


async def _invoke_llm_langgraph(structured_llm, messages: List[Dict[str, Any]]):
    """ä»¥ LangGraph é£æ ¼ä¼˜å…ˆçš„æ–¹å¼å¼‚æ­¥è°ƒç”¨ LLMï¼ˆç»“æ„åŒ–è¾“å‡ºï¼‰ã€‚

    ä¼˜å…ˆå°è¯•ä¼ å…¥ {"messages": [...]}ï¼›
    è‹¥æ¨¡å‹ä¸æ¥å—è¯¥è¾“å…¥æ ¼å¼ï¼Œåˆ™å›é€€ä¸ºçº¯åˆ—è¡¨ [...]
    """
    try:
        return await structured_llm.ainvoke({"messages": messages})
    except Exception:
        # å›é€€ä¸ºç›´æ¥ä¼ åˆ—è¡¨
        return await structured_llm.ainvoke(messages)


def _validate_input_results(state: AgentState) -> Dict[str, Any]:
    """å‰ç½®æ ¡éªŒï¼šæ£€æŸ¥æ‰€æœ‰æ­¥éª¤çš„ç»“æœçŠ¶æ€"""
    fastp_results = state.fastp_results or {}
    star_results = state.star_results or {}
    featurecounts_results = state.featurecounts_results or {}
    
    # æ£€æŸ¥è‡³å°‘æœ‰ä¸€ä¸ªæ­¥éª¤æˆåŠŸ
    any_success = (
        fastp_results.get("success") or
        star_results.get("success") or  
        featurecounts_results.get("success")
    )
    
    if not any_success:
        return {
            "success": False,
            "error": "æ‰€æœ‰æµæ°´çº¿æ­¥éª¤å‡æœªæˆåŠŸæ‰§è¡Œï¼Œæ— æ³•è¿›è¡Œç»¼åˆåˆ†æ"
        }
    
    # ç¡®å®šç»“æœç›®å½•ä¼˜å…ˆçº§ï¼šfeaturecounts > star > fastp
    results_dir = None
    for results in [featurecounts_results, star_results, fastp_results]:
        if results.get("success") and results.get("results_dir"):
            results_dir = results["results_dir"]
            break
    
    if not results_dir:
        return {
            "success": False,
            "error": "æ— æ³•ç¡®å®šåˆ†æç»“æœç›®å½•è·¯å¾„"
        }
    
    if not Path(results_dir).exists():
        return {
            "success": False,
            "error": f"ç»“æœç›®å½•ä¸å­˜åœ¨: {results_dir}"
        }
    
    return {
        "success": True,
        "results_dir": results_dir
    }


def _parse_pipeline_metrics(results_dir: str) -> Dict[str, Any]:
    """è§£æä¸‰ä¸ªæµæ°´çº¿æ­¥éª¤çš„æŒ‡æ ‡ï¼Œå¢å¼ºé”™è¯¯å¯è§‚æµ‹æ€§"""
    parsing_status = {"fastp": False, "star": False, "featurecounts": False}
    parsing_errors = {}
    
    try:
        # è°ƒç”¨è§£æå™¨å·¥å…·ï¼Œåˆ†åˆ«æ•è·æ¯ä¸ªè§£æå™¨çš„æˆåŠŸ/å¤±è´¥çŠ¶æ€
        logger.debug("è§£æFastPç»“æœ...")
        try:
            fastp_result = parse_fastp_results.invoke({"results_directory": results_dir})
            parsing_status["fastp"] = fastp_result.get("success", False)
            if not parsing_status["fastp"]:
                parsing_errors["fastp"] = fastp_result.get("error", "æœªçŸ¥FastPè§£æé”™è¯¯")
        except Exception as e:
            fastp_result = {"success": False, "error": f"FastPè§£æå¼‚å¸¸: {str(e)}"}
            parsing_errors["fastp"] = str(e)
            
        logger.debug("è§£æSTARç»“æœ...")
        try:
            star_result = parse_star_metrics.invoke({"results_directory": results_dir})
            parsing_status["star"] = star_result.get("success", False)
            if not parsing_status["star"]:
                parsing_errors["star"] = star_result.get("error", "æœªçŸ¥STARè§£æé”™è¯¯")
        except Exception as e:
            star_result = {"success": False, "error": f"STARè§£æå¼‚å¸¸: {str(e)}"}
            parsing_errors["star"] = str(e)
            
        logger.debug("è§£æFeatureCountsç»“æœ...")
        try:
            fc_result = parse_featurecounts_metrics.invoke({"results_directory": results_dir})
            parsing_status["featurecounts"] = fc_result.get("success", False)
            if not parsing_status["featurecounts"]:
                parsing_errors["featurecounts"] = fc_result.get("error", "æœªçŸ¥FeatureCountsè§£æé”™è¯¯")
        except Exception as e:
            fc_result = {"success": False, "error": f"FeatureCountsè§£æå¼‚å¸¸: {str(e)}"}
            parsing_errors["featurecounts"] = str(e)
        
        # ç»Ÿè®¡è§£ææˆåŠŸæƒ…å†µ
        success_count = sum(parsing_status.values())
        total_count = len(parsing_status)
        
        logger.info(f"è§£æå®Œæˆ: {success_count}/{total_count} ä¸ªæ­¥éª¤æˆåŠŸ")
        if parsing_errors:
            logger.warning(f"è§£æé”™è¯¯: {list(parsing_errors.keys())}")
        
        return {
            "success": True,
            "fastp": fastp_result,
            "star": star_result, 
            "featurecounts": fc_result,
            "parsing_status": parsing_status,
            "parsing_errors": parsing_errors
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"è°ƒç”¨è§£æå™¨å·¥å…·å¤±è´¥: {str(e)}",
            "parsing_status": parsing_status,
            "parsing_errors": parsing_errors
        }


def _align_sample_metrics(sample_groups: List[Dict], fastp_data: Dict, star_data: Dict, fc_data: Dict) -> Dict[str, Any]:
    """æ ·æœ¬æŒ‡æ ‡å¯¹é½ï¼ˆç®€åŒ–ç‰ˆï¼‰

    ä¾èµ–å„æ­¥éª¤è¿”å›çš„ sample_id å·²ä¸€è‡´ï¼ˆFeatureCounts ç”±å‚æ•°æ–‡ä»¶æä¾›æƒå¨æ ·æœ¬é¡ºåºï¼‰ï¼Œ
    ç›´æ¥æŒ‰ sample_groups ä¸­çš„ sample_id ç²¾ç¡®åˆå¹¶ï¼Œä¸å†åšé¢å¤–è§„èŒƒåŒ–å›é€€ã€‚
    """

    # æå–é¢„æœŸçš„æ ·æœ¬IDåˆ—è¡¨ï¼ˆä¿æŒé¡ºåºï¼‰
    expected_sample_ids: List[str] = []
    for group in sample_groups:
        sid = group.get("sample_id") or group.get("id")
        if sid:
            expected_sample_ids.append(sid)

    # å„æ­¥éª¤æ ·æœ¬æŒ‡æ ‡æ˜ å°„
    fastp_samples = {s.get("sample_id"): s for s in fastp_data.get("sample_metrics", []) if s.get("sample_id")}
    star_samples = {s.get("sample_id"): s for s in star_data.get("sample_metrics", []) if s.get("sample_id")}
    fc_samples = {s.get("sample_id"): s for s in fc_data.get("sample_metrics", []) if s.get("sample_id")}

    # åˆå¹¶
    aligned_samples: List[Dict[str, Any]] = []
    for sid in expected_sample_ids:
        sample_data = {
            "sample_id": sid,
            "fastp": fastp_samples.get(sid, {"error": "æœªæ‰¾åˆ°FastPæ•°æ®"}),
            "star": star_samples.get(sid, {"error": "æœªæ‰¾åˆ°STARæ•°æ®"}),
            "featurecounts": fc_samples.get(sid, {"error": "æœªæ‰¾åˆ°FeatureCountsæ•°æ®"}),
            "notes": []
        }

        if "error" in sample_data["fastp"]:
            sample_data["notes"].append("FastPæ•°æ®ç¼ºå¤±")
        if "error" in sample_data["star"]:
            sample_data["notes"].append("STARæ•°æ®ç¼ºå¤±")
        if "error" in sample_data["featurecounts"]:
            sample_data["notes"].append("FeatureCountsæ•°æ®ç¼ºå¤±")

        aligned_samples.append(sample_data)

    return {
        "samples": aligned_samples,
        "expected_count": len(expected_sample_ids),
        "fastp_available": len(fastp_samples),
        "star_available": len(star_samples),
        "featurecounts_available": len(fc_samples)
    }


def _assess_sample_health(alignment_data: Dict[str, Any]) -> Dict[str, Any]:
    """è®¡ç®—æ ·æœ¬å¥åº·åº¦å’Œæ€»ä½“ç»“è®º"""
    
    # é˜ˆå€¼å®šä¹‰ï¼ˆåŸºäºè®¾è®¡æ–‡æ¡£ç¬¬3èŠ‚ï¼‰
    thresholds = {
        "fastp": {
            "q30_good": 0.85, "q30_warn": 0.7,
            "pass_rate_good": 0.8, "pass_rate_warn": 0.6
        },
        "star": {
            "mapping_good": 0.85, "mapping_warn": 0.7,
            "unique_good": 0.8, "unique_warn": 0.6,
            "mismatch_good": 0.05, "mismatch_warn": 0.08
        },
        "featurecounts": {
            "assignment_good": 0.6, "assignment_warn": 0.4
        }
    }
    
    def _evaluate_metric(value, good_thresh, warn_thresh, reverse=False):
        """è¯„ä¼°å•ä¸ªæŒ‡æ ‡ï¼ˆreverse=Trueè¡¨ç¤ºè¶Šå°è¶Šå¥½ï¼‰"""
        if value is None:
            return "UNKNOWN"
        try:
            val = float(value)
            if reverse:
                return "PASS" if val <= good_thresh else ("WARN" if val <= warn_thresh else "FAIL")
            else:
                return "PASS" if val >= good_thresh else ("WARN" if val >= warn_thresh else "FAIL")
        except (ValueError, TypeError):
            return "UNKNOWN"
    
    assessed_samples = []
    health_counts = {"PASS": 0, "WARN": 0, "FAIL": 0, "UNKNOWN": 0}
    
    for sample in alignment_data["samples"]:
        sample_id = sample["sample_id"]
        fastp_metrics = sample.get("fastp", {})
        star_metrics = sample.get("star", {})  
        fc_metrics = sample.get("featurecounts", {})
        
        # è¯„ä¼°å„æ­¥éª¤
        evaluations = []
        
        # FastPè¯„ä¼°
        if "error" not in fastp_metrics:
            q30_status = _evaluate_metric(
                fastp_metrics.get("q30_after"), 
                thresholds["fastp"]["q30_good"], 
                thresholds["fastp"]["q30_warn"]
            )
            pass_rate_status = _evaluate_metric(
                fastp_metrics.get("read_pass_rate"),
                thresholds["fastp"]["pass_rate_good"],
                thresholds["fastp"]["pass_rate_warn"] 
            )
            fastp_status = "FAIL" if "FAIL" in [q30_status, pass_rate_status] else (
                "WARN" if "WARN" in [q30_status, pass_rate_status] else "PASS"
            )
            evaluations.append(fastp_status)
        
        # STARè¯„ä¼°
        if "error" not in star_metrics:
            mapping_status = _evaluate_metric(
                star_metrics.get("mapping_rate"),
                thresholds["star"]["mapping_good"],
                thresholds["star"]["mapping_warn"]
            )
            unique_status = _evaluate_metric(
                star_metrics.get("unique_mapping_rate"),
                thresholds["star"]["unique_good"], 
                thresholds["star"]["unique_warn"]
            )
            mismatch_status = _evaluate_metric(
                star_metrics.get("mismatch_rate"),
                thresholds["star"]["mismatch_good"],
                thresholds["star"]["mismatch_warn"],
                reverse=True
            )
            star_status = "FAIL" if "FAIL" in [mapping_status, unique_status, mismatch_status] else (
                "WARN" if "WARN" in [mapping_status, unique_status, mismatch_status] else "PASS"
            )
            evaluations.append(star_status)
        
        # FeatureCountsè¯„ä¼°
        if "error" not in fc_metrics:
            assignment_status = _evaluate_metric(
                fc_metrics.get("assignment_rate"),
                thresholds["featurecounts"]["assignment_good"],
                thresholds["featurecounts"]["assignment_warn"]
            )
            evaluations.append(assignment_status)
        
        # ç»¼åˆå¥åº·åº¦ï¼šå–æœ€å·®çº§åˆ«
        if not evaluations:
            overall_health = "UNKNOWN"
        else:
            if "FAIL" in evaluations:
                overall_health = "FAIL" 
            elif "WARN" in evaluations:
                overall_health = "WARN"
            else:
                overall_health = "PASS"
        
        health_counts[overall_health] += 1
        
        assessed_sample = {
            "sample_id": sample_id,
            "health": overall_health,
            "fastp": fastp_metrics,
            "star": star_metrics,
            "featurecounts": fc_metrics,
            "notes": sample.get("notes", [])
        }
        assessed_samples.append(assessed_sample)
    
    # æ€»ä½“ç»“è®º
    total_samples = len(assessed_samples)
    if health_counts["FAIL"] > 0:
        overall_status = "FAIL" if health_counts["FAIL"] / total_samples > 0.5 else "WARN"
    elif health_counts["WARN"] > 0:
        overall_status = "WARN"
    else:
        overall_status = "PASS"
    
    return {
        "samples": assessed_samples,
        "summary": {
            "status": overall_status,
            "samples": {
                "total": total_samples,
                "pass": health_counts["PASS"],
                "warn": health_counts["WARN"], 
                "fail": health_counts["FAIL"],
                "unknown": health_counts["UNKNOWN"]
            }
        }
    }


def _build_base_report(state: AgentState, results_dir: str, timestamp: str, 
                      fastp_data: Dict, star_data: Dict, fc_data: Dict,
                      alignment_result: Dict, assessment_result: Dict) -> Dict[str, Any]:
    """æ„å»ºåŸºç¡€æŠ¥å‘Šç»“æ„"""
    
    nextflow_config = state.nextflow_config or {}
    
    return {
        "pipeline": {
            "steps": ["fastp", "star", "featurecounts"],
            "species": nextflow_config.get("species", "unknown"),
            "genome_version": nextflow_config.get("genome_version", "unknown")
        },
        "context": {
            "results_dir": results_dir,
            "timestamp": timestamp,
            "sample_count": len(alignment_result.get("samples", []))
        },
        "metrics": {
            "fastp": {
                "overall": fastp_data.get("overall_statistics", {}),
                "samples": fastp_data.get("sample_metrics", [])
            },
            "star": {
                "overall": star_data.get("overall_statistics", {}),
                "samples": star_data.get("sample_metrics", [])
            },
            "featurecounts": {
                "overall": fc_data.get("overall_statistics", {}),
                "samples": fc_data.get("sample_metrics", [])
            }
        },
        "per_sample": assessment_result["samples"],
        "summary": assessment_result["summary"],
        "files": {
            "matrix_path": fc_data.get("matrix_path", "")
        },
        "recommendations": _generate_basic_recommendations(assessment_result)
    }


def _generate_basic_recommendations(assessment_result: Dict) -> List[Dict[str, str]]:
    """ç”ŸæˆåŸºç¡€å»ºè®®ï¼ˆè§„åˆ™åŸºç¡€ï¼ŒLLMå¯è¡¥å……ï¼‰"""
    recommendations = []
    
    summary = assessment_result.get("summary", {})
    samples_info = summary.get("samples", {})
    
    # åŸºäºå¥åº·åº¦ç»Ÿè®¡çš„å»ºè®®
    if samples_info.get("fail", 0) > 0:
        recommendations.append({
            "type": "action",
            "title": "æ£€æŸ¥å¤±è´¥æ ·æœ¬",
            "detail": f"æœ‰{samples_info['fail']}ä¸ªæ ·æœ¬è´¨é‡è¯„ä¼°ä¸ºFAILï¼Œå»ºè®®æ£€æŸ¥åŸå§‹æ•°æ®è´¨é‡ã€æµæ°´çº¿å‚æ•°è®¾ç½®"
        })
    
    if samples_info.get("warn", 0) > 0:
        recommendations.append({
            "type": "action", 
            "title": "å…³æ³¨è­¦å‘Šæ ·æœ¬",
            "detail": f"æœ‰{samples_info['warn']}ä¸ªæ ·æœ¬å­˜åœ¨è´¨é‡è­¦å‘Šï¼Œå»ºè®®ä¼˜åŒ–å‚æ•°æˆ–æ’é™¤å¼‚å¸¸æ ·æœ¬"
        })
    
    # é€šç”¨åç»­åˆ†æå»ºè®®
    if samples_info.get("pass", 0) > 0:
        recommendations.append({
            "type": "next",
            "title": "å·®å¼‚è¡¨è¾¾åˆ†æ", 
            "detail": "å¯ä½¿ç”¨DESeq2æˆ–edgeRç­‰å·¥å…·è¿›è¡Œå·®å¼‚è¡¨è¾¾åˆ†æ"
        })
        
        recommendations.append({
            "type": "next",
            "title": "åŠŸèƒ½å¯Œé›†åˆ†æ",
            "detail": "å»ºè®®è¿›è¡ŒGO enrichmentå’ŒKEGG pathwayåˆ†æ"
        })
    
    return recommendations


async def _execute_llm_analysis(base_report: Dict[str, Any]) -> Dict[str, Any]:
    """æ‰§è¡ŒLLMæ™ºèƒ½åˆ†æï¼ŒåŒ…å«é€€é¿å’Œé™çº§ç­–ç•¥"""
    
    # é¢„å®šä¹‰å˜é‡é¿å…UnboundLocalError
    llm = None
    structured_llm = None 
    system_prompt = ""
    user_message = ""
    
    try:
        llm = get_shared_llm()
        
        # æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = ANALYSIS_LLM_SYSTEM_PROMPT

        # æ„å»ºç”¨æˆ·æ¶ˆæ¯ï¼ˆå¼‚å¸¸æ ·æœ¬ä¼˜å…ˆé‡‡æ ·ï¼‰
        pipeline_info = base_report.get("pipeline", {})
        context = base_report.get("context", {})
        summary = base_report.get("summary", {})
        per_sample = base_report.get("per_sample", [])
        
        # å¼‚å¸¸æ ·æœ¬ä¼˜å…ˆé‡‡æ ·ç­–ç•¥
        sorted_samples = sorted(per_sample, key=lambda x: {
            "FAIL": 0, "WARN": 1, "PASS": 2, "UNKNOWN": 3
        }.get(x.get("health", "UNKNOWN"), 3))
        sampled_per_sample = sorted_samples[:10]  # é™åˆ¶æ ·æœ¬æ•°é‡é¿å…è¿‡é•¿
        
        user_message = f"""
åˆ†æåŸºæœ¬ä¿¡æ¯ï¼š
- æµç¨‹æ­¥éª¤ï¼š{' â†’ '.join(pipeline_info.get('steps', []))}
- ç‰©ç§ï¼š{pipeline_info.get('species')}ï¼ŒåŸºå› ç»„ç‰ˆæœ¬ï¼š{pipeline_info.get('genome_version')}
- æ ·æœ¬æ•°é‡ï¼š{context.get('sample_count')}
- åˆ†æç›®å½•ï¼š{context.get('results_dir')}

æ€»ä½“ç»“è®ºï¼š
- çŠ¶æ€ï¼š{summary.get('status')}
- æ ·æœ¬ç»Ÿè®¡ï¼š{summary.get('samples')}

å‰{len(sampled_per_sample)}ä¸ªæ ·æœ¬è¯¦æƒ…ï¼ˆå¼‚å¸¸æ ·æœ¬ä¼˜å…ˆï¼‰ï¼š
{json.dumps(sampled_per_sample, ensure_ascii=False, indent=2)}

è¯·æä¾›ï¼š
1. é¢å‘ç”¨æˆ·çš„è¯¦ç»†çš„æ€»ä½“è¯„ä¼°
2. å…³é”®å‘ç°åˆ—è¡¨ï¼ˆåŒ…å«å…·ä½“æ•°æ®ï¼‰
3. æ¯ä¸ªæœ‰é—®é¢˜æ ·æœ¬çš„å…·ä½“é—®é¢˜å’Œä¸¥é‡æ€§
4. åˆ†ç±»å»ºè®®ï¼ˆæ“ä½œå»ºè®®ã€åç»­åˆ†ææ–¹å‘ï¼‰ 
5. æ½œåœ¨é£é™©æç¤º
"""

        # è°ƒç”¨LLMï¼ˆå¸¦è¶…æ—¶ï¼‰
        structured_llm = llm.with_structured_output(LLMAnalysisModel)
        
        logger.info("è°ƒç”¨LLMè¿›è¡Œæ™ºèƒ½åˆ†æ...")
        msgs = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
        llm_response = await _invoke_llm_langgraph(structured_llm, msgs)
        try:
            log_llm_preview(logger, "analysis", llm_response)
        except Exception:
            pass
        
        return {
            "success": True,
            "analysis": dict(llm_response)
        }
        
    except Exception as e:
        # å®ç°é€€é¿ç­–ç•¥
        error_msg = str(e)
        
        # æ›´ç²¾ç¡®çš„é”™è¯¯ç è¯†åˆ«
        if "429" in error_msg:  # é€Ÿç‡é™åˆ¶
            logger.warning("é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…20ç§’åé‡è¯•...")
            await asyncio.sleep(20)
            try:
                # é‡æ–°æ„é€ LLMå’Œæ¶ˆæ¯ï¼Œé¿å…å¼•ç”¨æœªå®šä¹‰å˜é‡
                if llm is None or structured_llm is None:
                    llm = get_shared_llm()
                    structured_llm = llm.with_structured_output(LLMAnalysisModel)
                if not system_prompt:
                    system_prompt = ANALYSIS_LLM_SYSTEM_PROMPT
                if not user_message:
                    # é‡æ–°æ„é€ user_messageï¼ˆç®€åŒ–ç‰ˆé¿å…é‡å¤é€»è¾‘ï¼‰
                    user_message = f"ç®€åŒ–åˆ†æè¯·æ±‚ - çŠ¶æ€ï¼š{base_report.get('summary', {}).get('status', 'UNKNOWN')}"
                
                msgs = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ]
                llm_response = await _invoke_llm_langgraph(structured_llm, msgs)
                try:
                    log_llm_preview(logger, "analysis.retry429", llm_response)
                except Exception:
                    pass
                return {
                    "success": True,
                    "analysis": dict(llm_response)
                }
            except Exception as retry_e:
                return {
                    "success": False,
                    "error": f"LLMåˆ†æå¤±è´¥ï¼ˆé€Ÿç‡é™åˆ¶é‡è¯•åï¼‰ï¼š{str(retry_e)}"
                }
        
        # æ›´ç²¾ç¡®çš„5xxæœåŠ¡å™¨é”™è¯¯è¯†åˆ«
        elif any(code in error_msg for code in ["500", "502", "503", "504", "timeout"]):  # æœåŠ¡å™¨é”™è¯¯æˆ–è¶…æ—¶
            logger.warning("é‡åˆ°æœåŠ¡å™¨é—®é¢˜ï¼Œç­‰å¾…2ç§’åé‡è¯•...")
            await asyncio.sleep(2)
            try:
                # é‡æ–°æ„é€ LLMå’Œæ¶ˆæ¯
                if llm is None or structured_llm is None:
                    llm = get_shared_llm()
                    structured_llm = llm.with_structured_output(LLMAnalysisModel)
                if not system_prompt:
                    system_prompt = ANALYSIS_LLM_SYSTEM_PROMPT
                if not user_message:
                    user_message = f"ç®€åŒ–åˆ†æè¯·æ±‚ - çŠ¶æ€ï¼š{base_report.get('summary', {}).get('status', 'UNKNOWN')}"
                
                msgs = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ]
                llm_response = await _invoke_llm_langgraph(structured_llm, msgs)
                try:
                    log_llm_preview(logger, "analysis.retry5xx", llm_response)
                except Exception:
                    pass
                return {
                    "success": True,
                    "analysis": dict(llm_response)
                }
            except Exception as retry_e:
                return {
                    "success": False,
                    "error": f"LLMåˆ†æå¤±è´¥ï¼ˆæœåŠ¡å™¨é”™è¯¯é‡è¯•åï¼‰ï¼š{str(retry_e)}"
                }
        
        return {
            "success": False,
            "error": f"LLMåˆ†æå¤±è´¥ï¼š{error_msg}"
        }


def _save_reports(report_data: Dict[str, Any], results_dir: str, timestamp: str) -> Dict[str, Any]:
    """ä¿å­˜JSONæŠ¥å‘Šå’ŒMarkdownæ‘˜è¦"""
    
    try:
        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        report_dir = Path(results_dir) / "reports" / timestamp
        report_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜JSONæŠ¥å‘Š
        json_path = report_dir / "analysis_report.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        # æ›´æ–°æŠ¥å‘Šä¸­çš„æ–‡ä»¶è·¯å¾„
        report_data.setdefault("files", {})["report_json"] = str(json_path)
        
        # è°ƒç”¨Markdownç”Ÿæˆå·¥å…·
        md_result = write_analysis_markdown.invoke({
            "analysis_report": report_data,
            "output_dir": str(report_dir),
            "filename": "analysis_summary.md",
            "append_llm_section": True
        })
        
        if md_result.get("success"):
            report_data["files"]["report_md"] = md_result["path"]
        
        return {
            "success": True,
            "json_path": str(json_path),
            "md_path": md_result.get("path") if md_result.get("success") else None,
            "report_dir": str(report_dir)
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"æŠ¥å‘Šä¿å­˜å¤±è´¥ï¼š{str(e)}"
        }


def _create_success_response(report_data: Dict[str, Any], save_result: Dict[str, Any]) -> Dict[str, Any]:
    """åˆ›å»ºæˆåŠŸå“åº”"""
    
    # ç”Ÿæˆç”¨æˆ·å‹å¥½çš„æ‘˜è¦
    summary = report_data.get("summary", {})
    
    status = summary.get("status", "UNKNOWN")
    status_emoji = {"PASS": "âœ…", "WARN": "âš ï¸", "FAIL": "âŒ"}.get(status, "â“")
    
    samples_info = summary.get("samples", {})
    
    user_response = f"""
ğŸ‰ RNA-seqç»¼åˆåˆ†æå®Œæˆï¼

{status_emoji} æ€»ä½“ç»“è®º: {status}

ğŸ“Š æ ·æœ¬ç»Ÿè®¡:
- æ€»è®¡ï¼š{samples_info.get('total', 0)} ä¸ªæ ·æœ¬
- é€šè¿‡ï¼š{samples_info.get('pass', 0)} ä¸ª âœ…
- è­¦å‘Šï¼š{samples_info.get('warn', 0)} ä¸ª âš ï¸  
- å¤±è´¥ï¼š{samples_info.get('fail', 0)} ä¸ª âŒ

ğŸ“ åˆ†ææŠ¥å‘Š:
- JSONè¯¦ç»†æŠ¥å‘Š: {save_result.get('json_path', 'ç”Ÿæˆå¤±è´¥')}
- Markdownæ‘˜è¦: {save_result.get('md_path', 'ç”Ÿæˆå¤±è´¥')}

ğŸ’¡ åç»­å»ºè®®: æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Šäº†è§£å…·ä½“æŒ‡æ ‡å’Œå»ºè®®
"""

    # æå–å…³é”®ç»Ÿè®¡ä¿¡æ¯
    workflow_statistics = {
        "total_samples": samples_info.get("total", 0),
        "pass_samples": samples_info.get("pass", 0), 
        "warn_samples": samples_info.get("warn", 0),
        "fail_samples": samples_info.get("fail", 0),
        "overall_status": status
    }
    
    return {
        "success": True,
        "status": "analysis_completed",
        "response": user_response.strip(),
        "analysis_report": report_data,
        "analysis_report_path": save_result.get("json_path"),
        "workflow_statistics": workflow_statistics,
        
        # æ¸…ç©ºæ‰§è¡Œè¿›åº¦çŠ¶æ€
        "current_step": "",
        "completed_steps": [],
        "execution_mode": "single",
        
        # æ¸…ç©ºå„èŠ‚ç‚¹çš„ç»“æœçŠ¶æ€ï¼ˆä½†ä¿ç•™å¼•ç”¨ï¼‰
        "fastp_results": {},
        "star_results": {},
        "featurecounts_results": {},
        
        # æ¸…ç©ºä¼˜åŒ–ç›¸å…³çŠ¶æ€
        "fastp_optimization_suggestions": "",
        "star_optimization_suggestions": "",
        "featurecounts_optimization_suggestions": ""
    }


def _create_error_response(error_message: str) -> Dict[str, Any]:
    """åˆ›å»ºé”™è¯¯å“åº”"""
    return {
        "success": False,
        "status": "analysis_failed",
        "response": f"âŒ ç»¼åˆåˆ†æå¤±è´¥\n\né”™è¯¯è¯¦æƒ…: {error_message}\n\nå»ºè®®ï¼šè¯·æ£€æŸ¥ä¸Šæ¸¸æµæ°´çº¿æ‰§è¡ŒçŠ¶æ€ï¼Œç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæ­¥éª¤æˆåŠŸå®Œæˆã€‚",
        "analysis_results": {
            "success": False,
            "status": "failed", 
            "error": error_message
        }
    }
