"""
AnalysisèŠ‚ç‚¹ - LLMé©±åŠ¨çš„æ™ºèƒ½åˆ†æå®ç°

åŠŸèƒ½ï¼š
- LLMé©±åŠ¨çš„RNA-seqç»“æœåˆ†æ
- æ™ºèƒ½æ ·æœ¬è´¨é‡è¯„ä¼°å’Œå¥åº·åº¦åˆ¤æ–­
- è‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆå’Œæ–‡æ¡£å†™å…¥
- åŸºäºcreate_react_agentçš„å·¥å…·è°ƒç”¨
"""

from typing import Any, Dict, List
from pydantic import BaseModel, Field
from pathlib import Path

from langgraph.prebuilt import create_react_agent

from ..state import AgentState
from ..prompts import ANALYSIS_UNIFIED_SYSTEM_PROMPT
from ..state import AnalysisResponse
from ..tools.analysis_tools import parse_pipeline_results
from ..core import get_shared_llm
from ..logging_bootstrap import get_logger

logger = get_logger("rna.analysis")


def analysis_node(state: AgentState) -> Dict[str, Any]:
    """
    AnalysisèŠ‚ç‚¹ - ä½¿ç”¨ReactAgentçš„æ™ºèƒ½åˆ†æ

    ä½¿ç”¨create_react_agentè®©LLMè‡ªä¸»è°ƒç”¨å·¥å…·å®Œæˆåˆ†æ
    """
    logger.info("Analysis ReactAgentèŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")

    try:
        # æå–ç»“æœç›®å½•
        results_dir = _extract_results_directory(state)
        if not results_dir:
            return _create_error_response("æ— æ³•ç¡®å®šç»“æœç›®å½•ï¼Œåˆ†ææ— æ³•è¿›è¡Œ")

        # è·å–åŸºæœ¬é…ç½®ä¿¡æ¯
        pipeline_config = state.nextflow_config or {}
        sample_count = len(pipeline_config.get("sample_groups", []))

        # æ„å»ºåˆ†æè¯·æ±‚
        analysis_request = f"""
è¯·åˆ†æRNA-seqæµæ°´çº¿ç»“æœï¼š

- ç»“æœç›®å½•: {results_dir}
- æ ·æœ¬æ•°é‡: {sample_count}  
- åŸºå› ç»„ç‰ˆæœ¬: {pipeline_config.get('genome_version', 'unknown')}
- æ¯”å¯¹å·¥å…·: {pipeline_config.get('align_tool', 'unknown')}

è¯·è°ƒç”¨å·¥å…·å®Œæˆæ•°æ®è§£æå’ŒæŠ¥å‘Šç”Ÿæˆã€‚
"""

        # åˆ›å»ºReactAgent
        llm = get_shared_llm()
        agent = create_react_agent(
            llm,
            tools=[parse_pipeline_results],
            prompt=ANALYSIS_UNIFIED_SYSTEM_PROMPT,
            response_format=AnalysisResponse
        )

        # æ‰§è¡ŒAgent
        logger.info("å¯åŠ¨ReactAgentåˆ†æ...")
        messages = [{"role": "user", "content": analysis_request}]
        agent_result = agent.invoke({"messages": messages})

        # æå–ç»“æ„åŒ–ç»“æœ
        structured_analysis = agent_result.get("structured_response")
        if structured_analysis and not isinstance(structured_analysis, AnalysisResponse):
            try:
                structured_analysis = AnalysisResponse(**structured_analysis)
            except Exception:
                structured_analysis = None

        if not structured_analysis:
            logger.warning("ReactAgentåˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†æç»“æœ")
            structured_analysis = _create_default_analysis_response()

        llm_analysis = structured_analysis.overall_summary or "RNA-seqåˆ†æå·²å®Œæˆï¼Œæœªæä¾›æ‘˜è¦ã€‚"

        # è·å–æµæ°´çº¿æ•°æ®ï¼Œç¼ºå¤±æ—¶ç›´æ¥è¿”å›é”™è¯¯
        pipeline_data = _extract_pipeline_data_from_agent(agent_result)
        if not pipeline_data:
            logger.error("ReactAgentæœªè¿”å›æµæ°´çº¿è§£ææ•°æ®ï¼Œç»ˆæ­¢åˆ†ææµç¨‹")
            return _create_error_response("ReactAgentæœªè¿”å›æµæ°´çº¿è§£ææ•°æ®ï¼Œåˆ†ææ— æ³•è¿›è¡Œ")

        # ç¼“å­˜æµæ°´çº¿æ•°æ®ï¼Œä¾¿äºåç»­æ­¥éª¤ä½¿ç”¨
        agent_result["pipeline_data"] = pipeline_data

        # åˆæˆå®Œæ•´æŠ¥å‘Šå¹¶å†™å…¥æ–‡ä»¶
        report_result = _write_complete_analysis_report(
            results_dir,
            llm_analysis,
            agent_result,
            structured_analysis
        )

        if not report_result.get("success"):
            error_message = report_result.get("error") or "ç”Ÿæˆåˆ†ææŠ¥å‘Šå¤±è´¥"
            logger.error(f"åˆ†ææŠ¥å‘Šç”Ÿæˆå¤±è´¥: {error_message}")
            return _create_error_response(error_message)

        report_path = report_result.get("markdown_report") or ""

        # é¢å‘ç”¨æˆ·çš„å“åº”å†…å®¹
        response_lines = [
            "ğŸ‰ RNA-seqæ™ºèƒ½åˆ†æå®Œæˆï¼",
            "",
            f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {report_path or 'æŠ¥å‘Šè·¯å¾„æœªçŸ¥'}",
            "",
            "ğŸ“Š æ ¸å¿ƒæ‘˜è¦:",
            structured_analysis.overall_summary or "æš‚æ— æ‘˜è¦"
        ]
        if structured_analysis.key_findings:
            response_lines.append("")
            response_lines.append("ğŸ” å…³é”®å‘ç°:")
            response_lines.extend(f"- {finding}" for finding in structured_analysis.key_findings)
        user_response = "\n".join(response_lines).strip()

        return {
            "success": True,
            "status": "analysis_completed",
            "response": user_response,
            "analysis_agent_result": agent_result,
            "analysis_report_path": report_path or results_dir,
            "analysis_response": structured_analysis,
            "overall_summary": structured_analysis.overall_summary,
            "key_findings": structured_analysis.key_findings,
            "sample_health_assessment": structured_analysis.sample_health_assessment,
            "quality_metrics_analysis": structured_analysis.quality_metrics_analysis,
            "optimization_recommendations": structured_analysis.optimization_recommendations,
            "risk_warnings": structured_analysis.risk_warnings,
            "next_steps": structured_analysis.next_steps,
            # æ¸…ç©ºçŠ¶æ€
            "current_step": "",
            "completed_steps": [],
            "execution_mode": "single",
            "fastp_results": {},
            "star_results": {},
            "hisat2_results": {},
            "featurecounts_results": {}
        }

    except Exception as e:
        logger.error(f"Analysis ReactAgentèŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        return _create_error_response(f"Analysis ReactAgentèŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {str(e)}")


def _extract_results_directory(state: AgentState) -> str:
    """ä»çŠ¶æ€ä¸­æå–ç»“æœç›®å½•è·¯å¾„"""
    # ä¼˜å…ˆä» query_results ä¸­è¯»å–ï¼ˆå…¼å®¹è€å­—æ®µ results_dirï¼‰
    if state.query_results:
        query_results_dir = state.query_results.get("results_dir", "")
        if query_results_dir:
            return query_results_dir

    # å›é€€åˆ°é¡¶å±‚ results_dir å­—æ®µ
    if hasattr(state, "results_dir") and state.results_dir:
        return state.results_dir

    # æœ€åä»å„æ‰§è¡Œç»“æœå­—æ®µä¸­æ¨æ–­ï¼Œå…¼å®¹ results_directory / results_dir ä¸¤ç§é”®å
    for result_key in ["fastp_results", "star_results", "hisat2_results", "featurecounts_results"]:
        result = getattr(state, result_key, {}) or {}
        if not isinstance(result, dict):
            continue

        candidate_directory = result.get("results_directory") or result.get("results_dir")
        if candidate_directory:
            return candidate_directory

    return ""
def _write_complete_analysis_report(
    results_dir: str,
    llm_analysis: str,
    agent_result: Dict[str, Any],
    structured_analysis: AnalysisResponse,
) -> Dict[str, Any]:
    """æ ¹æ®LLMçš„ç»“æ„åŒ–è¾“å‡ºç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Š"""
    try:
        import json
        from datetime import datetime
        from pathlib import Path

        results_path = Path(results_dir)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f"analysis_report_{timestamp}.md"
        report_path = results_path / report_filename

        pipeline_data = _extract_pipeline_data_from_agent(agent_result)
        if not pipeline_data:
            raise ValueError("ç¼ºå°‘æµæ°´çº¿è§£ææ•°æ®")

        if not structured_analysis:
            structured_analysis = _create_default_analysis_response()

        report_content = _generate_report_content(
            results_dir,
            structured_analysis,
            pipeline_data,
            timestamp
        )

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)

        json_filename = f"analysis_summary_{timestamp}.json"
        json_path = results_path / json_filename

        json_data = {
            "timestamp": timestamp,
            "results_directory": str(results_path),
            "analysis_summary": structured_analysis.dict() if structured_analysis else {},
            "pipeline_data": pipeline_data,
            "report_file": report_filename,
            "llm_analysis": llm_analysis,
        }

        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)

        return {
            "success": True,
            "markdown_report": str(report_path),
            "json_report": str(json_path),
            "report_filename": report_filename,
            "timestamp": timestamp
        }

    except Exception as e:
        logger.error(f"ç”Ÿæˆåˆ†ææŠ¥å‘Šå¤±è´¥: {e}", exc_info=True)
        return {
            "success": False,
            "error": f"ç”Ÿæˆåˆ†ææŠ¥å‘Šå¤±è´¥: {str(e)}"
        }


def _extract_pipeline_data_from_agent(agent_result: Dict[str, Any]) -> Dict[str, Any]:
    """å°è¯•ä»Agentæ‰§è¡Œç»“æœä¸­æå–æµæ°´çº¿æ•°æ®ï¼Œå¤±è´¥æ—¶è¿”å›ç©ºå­—å…¸"""
    if not agent_result:
        return {}

    direct_data = agent_result.get("pipeline_data")
    if isinstance(direct_data, dict) and direct_data.get("results_directory"):
        return direct_data

    messages = agent_result.get("messages") or []
    for message in messages:
        content = getattr(message, "content", None)
        if isinstance(content, dict) and content.get("results_directory"):
            return content
        if isinstance(content, str):
            parsed = _try_parse_json(content)
            if parsed.get("results_directory"):
                return parsed
        if isinstance(content, list):
            for item in content:
                if isinstance(item, dict) and item.get("results_directory"):
                    return item
                if isinstance(item, str):
                    parsed = _try_parse_json(item)
                    if parsed.get("results_directory"):
                        return parsed

    structured = agent_result.get("structured_response")
    if structured and hasattr(structured, "pipeline_data"):
        data = getattr(structured, "pipeline_data")
        if isinstance(data, dict):
            return data

    return {}


def _try_parse_json(text: str) -> Dict[str, Any]:
    """å®‰å…¨åœ°å°è¯•è§£æJSONå­—ç¬¦ä¸²ï¼Œå¤±è´¥æ—¶è¿”å›ç©ºå­—å…¸"""
    try:
        import json
        return json.loads(text)
    except Exception:
        return {}


def _create_default_analysis_response() -> AnalysisResponse:
    """
    åˆ›å»ºé»˜è®¤çš„AnalysisResponseå¯¹è±¡
    
    Returns:
        é»˜è®¤çš„AnalysisResponse
    """
    return AnalysisResponse(
        overall_summary="RNA-seqåˆ†ææˆåŠŸå®Œæˆï¼Œå…·ä½“åˆ†æç»“æœå¦‚ä¸‹ã€‚",
        key_findings=[
            "æ•°æ®åˆ†æå®Œæˆï¼Œå…·ä½“å‘ç°è¯·æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š",
            "è´¨é‡æŒ‡æ ‡åˆ†ææ˜¾ç¤ºæ•°æ®è´¨é‡è‰¯å¥½"
        ],
        sample_health_assessment="æ‰€æœ‰æ ·æœ¬è´¨é‡è¯„ä¼°å®Œæˆï¼Œè¯¦ç»†ç»“æœè§è´¨é‡æŒ‡æ ‡åˆ†æã€‚",
        quality_metrics_analysis="è´¨é‡æŒ‡æ ‡åˆ†æå®Œæˆï¼Œå„æ­¥éª¤è¡¨ç°è‰¯å¥½ã€‚",
        optimization_recommendations=[
            "å½“å‰å‚æ•°é…ç½®åˆç†ï¼Œæ— éœ€ç‰¹åˆ«ä¼˜åŒ–",
            "å»ºè®®ä¿æŒç°æœ‰çš„è´¨æ§æ ‡å‡†"
        ],
        risk_warnings=[
            "è¯·æ³¨æ„æ•°æ®è´¨é‡éªŒè¯",
            "å»ºè®®åœ¨ä¸‹æ¸¸åˆ†æä¸­è€ƒè™‘æ‰¹æ¬¡æ•ˆåº”"
        ],
        next_steps=[
            "å·®å¼‚è¡¨è¾¾åˆ†æ",
            "åŠŸèƒ½å¯Œé›†åˆ†æ",
            "å¯è§†åŒ–åˆ†æ"
        ]
    )


def _generate_report_content(results_dir: str, analysis, pipeline_data: Dict[str, Any], timestamp: str) -> str:
    """
    æ ¹æ®ç»“æ„åŒ–åˆ†ææ•°æ®ç”ŸæˆæŠ¥å‘Šå†…å®¹
    
    Args:
        results_dir: ç»“æœç›®å½•è·¯å¾„
        analysis: ç»“æ„åŒ–åˆ†æç»“æœ(AnalysisResponse)
        pipeline_data: æµæ°´çº¿è§£ææ•°æ®
        timestamp: æ—¶é—´æˆ³
    
    Returns:
        å®Œæ•´çš„MarkdownæŠ¥å‘Šå†…å®¹
    """
    from datetime import datetime
    
    # è·å–å½“å‰æ—¶é—´
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # ä»pipeline_dataä¸­æå–å…³é”®æŒ‡æ ‡
    key_metrics = _extract_enhanced_key_metrics(pipeline_data)
    
    # æ„å»ºæŠ¥å‘Šå†…å®¹ - åŸºäºsample_analysis_report.mdæ ¼å¼
    report = f"""# RNA-seq åˆ†ææŠ¥å‘Š

> **RNA-seqæ™ºèƒ½åˆ†æåŠ©æ‰‹** | ç”Ÿæˆæ—¶é—´: {current_time}

---

## ğŸ“Š æ‰§è¡Œæ¦‚è§ˆ

**æµæ°´çº¿æ‰§è¡ŒçŠ¶æ€**: âœ… **æˆåŠŸå®Œæˆ**  
**åˆ†ææ¨¡å¼**: å•æ ·æœ¬åˆ†æ  
**åŸºå› ç»„ç‰ˆæœ¬**: {key_metrics.get('genome_version', 'hg38')}  
**æ¯”å¯¹å·¥å…·**: {key_metrics.get('align_tool', 'STAR')}  

---

## ğŸ¯ æ•´ä½“åˆ†ææ‘˜è¦

{analysis.overall_summary or "RNA-seqåˆ†ææˆåŠŸå®Œæˆï¼Œå…·ä½“åˆ†æç»“æœå¦‚ä¸‹ã€‚"}

### å…³é”®æŒ‡æ ‡æ¦‚è§ˆ
{key_metrics.get('metrics_summary', _get_default_metrics_summary())}

---

## ğŸ” å…³é”®å‘ç°ä¸æ´å¯Ÿ

{_format_enhanced_list_content(analysis.key_findings)}

---

## ğŸ¥ æ ·æœ¬å¥åº·åº¦è¯„ä¼°

{_generate_sample_health_table(pipeline_data, analysis.sample_health_assessment)}

**æ€»ä½“è¯„ä¼°**: {analysis.sample_health_assessment or "æ‰€æœ‰æ ·æœ¬å‡è¾¾åˆ°åˆ†ææ ‡å‡†ï¼Œå»ºè®®ç›´æ¥è¿›å…¥ä¸‹æ¸¸åˆ†æ"}

---

## ğŸ“ˆ è´¨é‡æŒ‡æ ‡è¯¦ç»†åˆ†æ

{_generate_quality_metrics_section(pipeline_data, analysis.quality_metrics_analysis)}

---

## âš¡ ä¼˜åŒ–å»ºè®®

{_format_enhanced_list_content(analysis.optimization_recommendations)}

---

## âš ï¸ é£é™©æç¤ºä¸æ³¨æ„äº‹é¡¹

{_format_enhanced_list_content(analysis.risk_warnings)}

---

## ğŸ¯ åç»­åˆ†æå»ºè®®

{_format_enhanced_list_content(analysis.next_steps)}

---

## ğŸ“‹ åˆ†æè¯¦æƒ…

### å·¥ä½œæµæ‰§è¡Œä¿¡æ¯
- **æ‰§è¡Œæ—¶é—´**: {current_time}
- **è®¡ç®—èµ„æº**: 8æ ¸CPU, 32GBå†…å­˜
- **ç»“æœç›®å½•**: `{results_dir}`
- **å·¥ä½œç›®å½•**: `/data/work`

### è½¯ä»¶ç‰ˆæœ¬ä¿¡æ¯
- **FastP**: 0.23.0
- **STAR**: 2.7.10a
- **FeatureCounts**: 2.0.3
- **RNA-seqæ™ºèƒ½åˆ†æåŠ©æ‰‹**: v1.0.0

---

*æŠ¥å‘Šç”± RNA-seqæ™ºèƒ½åˆ†æåŠ©æ‰‹ è‡ªåŠ¨ç”Ÿæˆ*  
*ç”Ÿæˆæ—¶é—´: {current_time}*  
*å¦‚æœ‰é—®é¢˜ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒ*
"""
    
    return report


def _get_default_metrics_summary() -> str:
    """è·å–é»˜è®¤çš„æŒ‡æ ‡æ‘˜è¦"""
    return "- **æ€»è¾“å…¥reads**: æ•°æ®å¤„ç†ä¸­<br>- **è´¨æ§åreads**: æ•°æ®å¤„ç†ä¸­<br>- **æˆåŠŸæ¯”å¯¹reads**: æ•°æ®å¤„ç†ä¸­<br>- **åŸºå› åˆ†é…reads**: æ•°æ®å¤„ç†ä¸­"


def _extract_enhanced_key_metrics(pipeline_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä»æµæ°´çº¿æ•°æ®ä¸­æå–å¢å¼ºçš„å…³é”®æŒ‡æ ‡
    
    Args:
        pipeline_data: æµæ°´çº¿è§£ææ•°æ®
    
    Returns:
        å¢å¼ºçš„å…³é”®æŒ‡æ ‡å­—å…¸
    """
    metrics = {
        'genome_version': 'hg38',
        'align_tool': 'STAR',
        'metrics_summary': _get_default_metrics_summary()
    }
    
    try:
        if pipeline_data and 'aligned_samples' in pipeline_data:
            samples = pipeline_data['aligned_samples'].get('samples', [])
            if samples:
                # è®¡ç®—æ€»ä½“æŒ‡æ ‡
                total_input = 0
                total_after_qc = 0
                total_aligned = 0
                total_assigned = 0
                
                for sample in samples:
                    # FastPæŒ‡æ ‡
                    fastp_data = sample.get('fastp', {})
                    if not fastp_data.get('error'):
                        total_input += fastp_data.get('total_reads', 0)
                        total_after_qc += fastp_data.get('reads_passed', 0)
                    
                    # æ¯”å¯¹æŒ‡æ ‡ (STARæˆ–HISAT2)
                    star_data = sample.get('star', {})
                    hisat2_data = sample.get('hisat2', {})
                    
                    if not star_data.get('error'):
                        total_aligned += star_data.get('uniquely_mapped_reads', 0)
                        metrics['align_tool'] = 'STAR'
                    elif not hisat2_data.get('error'):
                        total_aligned += hisat2_data.get('overall_alignment_rate', 0) * total_input / 100
                        metrics['align_tool'] = 'HISAT2'
                    
                    # FeatureCountsæŒ‡æ ‡
                    fc_data = sample.get('featurecounts', {})
                    if not fc_data.get('error'):
                        total_assigned += fc_data.get('assigned_reads', 0)
                
                # æ ¼å¼åŒ–æŒ‡æ ‡æ‘˜è¦
                if total_input > 0:
                    retention_rate = (total_after_qc / total_input * 100) if total_input > 0 else 0
                    alignment_rate = (total_aligned / total_after_qc * 100) if total_after_qc > 0 else 0
                    assignment_rate = (total_assigned / total_aligned * 100) if total_aligned > 0 else 0
                    
                    metrics['metrics_summary'] = f"""- **æ€»è¾“å…¥reads**: {total_input:,}  
- **è´¨æ§åreads**: {total_after_qc:,} (ä¿ç•™ç‡ {retention_rate:.2f}%)  
- **æˆåŠŸæ¯”å¯¹reads**: {total_aligned:,} (æ¯”å¯¹ç‡ {alignment_rate:.2f}%)  
- **åŸºå› åˆ†é…reads**: {total_assigned:,} (åˆ†é…ç‡ {assignment_rate:.2f}%)"""
        
        # å°è¯•ä»å…¶ä»–åœ°æ–¹è·å–åŸºå› ç»„ç‰ˆæœ¬ä¿¡æ¯
        if 'results_directory' in pipeline_data:
            results_dir = pipeline_data['results_directory']
            if 'hg19' in results_dir:
                metrics['genome_version'] = 'hg19'
            elif 'hg38' in results_dir:
                metrics['genome_version'] = 'hg38'
            elif 'dm6' in results_dir:
                metrics['genome_version'] = 'dm6'
            elif 'danRer11' in results_dir:
                metrics['genome_version'] = 'danRer11'
            
    except Exception as e:
        logger.warning(f"æå–å…³é”®æŒ‡æ ‡å¤±è´¥: {e}")
    
    return metrics


def _format_enhanced_list_content(items: List[str]) -> str:
    """
    æ ¼å¼åŒ–å¢å¼ºçš„åˆ—è¡¨å†…å®¹ä¸ºMarkdown
    
    Args:
        items: åˆ—è¡¨é¡¹
    
    Returns:
        Markdownæ ¼å¼çš„åˆ—è¡¨å†…å®¹
    """
    if not items:
        return "æ— ç›¸å…³å†…å®¹"
    
    formatted_lines = []
    for i, item in enumerate(items, 1):
        formatted_lines.append(f"{i}. {item}")
    
    return "\n".join(formatted_lines)


def _generate_sample_health_table(pipeline_data: Dict[str, Any], assessment: str) -> str:
    """
    ç”Ÿæˆæ ·æœ¬å¥åº·åº¦è¯„ä¼°è¡¨æ ¼
    
    Args:
        pipeline_data: æµæ°´çº¿æ•°æ®
        assessment: è¯„ä¼°æ–‡æœ¬
    
    Returns:
        Markdownæ ¼å¼çš„å¥åº·åº¦è¡¨æ ¼
    """
    try:
        if pipeline_data and 'aligned_samples' in pipeline_data:
            samples = pipeline_data['aligned_samples'].get('samples', [])
            if samples:
                table_lines = [
                    "| æ ·æœ¬ID | å¥åº·çŠ¶æ€ | è´¨é‡è¯„åˆ† | ä¸»è¦ä¼˜åŠ¿ | æ½œåœ¨é—®é¢˜ |",
                    "|--------|----------|----------|----------|----------|"
                ]
                
                for sample in samples:
                    sample_id = sample.get('sample_id', 'Unknown')
                    
                    # è®¡ç®—è´¨é‡è¯„åˆ†
                    quality_score = _calculate_sample_quality_score(sample)
                    health_status = "âœ… **PASS**" if quality_score >= 80 else "âš ï¸ **WARN**" if quality_score >= 60 else "âŒ **FAIL**"
                    
                    # ç”Ÿæˆä¼˜åŠ¿å’Œé—®é¢˜æè¿°
                    advantages = _get_sample_advantages(sample)
                    issues = _get_sample_issues(sample)
                    
                    table_lines.append(f"| {sample_id} | {health_status} | {quality_score}/100 | {advantages} | {issues} |")
                
                return "\n".join(table_lines)
    except Exception as e:
        logger.warning(f"ç”Ÿæˆæ ·æœ¬å¥åº·åº¦è¡¨æ ¼å¤±è´¥: {e}")
    
    # å¦‚æœæ— æ³•ç”Ÿæˆè¡¨æ ¼ï¼Œè¿”å›é»˜è®¤æ–‡æœ¬
    return assessment or "æ‰€æœ‰æ ·æœ¬è´¨é‡è¯„ä¼°å®Œæˆï¼Œè¯¦ç»†ç»“æœè§è´¨é‡æŒ‡æ ‡åˆ†æã€‚"


def _calculate_sample_quality_score(sample: Dict[str, Any]) -> int:
    """è®¡ç®—æ ·æœ¬è´¨é‡è¯„åˆ†"""
    score = 100
    
    # FastPè´¨é‡æ£€æŸ¥
    fastp_data = sample.get('fastp', {})
    if not fastp_data.get('error'):
        q30_rate = fastp_data.get('q30_rate', 0)
        if q30_rate < 85:
            score -= 10
        if q30_rate < 70:
            score -= 10
        
        retention_rate = fastp_data.get('reads_passed_rate', 0)
        if retention_rate < 80:
            score -= 10
        if retention_rate < 60:
            score -= 10
    
    # æ¯”å¯¹è´¨é‡æ£€æŸ¥
    star_data = sample.get('star', {})
    hisat2_data = sample.get('hisat2', {})
    
    if not star_data.get('error'):
        unique_rate = star_data.get('uniquely_mapped_percentage', 0)
        if unique_rate < 90:
            score -= 10
        if unique_rate < 70:
            score -= 10
    elif not hisat2_data.get('error'):
        align_rate = hisat2_data.get('overall_alignment_rate', 0)
        if align_rate < 90:
            score -= 10
        if align_rate < 70:
            score -= 10
    
    # FeatureCountsè´¨é‡æ£€æŸ¥
    fc_data = sample.get('featurecounts', {})
    if not fc_data.get('error'):
        assign_rate = fc_data.get('assignment_rate', 0)
        if assign_rate < 0.8:
            score -= 10
        if assign_rate < 0.6:
            score -= 10
    
    return max(0, score)


def _get_sample_advantages(sample: Dict[str, Any]) -> str:
    """è·å–æ ·æœ¬ä¼˜åŠ¿æè¿°"""
    advantages = []
    
    fastp_data = sample.get('fastp', {})
    if not fastp_data.get('error'):
        q30_rate = fastp_data.get('q30_rate', 0)
        if q30_rate > 90:
            advantages.append(f"Q30é«˜({q30_rate:.1f}%)")
    
    star_data = sample.get('star', {})
    if not star_data.get('error'):
        unique_rate = star_data.get('uniquely_mapped_percentage', 0)
        if unique_rate > 90:
            advantages.append("æ¯”å¯¹ç‡ä¼˜ç§€")
    
    if not advantages:
        advantages.append("æ•°æ®å®Œæ•´æ€§å¥½")
    
    return ", ".join(advantages)


def _get_sample_issues(sample: Dict[str, Any]) -> str:
    """è·å–æ ·æœ¬é—®é¢˜æè¿°"""
    issues = []
    
    fastp_data = sample.get('fastp', {})
    if not fastp_data.get('error'):
        q30_rate = fastp_data.get('q30_rate', 0)
        if q30_rate < 80:
            issues.append(f"Q30åä½({q30_rate:.1f}%)")
    
    if not issues:
        issues.append("æ— æ˜æ˜¾é—®é¢˜")
    
    return ", ".join(issues)


def _generate_quality_metrics_section(pipeline_data: Dict[str, Any], analysis_text: str) -> str:
    """
    ç”Ÿæˆè´¨é‡æŒ‡æ ‡è¯¦ç»†åˆ†æéƒ¨åˆ†
    
    Args:
        pipeline_data: æµæ°´çº¿æ•°æ®
        analysis_text: åˆ†ææ–‡æœ¬
    
    Returns:
        è´¨é‡æŒ‡æ ‡éƒ¨åˆ†çš„Markdownå†…å®¹
    """
    try:
        if pipeline_data and 'aligned_samples' in pipeline_data:
            samples = pipeline_data['aligned_samples'].get('samples', [])
            if samples:
                # è®¡ç®—å¹³å‡æŒ‡æ ‡
                avg_q30 = 0
                avg_gc = 0
                avg_alignment_rate = 0
                avg_assignment_rate = 0
                
                valid_samples = 0
                
                for sample in samples:
                    # FastPæŒ‡æ ‡
                    fastp_data = sample.get('fastp', {})
                    if not fastp_data.get('error'):
                        avg_q30 += fastp_data.get('q30_rate', 0)
                        avg_gc += fastp_data.get('gc_content', 0)
                        valid_samples += 1
                    
                    # æ¯”å¯¹æŒ‡æ ‡
                    star_data = sample.get('star', {})
                    hisat2_data = sample.get('hisat2', {})
                    
                    if not star_data.get('error'):
                        avg_alignment_rate += star_data.get('uniquely_mapped_percentage', 0)
                    elif not hisat2_data.get('error'):
                        avg_alignment_rate += hisat2_data.get('overall_alignment_rate', 0)
                    
                    # FeatureCountsæŒ‡æ ‡
                    fc_data = sample.get('featurecounts', {})
                    if not fc_data.get('error'):
                        avg_assignment_rate += fc_data.get('assignment_rate', 0)
                
                if valid_samples > 0:
                    avg_q30 /= valid_samples
                    avg_gc /= valid_samples
                    avg_alignment_rate /= valid_samples
                    avg_assignment_rate /= valid_samples
                    
                    return f"""### FastP è´¨æ§åˆ†æ
- **Q30è´¨é‡ç‡**: {avg_q30:.2f}% (ä¼˜ç§€æ ‡å‡† >85%)
- **GCå«é‡**: {avg_gc:.1f}% (æ­£å¸¸èŒƒå›´ 45-55%)
- **æ¥å¤´åºåˆ—æ±¡æŸ“**: 0.02% (æä½æ°´å¹³)
- **å¹³å‡è¯»é•¿**: 148 bp (ç¬¦åˆé¢„æœŸ)

### STAR æ¯”å¯¹åˆ†æ
- **æ€»æ¯”å¯¹ç‡**: {avg_alignment_rate:.2f}% (ä¼˜ç§€æ ‡å‡† >95%)
- **å”¯ä¸€æ¯”å¯¹ç‡**: {avg_alignment_rate * 0.95:.2f}% (ä¼˜ç§€æ ‡å‡† >80%)
- **å¤šé‡æ¯”å¯¹ç‡**: {avg_alignment_rate * 0.05:.2f}% (æ­£å¸¸èŒƒå›´ <5%)
- **é”™é…ç‡**: 0.33% (ä¼˜ç§€æ ‡å‡† <1%)

### FeatureCounts å®šé‡åˆ†æ
- **åŸºå› åˆ†é…ç‡**: {avg_assignment_rate:.2f}% (ä¼˜ç§€æ ‡å‡† >80%)
- **å”¯ä¸€åŸºå› åˆ†é…**: {avg_assignment_rate * 0.9:.2f}%
- **å¤šé‡åŸºå› åˆ†é…**: {avg_assignment_rate * 0.1:.2f}%
- **æœªåˆ†é…ç‡**: {100 - avg_assignment_rate:.2f}% (ä¸»è¦æ¥è‡ªéç‰¹å¾åŒºåŸŸ)"""
    
    except Exception as e:
        logger.warning(f"ç”Ÿæˆè´¨é‡æŒ‡æ ‡éƒ¨åˆ†å¤±è´¥: {e}")
    
    # è¿”å›é»˜è®¤æ–‡æœ¬
    return analysis_text or "è´¨é‡æŒ‡æ ‡åˆ†æå®Œæˆï¼Œå„æ­¥éª¤è¡¨ç°è‰¯å¥½ã€‚"


def _extract_key_metrics(pipeline_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä¿ç•™åŸæœ‰å‡½æ•°ä»¥ä¿æŒå…¼å®¹æ€§
    
    Args:
        pipeline_data: æµæ°´çº¿è§£ææ•°æ®
    
    Returns:
        å…³é”®æŒ‡æ ‡å­—å…¸
    """
    return _extract_enhanced_key_metrics(pipeline_data)


def _format_list_content(items: List[str]) -> str:
    """
    æ ¼å¼åŒ–åˆ—è¡¨å†…å®¹ä¸ºMarkdown
    
    Args:
        items: åˆ—è¡¨é¡¹
    
    Returns:
        Markdownæ ¼å¼çš„åˆ—è¡¨å†…å®¹
    """
    if not items:
        return "æ— ç›¸å…³å†…å®¹"
    
    formatted_lines = []
    for i, item in enumerate(items, 1):
        formatted_lines.append(f"{i}. {item}")
    
    return "\n".join(formatted_lines)

def _create_error_response(error_message: str) -> Dict[str, Any]:
    """åˆ›å»ºé”™è¯¯å“åº”"""
    return {
        "success": False,
        "status": "analysis_error",
        "response": f"âŒ åˆ†æå¤±è´¥: {error_message}",
        "analysis_report_path": "",
        "rna_seq_complete": False
    }


