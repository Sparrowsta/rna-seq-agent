import json
import re
from typing import Dict, Any
from pathlib import Path

from ..core import get_shared_llm
from ..state import AgentState, AnalysisResponse


def extract_nextflow_metrics(data_dir: str = ".") -> Dict[str, Any]:
    """æå–Nextflowæ‰§è¡ŒæŒ‡æ ‡ - ç›´æ¥è¯»å–æŠ¥å‘Šæ–‡ä»¶"""
    metrics = {}
    
    # è¯»å–æ‰§è¡ŒæŠ¥å‘Š
    report_file = Path(data_dir) / "reports" / "execution_report.txt"
    timeline_file = Path(data_dir) / "reports" / "execution_timeline.txt"
    trace_file = Path(data_dir) / "reports" / "execution_trace.txt"
    
    try:
        # 1. è¯»å–æ‰§è¡ŒæŠ¥å‘Š
        if report_file.exists():
            with open(report_file, 'r', encoding='utf-8') as f:
                report_content = f.read()
                
            # æå–å·¥ä½œæµçŠ¶æ€ä¿¡æ¯
            if "Workflow completed successfully" in report_content:
                metrics["workflow_status"] = "success"
            elif "Workflow execution failed" in report_content or "ERROR" in report_content:
                metrics["workflow_status"] = "failed"
            else:
                metrics["workflow_status"] = "unknown"
                
        # 2. è¯»å–æ‰§è¡Œè·Ÿè¸ª
        if trace_file.exists():
            with open(trace_file, 'r', encoding='utf-8') as f:
                trace_content = f.read()
                
            # ç»Ÿè®¡è¿›ç¨‹æ‰§è¡Œæƒ…å†µ
            lines = trace_content.strip().split('\n')
            if len(lines) > 1:  # è·³è¿‡è¡¨å¤´
                process_stats = {"total": 0, "completed": 0, "failed": 0}
                for line in lines[1:]:
                    parts = line.split('\t')
                    if len(parts) > 6:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åˆ—
                        status = parts[6].strip()  # statusåˆ—
                        process_stats["total"] += 1
                        if status == "COMPLETED":
                            process_stats["completed"] += 1
                        elif status == "FAILED":
                            process_stats["failed"] += 1
                            
                metrics["process_stats"] = process_stats
                
    except (FileNotFoundError, UnicodeDecodeError, IndexError) as e:
        # å¦‚æœæŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥ï¼Œå›é€€åˆ°ä»æ‰§è¡Œè¾“å‡ºæ¨æ–­
        pass
    
    return metrics


def extract_fastp_metrics(data_dir: str = ".") -> Dict[str, Any]:
    """æå–fastpè´¨æ§æŒ‡æ ‡ - ç›´æ¥è¯»å–JSONæŠ¥å‘Šæ–‡ä»¶"""
    metrics = {}
    
    fastp_dir = Path(data_dir) / "results" / "fastp"
    if not fastp_dir.exists():
        return metrics
    
    # åˆå¹¶æ‰€æœ‰æ ·æœ¬çš„fastpç»Ÿè®¡
    total_before_reads = 0
    total_after_reads = 0
    total_before_bases = 0
    total_after_bases = 0
    sample_count = 0
    
    for sample_dir in fastp_dir.iterdir():
        if not sample_dir.is_dir():
            continue
            
        json_files = list(sample_dir.glob("*.fastp.json"))
        if json_files:
            try:
                with open(json_files[0], 'r') as f:
                    data = json.load(f)
                    summary = data.get("summary", {})
                    
                    before = summary.get("before_filtering", {})
                    after = summary.get("after_filtering", {})
                    
                    total_before_reads += before.get("total_reads", 0)
                    total_after_reads += after.get("total_reads", 0)
                    total_before_bases += before.get("total_bases", 0)
                    total_after_bases += after.get("total_bases", 0)
                    sample_count += 1
                    
            except (json.JSONDecodeError, FileNotFoundError, KeyError):
                continue
    
    if sample_count > 0:
        metrics["total_samples"] = sample_count
        metrics["total_before_reads"] = total_before_reads
        metrics["total_after_reads"] = total_after_reads
        metrics["total_before_bases"] = total_before_bases  
        metrics["total_after_bases"] = total_after_bases
        metrics["filtering_rate"] = f"{((total_before_reads - total_after_reads) / total_before_reads * 100):.1f}%" if total_before_reads > 0 else "0%"
    
    return metrics


def extract_star_metrics(execution_output: str) -> Dict[str, Any]:
    """æå–STARæ¯”å¯¹æŒ‡æ ‡"""
    metrics = {}
    
    patterns = {
        "uniquely_mapped": r'Uniquely mapped reads %\s+\|\s+(\d+\.?\d*)%',
        "multimapped": r'% of reads mapped to multiple loci\s+\|\s+(\d+\.?\d*)%',
        "unmapped": r'% of reads unmapped.*?\s+\|\s+(\d+\.?\d*)%'
    }
    
    for key, pattern in patterns.items():
        match = re.search(pattern, execution_output)
        if match:
            metrics[key] = match.group(1) + "%"
    
    return metrics


def extract_featurecounts_metrics(data_dir: str = ".") -> Dict[str, Any]:
    """æå–featureCountså®šé‡æŒ‡æ ‡ - ç›´æ¥è¯»å–summaryæ–‡ä»¶"""
    metrics = {}
    
    summary_file = Path(data_dir) / "results" / "featurecounts" / "all_samples.counts.txt.summary"
    if not summary_file.exists():
        return metrics
    
    try:
        with open(summary_file, 'r') as f:
            lines = f.readlines()
            
        if len(lines) < 2:
            return metrics
            
        # ç¬¬ä¸€è¡Œæ˜¯è¡¨å¤´ï¼ŒåŒ…å«æ ·æœ¬å
        header = lines[0].strip().split('\t')
        sample_count = len(header) - 1  # å‡å»Statusåˆ—
        
        total_assigned = 0
        total_unassigned = 0
        
        for line in lines[1:]:
            parts = line.strip().split('\t')
            if len(parts) < 2:
                continue
                
            status = parts[0]
            counts = [int(x) for x in parts[1:] if x.isdigit()]
            
            if status == "Assigned":
                total_assigned = sum(counts)
            elif status.startswith("Unassigned"):
                total_unassigned += sum(counts)
        
        total_reads = total_assigned + total_unassigned
        
        metrics["sample_count"] = sample_count
        metrics["assigned_reads"] = total_assigned
        metrics["unassigned_reads"] = total_unassigned
        metrics["total_processed_reads"] = total_reads
        metrics["assignment_rate"] = f"{(total_assigned / total_reads * 100):.1f}%" if total_reads > 0 else "0%"
        
    except (FileNotFoundError, ValueError, IndexError):
        pass
    
    return metrics


def save_analysis_report(analysis_response: AnalysisResponse, data_dir: str = ".") -> Dict[str, str]:
    """ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°æ–‡ä»¶"""
    import datetime
    
    results_path = Path(data_dir) / "results"
    results_path.mkdir(exist_ok=True)
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    saved_files = {}
    
    try:
        # 1. ä¿å­˜JSONæ ¼å¼çš„å®Œæ•´æ•°æ®
        json_file = results_path / f"analysis_report_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                "timestamp": timestamp,
                "analysis_summary": analysis_response.analysis_summary,
                "analysis_insights": analysis_response.analysis_insights,
                "result_files": analysis_response.result_files,
                "quality_metrics": analysis_response.quality_metrics,
                "next_steps": analysis_response.next_steps
            }, f, ensure_ascii=False, indent=2)
        saved_files["json"] = str(json_file.relative_to(Path(data_dir)))
        
        # 2. ä¿å­˜Markdownæ ¼å¼çš„å¯è¯»æŠ¥å‘Š
        md_file = results_path / f"analysis_summary_{timestamp}.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(f"# RNA-seq åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write(f"## åˆ†ææ€»ç»“\n\n{analysis_response.analysis_summary}\n\n")
            
            if analysis_response.analysis_insights:
                f.write(f"## åˆ†ææ´å¯Ÿ\n\n")
                for insight in analysis_response.analysis_insights:
                    f.write(f"- {insight}\n")
                f.write(f"\n")
            
            if analysis_response.result_files:
                f.write(f"## ç»“æœæ–‡ä»¶\n\n")
                for category, path in analysis_response.result_files.items():
                    f.write(f"- **{category}**: `{path}`\n")
                f.write(f"\n")
            
            if analysis_response.next_steps:
                f.write(f"## ä¸‹ä¸€æ­¥å»ºè®®\n\n")
                for step in analysis_response.next_steps:
                    f.write(f"- {step}\n")
        saved_files["markdown"] = str(md_file.relative_to(Path(data_dir)))
        
        # 3. ä¿å­˜æœ€æ–°æŠ¥å‘Šçš„ç¬¦å·é“¾æ¥
        latest_json = results_path / "analysis_report_latest.json"
        latest_md = results_path / "analysis_summary_latest.md"
        
        # åˆ é™¤æ—§çš„ç¬¦å·é“¾æ¥(å¦‚æœå­˜åœ¨)
        if latest_json.is_symlink():
            latest_json.unlink()
        if latest_md.is_symlink():
            latest_md.unlink()
            
        # åˆ›å»ºæ–°çš„ç¬¦å·é“¾æ¥
        latest_json.symlink_to(json_file.name)
        latest_md.symlink_to(md_file.name)
        
        saved_files["latest_json"] = str(latest_json.relative_to(Path(data_dir)))
        saved_files["latest_md"] = str(latest_md.relative_to(Path(data_dir)))
        
    except Exception as e:
        print(f"ä¿å­˜åˆ†ææŠ¥å‘Šæ—¶å‡ºé”™: {e}")
    
    return saved_files


def scan_result_files(data_dir: str = ".") -> Dict[str, Any]:
    """æ‰«æå¹¶ç»Ÿè®¡ç»“æœæ–‡ä»¶"""
    file_stats = {
        "total_files": 0,
        "file_categories": {},
        "key_files": {}
    }
    
    results_path = Path(data_dir) / "results"
    if not results_path.exists():
        return file_stats
    
    # å®šä¹‰æ–‡ä»¶ç±»åˆ«
    categories = {
        "quality_reports": [".html", ".json"],
        "alignment_files": [".bam", ".sam"],
        "quantification": [".txt", ".tsv", ".counts"],
        "logs": [".log", ".out", ".err"]
    }
    
    all_files = list(results_path.rglob("*"))
    file_stats["total_files"] = len([f for f in all_files if f.is_file()])
    
    for category, extensions in categories.items():
        category_files = []
        for ext in extensions:
            category_files.extend([f for f in all_files if f.suffix == ext])
        
        file_stats["file_categories"][category] = len(category_files)
        if category_files:
            # è®°å½•ç¬¬ä¸€ä¸ªæ–‡ä»¶ä½œä¸ºä»£è¡¨
            file_stats["key_files"][category] = str(category_files[0].relative_to(Path(data_dir)))
    
    return file_stats


async def map_analysis_to_agent_state(analysis_response: AnalysisResponse, state: AgentState) -> Dict[str, Any]:
    """å°†AnalysisResponseæ˜ å°„åˆ°å®Œæ•´çš„AgentState"""
    return {
        "messages": state.messages,
        "input": state.input,
        "response": analysis_response.analysis_summary,
        "status": "analysis_complete",
        "analysis_summary": analysis_response.analysis_summary,
        "analysis_insights": analysis_response.analysis_insights,
        "result_files": analysis_response.result_files,
        "quality_metrics": analysis_response.quality_metrics,
        "next_steps": analysis_response.next_steps,
        # ä¿æŒæ‰§è¡ŒçŠ¶æ€
        "execution_status": state.execution_status,
        "execution_output": state.execution_output,
        "execution_result": state.execution_result,
        "nextflow_config": state.nextflow_config,
    }


async def analysis_node(state: AgentState) -> Dict[str, Any]:
    """
    AnalysisèŠ‚ç‚¹ - æå–å…³é”®æŒ‡æ ‡å¹¶è®©LLMç”Ÿæˆæ™ºèƒ½åˆ†ææ€»ç»“
    
    æ¶æ„ï¼š
    1. ä»æ‰§è¡Œè¾“å‡ºä¸­æå–å…·ä½“æŒ‡æ ‡
    2. æ‰«æç»“æœæ–‡ä»¶ç»Ÿè®¡
    3. å°†ç»“æ„åŒ–æŒ‡æ ‡äº¤ç»™LLMåˆ†æ
    4. LLMç”Ÿæˆä¸“ä¸šçš„æ€»ç»“å’Œå»ºè®®
    """
    
    execution_output = getattr(state, 'execution_output', '')
    execution_status = getattr(state, 'execution_status', '')
    nextflow_config = getattr(state, 'nextflow_config', {})
    
    # 1. æå–å„å·¥å…·çš„å…·ä½“æŒ‡æ ‡  
    data_dir = "."  # Dockerå®¹å™¨å†…å½“å‰ç›®å½•å°±æ˜¯/data
    extracted_metrics = {
        "nextflow": extract_nextflow_metrics(data_dir),
        "fastp": extract_fastp_metrics(data_dir),
        "star": extract_star_metrics(execution_output), 
        "featurecounts": extract_featurecounts_metrics(data_dir),
        "file_stats": scan_result_files(data_dir)
    }
    
    # 2. å‡†å¤‡é…ç½®ä¿¡æ¯ä¸Šä¸‹æ–‡
    analysis_context = {
        "execution_status": execution_status,
        "tools_used": {
            "qc_tool": nextflow_config.get("qc_tool", "unknown"),
            "align_tool": nextflow_config.get("align_tool", "unknown"),
            "quant_tool": nextflow_config.get("quant_tool", "unknown"),
            "genome_version": nextflow_config.get("genome_version", "unknown")
        },
        "extracted_metrics": extracted_metrics
    }
    
    # 3. è®©LLMåŸºäºå…·ä½“æŒ‡æ ‡ç”Ÿæˆä¸“ä¸šåˆ†æ
    llm = get_shared_llm()
    structured_llm = llm.with_structured_output(AnalysisResponse, method="json_mode")
    
    analysis_prompt = f"""ä½ æ˜¯RNA-seqæ•°æ®åˆ†æä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹å…·ä½“çš„æŠ€æœ¯æŒ‡æ ‡ç”Ÿæˆä¸“ä¸šçš„åˆ†ææ€»ç»“æŠ¥å‘Šã€‚

## åˆ†æé…ç½®
{json.dumps(analysis_context["tools_used"], ensure_ascii=False, indent=2)}

## æ‰§è¡ŒçŠ¶æ€
{execution_status}

## å…·ä½“æŠ€æœ¯æŒ‡æ ‡
{json.dumps(extracted_metrics, ensure_ascii=False, indent=2)}

è¯·ç”Ÿæˆä¸“ä¸šçš„RNA-seqåˆ†ææŠ¥å‘Šï¼ŒåŒ…å«ï¼š

1. **analysis_summary**: åŸºäºå…·ä½“æŒ‡æ ‡çš„æ€»ç»“(3-4å¥è¯ï¼Œçªå‡ºå…³é”®æ•°å€¼)
   - å¦‚æœæœ‰æ¯”å¯¹ç‡ï¼Œè¯´æ˜æ¯”å¯¹æ•ˆæœ
   - å¦‚æœæœ‰è´¨æ§æ•°æ®ï¼Œè¯„ä¼°æ•°æ®è´¨é‡
   - å¦‚æœæœ‰å®šé‡ç»“æœï¼Œè¯´æ˜åŸºå› æ£€å‡ºæƒ…å†µ

2. **analysis_insights**: åŸºäºæ•°å€¼çš„ä¸“ä¸šæ´å¯Ÿ(æ¯æ¡åŒ…å«å…·ä½“æ•°æ®)
   - ä¾‹å¦‚ï¼š"âœ… STARæ¯”å¯¹æˆåŠŸç‡è¾¾åˆ°85.2%ï¼Œè¡¨æ˜æ ·æœ¬ä¸å‚è€ƒåŸºå› ç»„åŒ¹é…è‰¯å¥½"
   - ä¾‹å¦‚ï¼š"ğŸ“Š featureCountsæˆåŠŸåˆ†é…äº†2,345,678æ¡readsåˆ°åŸºå› ç‰¹å¾"

3. **result_files**: é‡è¦ç»“æœæ–‡ä»¶è·¯å¾„
4. **quality_metrics**: å…³é”®è´¨é‡æŒ‡æ ‡çš„ç»“æ„åŒ–æ•°æ®
5. **next_steps**: åŸºäºå½“å‰ç»“æœçš„å…·ä½“å»ºè®®

è¦æ±‚ï¼š
- ä½¿ç”¨ä¸­æ–‡
- åŸºäºå®é™…æ•°å€¼è¿›è¡Œè¯„ä¼°
- å¦‚æœæŸä¸ªæŒ‡æ ‡å¼‚å¸¸ï¼Œæ˜ç¡®æŒ‡å‡ºé—®é¢˜
- æä¾›å…·ä½“å¯è¡Œçš„æ”¹è¿›å»ºè®®
- è¾“å‡ºJSONæ ¼å¼
"""
    
    try:
        # LLMåŸºäºå…·ä½“æŒ‡æ ‡ç”Ÿæˆåˆ†æ
        raw_response = await structured_llm.ainvoke([{"role": "user", "content": analysis_prompt}])
        
        # ç¡®ä¿è¿”å›æ­£ç¡®çš„AnalysisResponseç±»å‹
        if isinstance(raw_response, dict):
            analysis_response = AnalysisResponse(**raw_response)
        else:
            analysis_response = raw_response
            
        # ç¡®ä¿æŠ€æœ¯æŒ‡æ ‡è¢«ä¿ç•™
        if not analysis_response.quality_metrics:
            analysis_response.quality_metrics = extracted_metrics
            
    except Exception as e:
        print(f"LLMåˆ†æå¤±è´¥: {e}")
        # å¤‡ç”¨å“åº”åŸºäºæå–çš„æŒ‡æ ‡
        analysis_response = AnalysisResponse(
            analysis_summary=f"åˆ†æå®Œæˆ({execution_status})ã€‚æ£€æµ‹åˆ°{len(extracted_metrics)}ç±»æŠ€æœ¯æŒ‡æ ‡ã€‚",
            analysis_insights=[f"å·¥å…·ç»„åˆ: {analysis_context['tools_used']}"],
            result_files=extracted_metrics.get("file_stats", {}).get("key_files", {}),
            quality_metrics=extracted_metrics,
            next_steps=["æ£€æŸ¥è¯¦ç»†çš„æŠ€æœ¯æŒ‡æ ‡è¿›è¡Œè¿›ä¸€æ­¥åˆ†æ"]
        )
    
    # 4. ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°æ–‡ä»¶
    saved_files = save_analysis_report(analysis_response)
    print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {saved_files}")
    
    return await map_analysis_to_agent_state(analysis_response, state)