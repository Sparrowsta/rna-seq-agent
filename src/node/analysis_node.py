import json
import re
from typing import Dict, Any, List
from pathlib import Path
import pandas as pd

from ..core import get_shared_llm
from ..state import AgentState, AnalysisResponse
from ..prompts import ANALYSIS_NODE_PROMPT
from langgraph.prebuilt import create_react_agent

def create_analysis_agent():
    """åˆ›å»ºAnalysisèŠ‚ç‚¹çš„æ™ºèƒ½åˆ†æAgent"""
    llm = get_shared_llm()
    
    # ä½¿ç”¨create_react_agentä½†ä¸æä¾›toolsï¼Œçº¯æ¨ç†æ¨¡å¼
    agent = create_react_agent(
        model=llm,
        tools=[],  # ç©ºå·¥å…·åˆ—è¡¨ï¼Œçº¯æ¨ç†
        prompt=ANALYSIS_NODE_PROMPT,  # ä½¿ç”¨é›†ä¸­ç®¡ç†çš„prompt
        response_format=AnalysisResponse
    )
    return agent

def extract_sample_fastp_metrics(sample_dir: Path) -> Dict[str, Any]:
    """æå–å•ä¸ªæ ·æœ¬çš„fastpæŒ‡æ ‡"""
    metrics = {
        'sample_id': sample_dir.name,
        'sequencing_type': 'unknown',
        'total_reads_raw': 0,
        'total_reads_clean': 0,
        'q20_rate': 0.0,
        'q30_rate': 0.0,
        'gc_content': 0.0,
        'duplication_rate': 0.0,
        'adapter_trimmed_reads': 0,
        'reads_filtered_rate': 0.0
    }
    
    json_files = list(sample_dir.glob("*.fastp.json"))
    if not json_files:
        return metrics
    
    try:
        with open(json_files[0], 'r') as f:
            data = json.load(f)
            
        summary = data.get("summary", {})
        filtering = data.get("filtering_result", {})
        duplication = data.get("duplication", {})
        adapter_cutting = data.get("adapter_cutting", {})
        
        before = summary.get("before_filtering", {})
        after = summary.get("after_filtering", {})
        
        # åŸºæœ¬ä¿¡æ¯
        metrics['sample_id'] = sample_dir.name
        sequencing = summary.get("sequencing", "")
        if "paired end" in sequencing.lower():
            metrics['sequencing_type'] = 'paired_end'
        elif "single end" in sequencing.lower():
            metrics['sequencing_type'] = 'single_end'
        
        # è¯»å–æ•°é‡
        total_raw = before.get("total_reads", 0)
        total_clean = after.get("total_reads", 0)
        metrics['total_reads_raw'] = total_raw
        metrics['total_reads_clean'] = total_clean
        
        # è¿‡æ»¤ç‡
        if total_raw > 0:
            metrics['reads_filtered_rate'] = (total_raw - total_clean) / total_raw
        
        # è´¨é‡æŒ‡æ ‡
        metrics['q20_rate'] = after.get("q20_rate", 0.0)
        metrics['q30_rate'] = after.get("q30_rate", 0.0)
        metrics['gc_content'] = after.get("gc_content", 0.0)
        
        # é‡å¤ç‡
        metrics['duplication_rate'] = duplication.get("rate", 0.0)
        
        # æ¥å¤´ä¿®å‰ª
        metrics['adapter_trimmed_reads'] = adapter_cutting.get("adapter_trimmed_reads", 0)
        
    except (json.JSONDecodeError, FileNotFoundError, KeyError) as e:
        print(f"Warning: Failed to parse fastp data for {sample_dir.name}: {e}")
    
    return metrics


def extract_sample_star_metrics(sample_dir: Path) -> Dict[str, Any]:
    """æå–å•ä¸ªæ ·æœ¬çš„STARæŒ‡æ ‡"""
    metrics = {
        'sample_id': sample_dir.name,
        'input_reads': 0,
        'uniquely_mapped_reads': 0,
        'uniquely_mapped_rate': 0.0,
        'multi_mapped_reads': 0,
        'multi_mapped_rate': 0.0,
        'unmapped_reads': 0,
        'unmapped_rate': 0.0,
        'mismatch_rate': 0.0,
        'splice_sites_total': 0
    }
    
    log_files = list(sample_dir.glob("*.star.log"))
    if not log_files:
        return metrics
    
    try:
        with open(log_files[0], 'r', encoding='utf-8') as f:
            log_content = f.read()
        
        # æå–å…³é”®ç»Ÿè®¡æŒ‡æ ‡
        patterns = {
            "input_reads": r'Number of input reads \|\s+(\d+)',
            "uniquely_mapped": r'Uniquely mapped reads number \|\s+(\d+)',
            "uniquely_mapped_pct": r'Uniquely mapped reads % \|\s+([\d.]+)%',
            "multi_mapped": r'Number of reads mapped to multiple loci \|\s+(\d+)',
            "multi_mapped_pct": r'% of reads mapped to multiple loci \|\s+([\d.]+)%',
            "unmapped_mismatches": r'Number of reads unmapped: too many mismatches \|\s+(\d+)',
            "unmapped_short": r'Number of reads unmapped: too short \|\s+(\d+)',
            "unmapped_other": r'Number of reads unmapped: other \|\s+(\d+)',
            "mismatch_rate": r'Mismatch rate per base, % \|\s+([\d.]+)%',
            "splice_total": r'Number of splices: Total \|\s+(\d+)'
        }
        
        sample_data = {}
        for key, pattern in patterns.items():
            match = re.search(pattern, log_content)
            if match:
                sample_data[key] = match.group(1)
        
        # å¡«å……æŒ‡æ ‡
        metrics['sample_id'] = sample_dir.name
        
        if "input_reads" in sample_data:
            input_reads = int(sample_data["input_reads"])
            metrics['input_reads'] = input_reads
            
            if "uniquely_mapped" in sample_data:
                uniquely_mapped = int(sample_data["uniquely_mapped"])
                metrics['uniquely_mapped_reads'] = uniquely_mapped
                metrics['uniquely_mapped_rate'] = uniquely_mapped / input_reads if input_reads > 0 else 0.0
            
            if "multi_mapped" in sample_data:
                multi_mapped = int(sample_data["multi_mapped"])
                metrics['multi_mapped_reads'] = multi_mapped
                metrics['multi_mapped_rate'] = multi_mapped / input_reads if input_reads > 0 else 0.0
            
            # è®¡ç®—æ€»çš„æœªæ¯”å¯¹reads
            unmapped_total = 0
            for key in ["unmapped_mismatches", "unmapped_short", "unmapped_other"]:
                if key in sample_data:
                    unmapped_total += int(sample_data[key])
            
            metrics['unmapped_reads'] = unmapped_total
            metrics['unmapped_rate'] = unmapped_total / input_reads if input_reads > 0 else 0.0
        
        if "mismatch_rate" in sample_data:
            metrics['mismatch_rate'] = float(sample_data["mismatch_rate"]) / 100.0
        
        if "splice_total" in sample_data:
            metrics['splice_sites_total'] = int(sample_data["splice_total"])
            
    except (FileNotFoundError, ValueError, AttributeError) as e:
        print(f"Warning: Failed to parse STAR data for {sample_dir.name}: {e}")
    
    return metrics


def extract_sample_featurecounts_metrics(summary_file: Path) -> Dict[str, Dict[str, Any]]:
    """æå–æ‰€æœ‰æ ·æœ¬çš„featureCountsæŒ‡æ ‡"""
    sample_metrics = {}
    
    if not summary_file.exists():
        return sample_metrics
    
    try:
        # è¯»å–summaryæ–‡ä»¶
        df = pd.read_csv(summary_file, sep='\t')
        
        # è·å–æ ·æœ¬åˆ—ï¼ˆé™¤äº†Statusåˆ—ï¼‰
        sample_columns = [col for col in df.columns if col != 'Status']
        
        for sample_col in sample_columns:
            # æå–æ ·æœ¬IDï¼ˆå»æ‰.bamåç¼€ï¼‰
            sample_id = sample_col.replace('.bam', '')
            
            # è®¡ç®—å„é¡¹æŒ‡æ ‡
            assigned = df[df['Status'] == 'Assigned'][sample_col].iloc[0] if len(df[df['Status'] == 'Assigned']) > 0 else 0
            total_reads = df[sample_col].sum()
            
            unassigned_unmapped = df[df['Status'] == 'Unassigned_Unmapped'][sample_col].iloc[0] if len(df[df['Status'] == 'Unassigned_Unmapped']) > 0 else 0
            unassigned_multimapping = df[df['Status'] == 'Unassigned_MultiMapping'][sample_col].iloc[0] if len(df[df['Status'] == 'Unassigned_MultiMapping']) > 0 else 0
            unassigned_nofeatures = df[df['Status'] == 'Unassigned_NoFeatures'][sample_col].iloc[0] if len(df[df['Status'] == 'Unassigned_NoFeatures']) > 0 else 0
            unassigned_ambiguity = df[df['Status'] == 'Unassigned_Ambiguity'][sample_col].iloc[0] if len(df[df['Status'] == 'Unassigned_Ambiguity']) > 0 else 0
            
            sample_metrics[sample_id] = {
                'sample_id': sample_id,
                'assigned_reads': int(assigned),
                'assigned_rate': assigned / total_reads if total_reads > 0 else 0.0,
                'unassigned_unmapped': int(unassigned_unmapped),
                'unassigned_multimapping': int(unassigned_multimapping),
                'unassigned_nofeatures': int(unassigned_nofeatures),
                'unassigned_ambiguity': int(unassigned_ambiguity),
                'total_processed_reads': int(total_reads),
                'unassigned_unmapped_rate': unassigned_unmapped / total_reads if total_reads > 0 else 0.0,
                'unassigned_multimapping_rate': unassigned_multimapping / total_reads if total_reads > 0 else 0.0
            }
            
    except (FileNotFoundError, ValueError, KeyError) as e:
        print(f"Warning: Failed to parse featureCounts data: {e}")
    
    return sample_metrics


def build_sample_quality_dataframe(data_dir: str = ".") -> pd.DataFrame:
    """æ„å»ºæ ·æœ¬è´¨é‡åˆ†æçš„pandas DataFrame"""
    
    # æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„æŒ‡æ ‡
    all_sample_metrics = []
    
    # 1. æå–fastpæŒ‡æ ‡
    fastp_dir = Path(data_dir) / "results" / "fastp"
    fastp_metrics = {}
    if fastp_dir.exists():
        for sample_dir in fastp_dir.iterdir():
            if sample_dir.is_dir():
                metrics = extract_sample_fastp_metrics(sample_dir)
                fastp_metrics[metrics['sample_id']] = metrics
    
    # 2. æå–STARæŒ‡æ ‡
    star_dir = Path(data_dir) / "results" / "bam"
    star_metrics = {}
    if star_dir.exists():
        for sample_dir in star_dir.iterdir():
            if sample_dir.is_dir():
                metrics = extract_sample_star_metrics(sample_dir)
                star_metrics[metrics['sample_id']] = metrics
    
    # 3. æå–featureCountsæŒ‡æ ‡
    featurecounts_file = Path(data_dir) / "results" / "featurecounts" / "all_samples.counts.txt.summary"
    featurecounts_metrics = extract_sample_featurecounts_metrics(featurecounts_file)
    
    # 4. åˆå¹¶æ‰€æœ‰æ ·æœ¬çš„æŒ‡æ ‡
    all_sample_ids = set()
    all_sample_ids.update(fastp_metrics.keys())
    all_sample_ids.update(star_metrics.keys())
    all_sample_ids.update(featurecounts_metrics.keys())
    
    for sample_id in all_sample_ids:
        sample_data = {'sample_id': sample_id}
        
        # åˆå¹¶fastpæŒ‡æ ‡
        if sample_id in fastp_metrics:
            sample_data.update(fastp_metrics[sample_id])
        
        # åˆå¹¶STARæŒ‡æ ‡
        if sample_id in star_metrics:
            star_data = star_metrics[sample_id]
            # é‡å‘½åé¿å…å†²çª
            sample_data.update({
                'star_input_reads': star_data['input_reads'],
                'uniquely_mapped_reads': star_data['uniquely_mapped_reads'],
                'uniquely_mapped_rate': star_data['uniquely_mapped_rate'],
                'multi_mapped_reads': star_data['multi_mapped_reads'],
                'multi_mapped_rate': star_data['multi_mapped_rate'],
                'unmapped_reads': star_data['unmapped_reads'],
                'unmapped_rate': star_data['unmapped_rate'],
                'mismatch_rate': star_data['mismatch_rate'],
                'splice_sites_total': star_data['splice_sites_total']
            })
        
        # åˆå¹¶featureCountsæŒ‡æ ‡
        if sample_id in featurecounts_metrics:
            fc_data = featurecounts_metrics[sample_id]
            sample_data.update({
                'assigned_reads': fc_data['assigned_reads'],
                'assigned_rate': fc_data['assigned_rate'],
                'unassigned_unmapped': fc_data['unassigned_unmapped'],
                'unassigned_multimapping': fc_data['unassigned_multimapping'],
                'unassigned_nofeatures': fc_data['unassigned_nofeatures'],
                'unassigned_ambiguity': fc_data['unassigned_ambiguity'],
                'fc_total_processed_reads': fc_data['total_processed_reads'],
                'unassigned_unmapped_rate': fc_data['unassigned_unmapped_rate'],
                'unassigned_multimapping_rate': fc_data['unassigned_multimapping_rate']
            })
        
        all_sample_metrics.append(sample_data)
    
    # 5. åˆ›å»ºDataFrame
    if all_sample_metrics:
        df = pd.DataFrame(all_sample_metrics)
        
        # æ·»åŠ è´¨é‡è¯„ä¼°
        df = add_quality_assessment(df)
        
        return df
    else:
        # è¿”å›ç©ºDataFrameä½†åŒ…å«é¢„æœŸçš„åˆ—
        return pd.DataFrame(columns=[
            'sample_id', 'sequencing_type', 'total_reads_raw', 'total_reads_clean',
            'q20_rate', 'q30_rate', 'gc_content', 'duplication_rate',
            'uniquely_mapped_rate', 'multi_mapped_rate', 'unmapped_rate',
            'assigned_rate', 'quality_score', 'quality_status'
        ])


def add_quality_assessment(df: pd.DataFrame) -> pd.DataFrame:
    """ä¸ºDataFrameæ·»åŠ è´¨é‡è¯„ä¼°"""
    
    # è´¨é‡é˜ˆå€¼
    thresholds = {
        'min_mapping_rate': 0.70,
        'max_duplication_rate': 0.30,
        'min_assignment_rate': 0.60,
        'min_q30_rate': 0.80,
        'gc_content_range': (0.40, 0.60),
        'max_mismatch_rate': 0.05
    }
    
    # è®¡ç®—è´¨é‡åˆ†æ•°å’ŒçŠ¶æ€
    quality_scores = []
    quality_statuses = []
    quality_flags = []
    
    for _, row in df.iterrows():
        score = 100  # èµ·å§‹åˆ†æ•°
        flags = []
        
        # æ£€æŸ¥æ¯”å¯¹ç‡
        if 'uniquely_mapped_rate' in row and pd.notna(row['uniquely_mapped_rate']):
            if row['uniquely_mapped_rate'] < thresholds['min_mapping_rate']:
                score -= 20
                flags.append('low_mapping_rate')
        
        # æ£€æŸ¥é‡å¤ç‡
        if 'duplication_rate' in row and pd.notna(row['duplication_rate']):
            if row['duplication_rate'] > thresholds['max_duplication_rate']:
                score -= 15
                flags.append('high_duplication')
        
        # æ£€æŸ¥åŸºå› åˆ†é…ç‡
        if 'assigned_rate' in row and pd.notna(row['assigned_rate']):
            if row['assigned_rate'] < thresholds['min_assignment_rate']:
                score -= 20
                flags.append('low_assignment_rate')
        
        # æ£€æŸ¥Q30ç‡
        if 'q30_rate' in row and pd.notna(row['q30_rate']):
            if row['q30_rate'] < thresholds['min_q30_rate']:
                score -= 15
                flags.append('low_q30_rate')
        
        # æ£€æŸ¥GCå«é‡
        if 'gc_content' in row and pd.notna(row['gc_content']):
            gc_min, gc_max = thresholds['gc_content_range']
            if row['gc_content'] < gc_min or row['gc_content'] > gc_max:
                score -= 10
                flags.append('abnormal_gc_content')
        
        # æ£€æŸ¥é”™é…ç‡
        if 'mismatch_rate' in row and pd.notna(row['mismatch_rate']):
            if row['mismatch_rate'] > thresholds['max_mismatch_rate']:
                score -= 10
                flags.append('high_mismatch_rate')
        
        # ç¡®ä¿åˆ†æ•°ä¸ä½äº0
        score = max(0, score)
        
        # ç¡®å®šè´¨é‡çŠ¶æ€
        if score >= 80:
            status = 'PASS'
        elif score >= 60:
            status = 'WARNING'
        else:
            status = 'FAIL'
        
        quality_scores.append(score)
        quality_statuses.append(status)
        quality_flags.append(flags)
    
    # æ·»åŠ è´¨é‡è¯„ä¼°åˆ—
    df['quality_score'] = quality_scores
    df['quality_status'] = quality_statuses
    df['quality_flags'] = quality_flags
    
    return df








def save_analysis_report(analysis_response: AnalysisResponse, data_dir: str = ".") -> Dict[str, str]:
    """ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°åŸºäºæ—¶é—´æˆ³çš„å½’æ¡£æ–‡ä»¶å¤¹"""
    import datetime
    import shutil
    
    reports_path = Path(data_dir) / "reports"
    reports_path.mkdir(exist_ok=True)
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # åˆ›å»ºæ—¶é—´æˆ³æ–‡ä»¶å¤¹
    archive_folder = reports_path / timestamp
    archive_folder.mkdir(exist_ok=True)
    
    saved_files = {}
    
    try:
        # 1. ä¿å­˜JSONæ ¼å¼çš„å®Œæ•´æ•°æ®
        json_file = archive_folder / "analysis_report.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                "timestamp": timestamp,
                "analysis_summary": analysis_response.analysis_summary,
                "analysis_insights": analysis_response.analysis_insights,
                "result_files": analysis_response.result_files,
                "quality_metrics": analysis_response.quality_metrics,
                "next_steps": analysis_response.next_steps
            }, f, ensure_ascii=False, indent=2)
        saved_files["json"] = str(json_file.relative_to(Path(data_dir)))
        
        # 2. ä¿å­˜Markdownæ ¼å¼çš„å¯è¯»æŠ¥å‘Š
        md_file = archive_folder / "analysis_summary.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(f"# RNA-seq åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write(f"## åˆ†ææ€»ç»“\n\n{analysis_response.analysis_summary}\n\n")
            
            if analysis_response.analysis_insights:
                f.write(f"## åˆ†ææ´å¯Ÿ\n\n")
                for insight in analysis_response.analysis_insights:
                    f.write(f"- {insight}\n")
                f.write(f"\n")
            
            if analysis_response.result_files:
                f.write(f"## ç»“æœæ–‡ä»¶\n\n")
                for category, path in analysis_response.result_files.items():
                    f.write(f"- **{category}**: `{path}`\n")
                f.write(f"\n")
            
            if analysis_response.next_steps:
                f.write(f"## ä¸‹ä¸€æ­¥å»ºè®®\n\n")
                for step in analysis_response.next_steps:
                    f.write(f"- {step}\n")
        saved_files["markdown"] = str(md_file.relative_to(Path(data_dir)))
        
        # 3. å¤åˆ¶runtime_config.jsonåˆ°å½’æ¡£æ–‡ä»¶å¤¹
        runtime_config_source = Path(data_dir) / "config" / "runtime_config.json"
        if runtime_config_source.exists():
            runtime_config_dest = archive_folder / "runtime_config.json"
            shutil.copy2(runtime_config_source, runtime_config_dest)
            saved_files["runtime_config"] = str(runtime_config_dest.relative_to(Path(data_dir)))
        
        # 4. åˆ›å»ºæŒ‡å‘æœ€æ–°å½’æ¡£æ–‡ä»¶å¤¹çš„ç¬¦å·é“¾æ¥
        latest_link = reports_path / "latest"
        if latest_link.is_symlink() or latest_link.exists():
            latest_link.unlink()
        latest_link.symlink_to(archive_folder.name)
        saved_files["latest_folder"] = str(latest_link.relative_to(Path(data_dir)))
        
        # 5. åˆ›å»ºæ‰§è¡Œæ—¥å¿—æ–‡ä»¶å ä½ç¬¦(ä¾›åç»­ä½¿ç”¨)
        log_file = archive_folder / "execution_log.txt"
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write(f"# RNA-seq åˆ†ææ‰§è¡Œæ—¥å¿—\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"åˆ†æID: {timestamp}\n\n")
            f.write("è¯¦ç»†çš„æ‰§è¡Œæ—¥å¿—å°†åœ¨åç»­ç‰ˆæœ¬ä¸­æ·»åŠ ã€‚\n")
        saved_files["execution_log"] = str(log_file.relative_to(Path(data_dir)))
        
    except Exception as e:
        print(f"ä¿å­˜åˆ†ææŠ¥å‘Šæ—¶å‡ºé”™: {e}")
    
    return saved_files


def scan_result_files(data_dir: str = ".") -> Dict[str, Any]:
    """æ‰«æå¹¶ç»Ÿè®¡ç»“æœæ–‡ä»¶"""
    file_stats = {
        "total_files": 0,
        "file_categories": {},
        "key_files": {}
    }
    
    results_path = Path(data_dir) / "results"
    if not results_path.exists():
        return file_stats
    
    # å®šä¹‰æ–‡ä»¶ç±»åˆ«
    categories = {
        "quality_reports": [".html", ".json"],
        "alignment_files": [".bam", ".sam"],
        "quantification": [".txt", ".tsv", ".counts"],
        "logs": [".log", ".out", ".err"]
    }
    
    all_files = list(results_path.rglob("*"))
    file_stats["total_files"] = len([f for f in all_files if f.is_file()])
    
    for category, extensions in categories.items():
        category_files = []
        for ext in extensions:
            category_files.extend([f for f in all_files if f.suffix == ext])
        
        file_stats["file_categories"][category] = len(category_files)
        if category_files:
            # è®°å½•ç¬¬ä¸€ä¸ªæ–‡ä»¶ä½œä¸ºä»£è¡¨
            file_stats["key_files"][category] = str(category_files[0].relative_to(Path(data_dir)))
    
    return file_stats


async def map_analysis_to_agent_state(analysis_response: AnalysisResponse, state: AgentState) -> Dict[str, Any]:
    """å°†AnalysisResponseæ˜ å°„åˆ°å®Œæ•´çš„AgentState"""
    return {
        "messages": state.messages,
        "input": state.input,
        "response": analysis_response.analysis_summary,
        "status": "user_communication",
        "analysis_summary": analysis_response.analysis_summary,
        "analysis_insights": analysis_response.analysis_insights,
        "result_files": analysis_response.result_files,
        "quality_metrics": analysis_response.quality_metrics,
        "next_steps": analysis_response.next_steps,
        # ä¿æŒæ‰§è¡ŒçŠ¶æ€
        "execution_status": state.execution_status,
        "execution_output": state.execution_output,
        "execution_result": state.execution_result,
        "nextflow_config": state.nextflow_config,
    }


async def analysis_node(state: AgentState) -> Dict[str, Any]:
    """
    AnalysisèŠ‚ç‚¹ - åŸºäºæ ·æœ¬çº§åˆ«çš„è¡¨æ ¼åŒ–æ•°æ®åˆ†æ
    
    æ–°æ¶æ„ï¼š
    1. æ„å»ºæ ·æœ¬è´¨é‡åˆ†æDataFrame
    2. è¿›è¡Œè´¨é‡å¼‚å¸¸æ£€æµ‹å’Œè¯„ä¼°
    3. ç”Ÿæˆè¡¨æ ¼åŒ–çš„è´¨é‡æŠ¥å‘Š
    4. LLMåŸºäºè¡¨æ ¼æ•°æ®ç”Ÿæˆä¸“ä¸šåˆ†æ
    """
    
    execution_output = getattr(state, 'execution_output', '')
    execution_status = getattr(state, 'execution_status', '')
    nextflow_config = getattr(state, 'nextflow_config', {})
    
    # 1. æ„å»ºæ ·æœ¬è´¨é‡åˆ†æDataFrame
    data_dir = "."  # Dockerå®¹å™¨å†…å½“å‰ç›®å½•å°±æ˜¯/data
    
    try:
        quality_df = build_sample_quality_dataframe(data_dir)
        print(f"âœ… æ„å»ºäº†åŒ…å« {len(quality_df)} ä¸ªæ ·æœ¬çš„è´¨é‡åˆ†æè¡¨æ ¼")
        
        # 2. ç”Ÿæˆå®Œæ•´çš„èšåˆåˆ†æ
        comprehensive_summary = generate_comprehensive_summary(quality_df)
        
        # 3. ä¿ç•™åŸæœ‰çš„æ–‡ä»¶ç»Ÿè®¡ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
        file_stats = scan_result_files(data_dir)
        
        # 4. æ„å»ºæ–°çš„åˆ†æä¸Šä¸‹æ–‡
        analysis_context = {
            "tools_used": {
                "qc_tool": nextflow_config.get("qc_tool", "fastp"),
                "align_tool": nextflow_config.get("align_tool", "STAR"),
                "quant_tool": nextflow_config.get("quant_tool", "featureCounts"),
                "genome_version": nextflow_config.get("genome_version", "unknown")
            },
            "sample_count": len(quality_df),
            "comprehensive_summary": comprehensive_summary,
            "sample_quality_table": quality_df.to_dict('records') if not quality_df.empty else [],
            "file_stats": file_stats
        }
        
    except Exception as e:
        print(f"Warning: è¡¨æ ¼åŒ–åˆ†æå¤±è´¥ï¼Œå›é€€åˆ°ç©ºDataFrame: {e}")
        # å›é€€åˆ°ç©ºDataFrameï¼Œä¿æŒä¸€è‡´çš„æ•°æ®ç»“æ„
        quality_df = pd.DataFrame()
        comprehensive_summary = {"total_samples": 0}
        file_stats = scan_result_files(data_dir)
        
        analysis_context = {
            "tools_used": {
                "qc_tool": nextflow_config.get("qc_tool", "unknown"),
                "align_tool": nextflow_config.get("align_tool", "unknown"),
                "quant_tool": nextflow_config.get("quant_tool", "unknown"),
                "genome_version": nextflow_config.get("genome_version", "unknown")
            },
            "sample_count": 0,
            "comprehensive_summary": comprehensive_summary,
            "sample_quality_table": [],
            "file_stats": file_stats
        }
    
    # 5. è®©LLMåŸºäºè¡¨æ ¼åŒ–æ•°æ®ç”Ÿæˆä¸“ä¸šåˆ†æ
    agent_executor = create_analysis_agent()
    
    analysis_prompt = f"""ä½ æ˜¯RNA-seqæ•°æ®åˆ†æä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹æ ·æœ¬çº§åˆ«çš„è´¨é‡åˆ†ææ•°æ®ç”Ÿæˆä¸“ä¸šçš„åˆ†ææ€»ç»“æŠ¥å‘Šã€‚

## åˆ†æé…ç½®
{json.dumps(analysis_context["tools_used"], ensure_ascii=False, indent=2)}

## æ ·æœ¬è´¨é‡åˆ†ææ•°æ®
{json.dumps(analysis_context, ensure_ascii=False, indent=2)}

è¯·ç”Ÿæˆä¸“ä¸šçš„RNA-seqåˆ†ææŠ¥å‘Šï¼ŒåŒ…å«ï¼š

1. **analysis_summary**: åŸºäºæ ·æœ¬çº§åˆ«æ•°æ®çš„æ€»ç»“(3-4å¥è¯ï¼Œçªå‡ºå…³é”®å‘ç°)
   - æ€»ä½“æ ·æœ¬æ•°é‡å’Œè´¨é‡çŠ¶æ€åˆ†å¸ƒ
   - å…³é”®è´¨é‡æŒ‡æ ‡çš„è¡¨ç°ï¼ˆæ¯”å¯¹ç‡ã€åˆ†é…ç‡ç­‰ï¼‰
   - æ˜¯å¦å‘ç°å¼‚å¸¸æ ·æœ¬åŠå…¶ç‰¹å¾

2. **analysis_insights**: åŸºäºæ ·æœ¬æ•°æ®çš„ä¸“ä¸šæ´å¯Ÿ(æ¯æ¡åŒ…å«å…·ä½“æ•°æ®)
   - ä¾‹å¦‚ï¼š"âœ… 3ä¸ªæ ·æœ¬ä¸­æœ‰2ä¸ªè¾¾åˆ°PASSæ ‡å‡†ï¼Œå¹³å‡æ¯”å¯¹ç‡ä¸º85.2%"
   - ä¾‹å¦‚ï¼š"âš ï¸ æ ·æœ¬SRR123456çš„æ¯”å¯¹ç‡ä»…ä¸º15.3%ï¼Œå¯èƒ½å­˜åœ¨æ ·æœ¬è´¨é‡é—®é¢˜"
   - ä¾‹å¦‚ï¼š"ğŸ“Š æ‰€æœ‰æ ·æœ¬çš„åŸºå› åˆ†é…ç‡å‡è¶…è¿‡60%ï¼Œå®šé‡ç»“æœå¯é "

3. **result_files**: é‡è¦ç»“æœæ–‡ä»¶è·¯å¾„
4. **quality_metrics**: æ ·æœ¬è´¨é‡åˆ†æçš„ç»“æ„åŒ–æ•°æ®
5. **next_steps**: åŸºäºæ ·æœ¬è´¨é‡è¯„ä¼°çš„å…·ä½“å»ºè®®

è¦æ±‚ï¼š
- ä½¿ç”¨ä¸­æ–‡
- é‡ç‚¹å…³æ³¨æ ·æœ¬çº§åˆ«çš„è´¨é‡å·®å¼‚
- æ˜ç¡®æŒ‡å‡ºè´¨é‡å¼‚å¸¸çš„æ ·æœ¬
- æä¾›é’ˆå¯¹æ€§çš„æ”¹è¿›å»ºè®®
- è¾“å‡ºJSONæ ¼å¼
"""
    
    try:
        # LLMåŸºäºè¡¨æ ¼åŒ–æ•°æ®ç”Ÿæˆåˆ†æ
        messages_input = {"messages": [{"role": "user", "content": analysis_prompt}]}
        result = await agent_executor.ainvoke(messages_input)
        structured_response = result.get("structured_response")
        
        if structured_response:
            analysis_response = structured_response
        else:
            raise Exception("Agentæœªè¿”å›é¢„æœŸçš„ç»“æ„åŒ–å“åº”")
            
        # ç¡®ä¿è´¨é‡æŒ‡æ ‡è¢«ä¿ç•™
        if not analysis_response.quality_metrics:
            analysis_response.quality_metrics = analysis_context
            
    except Exception as e:
        print(f"LLMåˆ†æå¤±è´¥: {e}")
        # å¤‡ç”¨å“åº”åŸºäºè¡¨æ ¼åŒ–æ•°æ®
        if 'comprehensive_summary' in analysis_context:
            summary_text = f"å®Œæˆäº†{analysis_context['sample_count']}ä¸ªæ ·æœ¬çš„è´¨é‡åˆ†æã€‚"
            if analysis_context['comprehensive_summary'].get('fail_samples', 0) > 0:
                summary_text += f"å‘ç°{analysis_context['comprehensive_summary']['fail_samples']}ä¸ªè´¨é‡å¼‚å¸¸æ ·æœ¬ã€‚"
        else:
            summary_text = "åˆ†æå®Œæˆï¼Œä½¿ç”¨ä¼ ç»Ÿèšåˆæ–¹æ³•ã€‚"
            
        analysis_response = AnalysisResponse(
            analysis_summary=summary_text,
            analysis_insights=["å·²å®Œæˆæ ·æœ¬çº§åˆ«çš„è´¨é‡åˆ†æ"],
            result_files=analysis_context.get("file_stats", {}).get("key_files", {}),
            quality_metrics=analysis_context,
            next_steps=["æ£€æŸ¥æ ·æœ¬è´¨é‡è¡¨æ ¼è¿›è¡Œè¯¦ç»†åˆ†æ"]
        )
    
    # 6. ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°æ–‡ä»¶
    saved_files = save_analysis_report(analysis_response)
    print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {saved_files}")
    
    return await map_analysis_to_agent_state(analysis_response, state)


def generate_comprehensive_summary(df: pd.DataFrame) -> Dict[str, Any]:
    """ä»DataFrameç”Ÿæˆå®Œæ•´çš„èšåˆåˆ†æ - åŒ…æ‹¬è´¨é‡è¯„ä¼°å’Œå·¥å…·ç»Ÿè®¡"""
    if df.empty:
        return {"total_samples": 0}
    
    summary: Dict[str, Any] = {
        "total_samples": len(df),
    }
    
    # ========== è´¨é‡è¯„ä¼°èšåˆ ==========
    if 'quality_status' in df.columns:
        summary.update({
            "pass_samples": len(df[df['quality_status'] == 'PASS']),
            "warning_samples": len(df[df['quality_status'] == 'WARNING']),
            "fail_samples": len(df[df['quality_status'] == 'FAIL']),
        })
        
        # å¼‚å¸¸æ ·æœ¬è¯†åˆ«
        fail_samples = df[df['quality_status'] == 'FAIL']['sample_id'].tolist()
        warning_samples = df[df['quality_status'] == 'WARNING']['sample_id'].tolist()
        summary["fail_sample_ids"] = fail_samples
        summary["warning_sample_ids"] = warning_samples
    
    # å…³é”®æŒ‡æ ‡ç»Ÿè®¡ï¼ˆå‡å€¼ã€æœ€å€¼ï¼‰
    numeric_columns = ['uniquely_mapped_rate', 'assigned_rate', 'q30_rate', 'duplication_rate']
    for col in numeric_columns:
        if col in df.columns and df[col].notna().any():
            summary[f"{col}_mean"] = float(df[col].mean())
            summary[f"{col}_min"] = float(df[col].min())
            summary[f"{col}_max"] = float(df[col].max())
    
    # ========== å·¥å…·æ•°æ®èšåˆ ==========
    # fastpèšåˆæŒ‡æ ‡
    if 'total_reads_raw' in df.columns and 'total_reads_clean' in df.columns:
        total_raw = df['total_reads_raw'].sum()
        total_clean = df['total_reads_clean'].sum()
        
        summary["fastp_summary"] = {
            "total_before_reads": int(total_raw),
            "total_after_reads": int(total_clean),
            "filtering_rate": f"{((total_raw - total_clean) / total_raw * 100):.1f}%" if total_raw > 0 else "0%",
            "average_q30_rate": f"{df['q30_rate'].mean() * 100:.1f}%" if 'q30_rate' in df.columns else "0%",
            "average_duplication_rate": f"{df['duplication_rate'].mean() * 100:.1f}%" if 'duplication_rate' in df.columns else "0%"
        }
    
    # STARèšåˆæŒ‡æ ‡
    if 'star_input_reads' in df.columns and 'uniquely_mapped_reads' in df.columns:
        total_input = df['star_input_reads'].sum()
        total_uniquely = df['uniquely_mapped_reads'].sum()
        
        summary["star_summary"] = {
            "total_input_reads": int(total_input),
            "total_uniquely_mapped": int(total_uniquely),
            "average_mapping_rate": f"{df['uniquely_mapped_rate'].mean() * 100:.1f}%" if 'uniquely_mapped_rate' in df.columns else "0%"
        }
    
    # featureCountsèšåˆæŒ‡æ ‡
    if 'assigned_reads' in df.columns and 'fc_total_processed_reads' in df.columns:
        total_assigned = df['assigned_reads'].sum()
        total_processed = df['fc_total_processed_reads'].sum()
        
        summary["featurecounts_summary"] = {
            "total_assigned_reads": int(total_assigned),
            "total_processed_reads": int(total_processed),
            "average_assignment_rate": f"{df['assigned_rate'].mean() * 100:.1f}%" if 'assigned_rate' in df.columns else "0%"
        }
    
    return summary
