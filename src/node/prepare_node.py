import json
from typing import Dict, Any
from ..state import AgentState, PrepareResponse
from ..core import get_shared_llm

def create_prepare_agent():
    """åˆ›å»ºPrepareèŠ‚ç‚¹çš„æ™ºèƒ½é…ç½®Agent"""
    llm = get_shared_llm()
    structured_llm = llm.with_structured_output(PrepareResponse, method="json_mode")
    return structured_llm

async def prepare_node(state: AgentState) -> Dict[str, Any]:
    """å‡†å¤‡èŠ‚ç‚¹ - åŸºäºæ£€æµ‹æ•°æ®è®©LLMç›´æ¥ç”Ÿæˆé…ç½®å‚æ•°"""
    print(f"âš™ï¸ å¼€å§‹æ™ºèƒ½é…ç½®åˆ†æ...")
    
    # ç›´æ¥ä»stateè·å–æ£€æµ‹æ•°æ®
    detection_results = state.query_results or {}
    current_config = state.nextflow_config or {}
    
    if not detection_results:
        print("âš ï¸ æœªæ£€æµ‹åˆ°ä»»ä½•æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆé…ç½®")
        return {
            "nextflow_config": current_config,
            "config_reasoning": "æœªè·å–åˆ°æ£€æµ‹æ•°æ®ï¼Œä¿æŒç°æœ‰é…ç½®",
            "response": "âš ï¸ ç¼ºå°‘æ£€æµ‹æ•°æ®ï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½é…ç½®åˆ†æ",
            "status": "error"
        }
    
    # ä½¿ç”¨LLMç›´æ¥ç”Ÿæˆé…ç½®
    prepare_agent = create_prepare_agent()
    
    try:
        print("ğŸ§  LLMæ­£åœ¨åˆ†ææ£€æµ‹æ•°æ®å¹¶ç”Ÿæˆé…ç½®...")
        
        # æ„å»ºç®€å•çš„ç³»ç»Ÿæ¶ˆæ¯
        system_message = """ä½ æ˜¯RNA-seqåˆ†æé…ç½®ä¸“å®¶ã€‚åŸºäºæ£€æµ‹æ•°æ®ç›´æ¥ç”ŸæˆNextflowé…ç½®å‚æ•°å’Œåˆ†æç†ç”±ã€‚

é‡è¦ï¼šè¯·è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- nextflow_config: é…ç½®å‚æ•°å¯¹è±¡
- config_reasoning: é…ç½®ç†ç”±å­—ç¬¦ä¸²

ç¤ºä¾‹ï¼š
{
  "nextflow_config": {"genome_version": "hg38", "qc_tool": "fastp", "threads": 8},
  "config_reasoning": "åŸºäºæ£€æµ‹æ•°æ®é€‰æ‹©æ ‡å‡†å·¥å…·é“¾"
}"""
        
        # ç›´æ¥ä½¿ç”¨åŸå§‹æ£€æµ‹æ•°æ®ï¼Œä¸éœ€è¦æ ¼å¼åŒ–
        user_message = f"""è¯·åŸºäºä»¥ä¸‹æ£€æµ‹æ•°æ®ç”Ÿæˆé…ç½®å‚æ•°ï¼š

æ£€æµ‹æ•°æ® (JSONæ ¼å¼):
{json.dumps(detection_results, indent=2, ensure_ascii=False)}

å½“å‰é…ç½®: {current_config}

è¯·åˆ†ææ£€æµ‹æ•°æ®å¹¶ç”Ÿæˆåˆé€‚çš„Nextflowé…ç½®å‚æ•°ã€‚"""
        
        messages = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message}
        ]
        
        # LLMç›´æ¥è¾“å‡ºPrepareResponseæ ¼å¼
        analysis_result = prepare_agent.invoke(messages)
        
        # æ£€æŸ¥LLMå“åº”
        if not analysis_result:
            raise Exception("LLMè¿”å›ç©ºå“åº”")
        
        # æå–ç»“æœ
        config_params = analysis_result.nextflow_config or {}
        reasoning = analysis_result.config_reasoning or "åŸºäºæ£€æµ‹æ•°æ®çš„æ™ºèƒ½åˆ†æ"
        
        print(f"âœ… é…ç½®ç”Ÿæˆå®Œæˆ")
        
        # åˆå¹¶é…ç½®å‚æ•°
        final_config = current_config.copy()
        final_config.update(config_params)
        
        return {
            "nextflow_config": final_config,
            "config_reasoning": reasoning,
            "response": f"æ™ºèƒ½é…ç½®åˆ†æå®Œæˆ\n\nğŸ’¡ {reasoning}\n\nğŸ”§ ç”Ÿæˆäº† {len(config_params)} ä¸ªé…ç½®å‚æ•°",
            "status": "confirm"
        }
        
    except Exception as e:
        print(f"âŒ LLMåˆ†æå¤±è´¥: {str(e)}")
        return {
            "nextflow_config": current_config,
            "config_reasoning": f"LLMåˆ†æå¤±è´¥: {str(e)}",
            "response": f"âŒ é…ç½®ç”Ÿæˆå¤±è´¥: {str(e)}",
            "status": "error"
        }
