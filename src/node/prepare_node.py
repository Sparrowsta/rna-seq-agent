import json
from typing import Dict, Any
from ..state import AgentState, PrepareResponse
from ..core import get_shared_llm

def create_prepare_agent():
    """åˆ›å»ºPrepareèŠ‚ç‚¹çš„æ™ºèƒ½é…ç½®Agent"""
    llm = get_shared_llm()
    structured_llm = llm.with_structured_output(PrepareResponse, method="json_mode")
    return structured_llm

async def prepare_node(state: AgentState) -> Dict[str, Any]:
    """å‡†å¤‡èŠ‚ç‚¹ - ç»¼åˆç”¨æˆ·éœ€æ±‚å’Œæ£€æµ‹æ•°æ®ç”Ÿæˆé…ç½®å‚æ•°"""
    print(f"âš™ï¸ å¼€å§‹æ™ºèƒ½é…ç½®åˆ†æ...")
    
    # è·å–æ‰€æœ‰å¿…è¦ä¿¡æ¯
    detection_results = state.query_results or {}
    current_config = state.nextflow_config or {}
    user_requirements = state.user_requirements or ""
    
    if user_requirements:
        print(f"ğŸ“ ç”¨æˆ·éœ€æ±‚: {user_requirements}")
    
    if not detection_results:
        print("âš ï¸ æœªæ£€æµ‹åˆ°ä»»ä½•æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆé…ç½®")
        return {
            "nextflow_config": current_config,
            "config_reasoning": "æœªè·å–åˆ°æ£€æµ‹æ•°æ®ï¼Œä¿æŒç°æœ‰é…ç½®",
            "response": "âš ï¸ ç¼ºå°‘æ£€æµ‹æ•°æ®ï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½é…ç½®åˆ†æ",
            "status": "error"
        }
    
    # ä½¿ç”¨LLMç»¼åˆåˆ†æç”¨æˆ·éœ€æ±‚å’Œæ£€æµ‹æ•°æ®
    prepare_agent = create_prepare_agent()
    
    try:
        print("ğŸ§  LLMæ­£åœ¨åˆ†ææ£€æµ‹æ•°æ®å¹¶ç”Ÿæˆé…ç½®...")
        
        # æ„å»ºç»¼åˆåˆ†æçš„ç³»ç»Ÿæ¶ˆæ¯
        system_message = """ä½ æ˜¯RNA-seqåˆ†æé…ç½®ä¸“å®¶ã€‚ç»¼åˆç”¨æˆ·éœ€æ±‚å’Œæ£€æµ‹æ•°æ®ç”Ÿæˆæœ€ä¼˜é…ç½®ã€‚

**æ ¸å¿ƒä»»åŠ¡ï¼šæ™ºèƒ½FASTQæ–‡ä»¶é…å¯¹åˆ†æ**
ä»fastq_analysis.file_pathsä¸­çš„æ–‡ä»¶åˆ—è¡¨ï¼Œæ ¹æ®æ–‡ä»¶åæ¨¡å¼æ™ºèƒ½è¯†åˆ«ï¼š
1. æ ·æœ¬åˆ†ç»„ï¼šæå–æ ·æœ¬ID (ç§»é™¤_1/_2/_R1/_R2ç­‰åç¼€)
2. é…å¯¹å…³ç³»ï¼šåˆ¤æ–­å•ç«¯(single-end)è¿˜æ˜¯åŒç«¯(paired-end)æµ‹åº
3. ç”Ÿæˆsample_groupsç»“æ„ï¼šä¸ºæ¯ä¸ªæ ·æœ¬æŒ‡å®šread1/read2æ–‡ä»¶

å¸¸è§æ–‡ä»¶åæ¨¡å¼ï¼š
- åŒç«¯ï¼šsample_1.fastq.gz + sample_2.fastq.gz
- åŒç«¯ï¼šsample_R1.fastq.gz + sample_R2.fastq.gz  
- å•ç«¯ï¼šsample.fastq.gz (æ²¡æœ‰é…å¯¹æ–‡ä»¶)

é…ç½®å†³ç­–ä¼˜å…ˆçº§ï¼š
1. **ç”¨æˆ·æ˜ç¡®éœ€æ±‚ä¼˜å…ˆ** - å¦‚ç”¨æˆ·æŒ‡å®šåŸºå› ç»„ç‰ˆæœ¬ï¼Œå¿…é¡»æŒ‰è¦æ±‚è®¾ç½®
2. **æŠ€æœ¯å¯è¡Œæ€§è€ƒè™‘** - å¦‚æœç”¨æˆ·éœ€æ±‚çš„èµ„æºä¸å­˜åœ¨ï¼Œè¯´æ˜éœ€è¦ä¸‹è½½
3. **ç³»ç»Ÿæ¨èé»˜è®¤å€¼** - åœ¨ç”¨æˆ·æ²¡æœ‰æ˜ç¡®è¦æ±‚æ—¶ä½¿ç”¨æ£€æµ‹åˆ°çš„å¯ç”¨èµ„æº

é‡è¦é…ç½®å­—æ®µï¼š
- genome_version, species: åŸºå› ç»„ç›¸å…³é…ç½®
- qc_tool, align_tool, quant_tool: å·¥å…·é€‰æ‹©ï¼ˆå¿…é¡»ä½¿ç”¨å°å†™ï¼šfastp, star, featurecountsï¼‰  
- local_fastq_files: åŸå§‹æ–‡ä»¶è·¯å¾„åˆ—è¡¨
- paired_end: æ•´ä½“æ˜¯å¦åŒ…å«åŒç«¯æµ‹åº
- sample_groups: æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†é…å¯¹ä¿¡æ¯
- run_build_star_indexï¼šå½“STARç´¢å¼•æ²¡æœ‰å»ºç«‹çš„æ—¶å€™ï¼Œè¦å¯åŠ¨æœ¬åœ°æ„å»ºåŸºå› ç»„
- run_download_genomeï¼šå½“æ‰€éœ€çš„åŸºå› ç»„æ²¡æœ‰åœ¨æœ¬åœ°æ—¶ï¼Œéœ€è¦é€šè¿‡urlä¸‹è½½

sample_groupsæ ¼å¼ç¤ºä¾‹ï¼š
[
  {
    "sample_id": "SRR17469061",
    "read1": "fastq/SRR17469061_1.fastq.gz",
    "read2": "fastq/SRR17469061_2.fastq.gz", 
    "is_paired": true
  },
  {
    "sample_id": "sample_single",
    "read1": "fastq/sample_single.fastq.gz",
    "read2": null,
    "is_paired": false
  }
]

è¿”å›JSONæ ¼å¼ï¼š
{
  "nextflow_config": {
    "genome_version": "hg38",
    "local_fastq_files": ["file1.fastq.gz", "file2.fastq.gz"],
    "paired_end": true,
    "sample_groups": [æ ·æœ¬é…å¯¹æ•°ç»„]
  },
  "config_reasoning": "è¯¦ç»†åˆ†æè¯´æ˜"
}"""
        
        # æ„å»ºåŒ…å«æ‰€æœ‰ä¿¡æ¯çš„ç”¨æˆ·æ¶ˆæ¯
        user_message_parts = [
            "è¯·ç»¼åˆä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆæœ€ä¼˜é…ç½®ï¼š",
            "",
            "=== æ£€æµ‹æ•°æ® ===",
            json.dumps(detection_results, indent=2, ensure_ascii=False),
            "",
            f"=== å½“å‰é…ç½® ===", 
            json.dumps(current_config, indent=2, ensure_ascii=False)
        ]
        
        if user_requirements:
            user_message_parts.extend([
                "",
                f"=== ç”¨æˆ·æ˜ç¡®éœ€æ±‚ ===",
                f"**{user_requirements}**",
                "",
                "æ³¨æ„ï¼šç”¨æˆ·éœ€æ±‚åº”ä¼˜å…ˆæ»¡è¶³ï¼Œå³ä½¿æ£€æµ‹æ•°æ®æ˜¾ç¤ºç›¸å…³èµ„æºä¸å­˜åœ¨ï¼Œä¹Ÿè¦æŒ‰ç”¨æˆ·è¦æ±‚é…ç½®ï¼Œå¹¶åœ¨reasoningä¸­è¯´æ˜è§£å†³æ–¹æ¡ˆã€‚"
            ])
        
        user_message_parts.extend([
            "",
            "è¯·åˆ†ææ‰€æœ‰ä¿¡æ¯ï¼Œç”Ÿæˆæ—¢æ»¡è¶³ç”¨æˆ·éœ€æ±‚åˆè€ƒè™‘æŠ€æœ¯å¯è¡Œæ€§çš„é…ç½®å‚æ•°ã€‚"
        ])
        
        user_message = "\n".join(user_message_parts)
        
        messages = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message}
        ]
        
        # LLMç›´æ¥è¾“å‡ºPrepareResponseæ ¼å¼
        analysis_result = prepare_agent.invoke(messages)
        
        # æ£€æŸ¥LLMå“åº”
        if not analysis_result:
            raise Exception("LLMè¿”å›ç©ºå“åº”")
        
        # æå–ç»“æœ
        config_params = analysis_result.nextflow_config or {}
        reasoning = analysis_result.config_reasoning or "åŸºäºæ£€æµ‹æ•°æ®çš„æ™ºèƒ½åˆ†æ"
        
        print(f"âœ… é…ç½®ç”Ÿæˆå®Œæˆ")
        
        # åˆå¹¶é…ç½®å‚æ•°
        final_config = current_config.copy()
        final_config.update(config_params)
        
        return {
            "nextflow_config": final_config,
            "config_reasoning": reasoning,
            "response": f"æ™ºèƒ½é…ç½®åˆ†æå®Œæˆ\n\nğŸ’¡ {reasoning}\n\nğŸ”§ ç”Ÿæˆäº† {len(config_params)} ä¸ªé…ç½®å‚æ•°",
            "status": "confirm"
        }
        
    except Exception as e:
        print(f"âŒ LLMåˆ†æå¤±è´¥: {str(e)}")
        return {
            "nextflow_config": current_config,
            "config_reasoning": f"LLMåˆ†æå¤±è´¥: {str(e)}",
            "response": f"âŒ é…ç½®ç”Ÿæˆå¤±è´¥: {str(e)}",
            "status": "error"
        }
