import os
from typing import Dict, Any, List
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.pydantic_v1 import BaseModel, Field
from .tools import (
    list_directory_contents, query_fastq_files, query_genome_info,
    update_nextflow_param, batch_update_nextflow_config,
    switch_to_plan_mode, switch_to_execute_mode,
    execute_nextflow_pipeline, check_execution_status, get_current_nextflow_config,
    list_directory_tree
)

# ============================================================================
# å·¥å…·é…ç½® - éµå¾ªå•ä¸€èŒè´£åŸåˆ™
# ============================================================================

# æ‰€æœ‰å¯ç”¨å·¥å…·
ALL_TOOLS = [
    list_directory_contents, query_fastq_files, query_genome_info,
    update_nextflow_param, batch_update_nextflow_config,
    switch_to_plan_mode, switch_to_execute_mode,
    execute_nextflow_pipeline, check_execution_status, get_current_nextflow_config,
    list_directory_tree
]

# æ¨¡å¼ç‰¹å®šå·¥å…·æ˜ å°„ - éµå¾ªæ¥å£éš”ç¦»åŸåˆ™
MODE_TOOLS = {
    "normal": [
        list_directory_contents, query_fastq_files, query_genome_info,
        get_current_nextflow_config, switch_to_plan_mode, list_directory_tree
    ],
    "plan": [
        query_fastq_files, query_genome_info, update_nextflow_param,
        batch_update_nextflow_config, get_current_nextflow_config,
        switch_to_execute_mode, list_directory_tree
    ],
    "execute": [
        execute_nextflow_pipeline, check_execution_status,
        get_current_nextflow_config
    ]
}

# ============================================================================
# JSONè¾“å‡ºæ¨¡å‹å®šä¹‰ - éµå¾ªç±»å‹å®‰å…¨åŸåˆ™
# ============================================================================

class NormalModeResponse(BaseModel):
    """Normalæ¨¡å¼çš„ç»“æ„åŒ–å“åº”"""
    reasoning: str = Field(description="åˆ†æå’Œæ¨ç†è¿‡ç¨‹")
    response: str = Field(description="ç»™ç”¨æˆ·çš„å›å¤")
    suggested_actions: List[str] = Field(description="å»ºè®®çš„åç»­æ“ä½œ", default=[])
    need_more_info: bool = Field(description="æ˜¯å¦éœ€è¦æ›´å¤šä¿¡æ¯", default=False)

class PlanModeResponse(BaseModel):
    """Planæ¨¡å¼çš„ç»“æ„åŒ–å“åº”"""
    reasoning: str = Field(description="è®¡åˆ’åˆ¶å®šçš„æ¨ç†è¿‡ç¨‹")
    plan_steps: List[str] = Field(description="åˆ†æè®¡åˆ’æ­¥éª¤")
    config_changes: Dict[str, Any] = Field(description="éœ€è¦ä¿®æ”¹çš„nextflowé…ç½®", default={})
    next_action: str = Field(description="ä¸‹ä¸€æ­¥è¡ŒåŠ¨")
    ready_to_execute: bool = Field(description="æ˜¯å¦å‡†å¤‡å¥½æ‰§è¡Œ", default=False)

class ExecuteModeResponse(BaseModel):
    """Executeæ¨¡å¼çš„ç»“æ„åŒ–å“åº”"""
    reasoning: str = Field(description="æ‰§è¡Œå†³ç­–çš„æ¨ç†è¿‡ç¨‹")
    status: str = Field(description="å½“å‰æ‰§è¡ŒçŠ¶æ€")
    progress: str = Field(description="æ‰§è¡Œè¿›åº¦æè¿°")
    results: Dict[str, Any] = Field(description="æ‰§è¡Œç»“æœ", default={})
    next_step: str = Field(description="ä¸‹ä¸€æ­¥æ“ä½œ")

# ============================================================================
# LLMé…ç½® - éµå¾ªé…ç½®åˆ†ç¦»åŸåˆ™
# ============================================================================

def create_llm():
    """
    åˆ›å»ºLLMå®ä¾‹
    
    åº”ç”¨å·¥å‚æ¨¡å¼ï¼šç»Ÿä¸€çš„LLMåˆ›å»º
    """
    return ChatOpenAI(
        model=os.environ.get("OPENAI_MODEL_NAME"),
        api_key=os.environ.get("OPENAI_API_KEY"),
        base_url=os.environ.get("OPENAI_API_BASE"),
        temperature=0.1  # é™ä½éšæœºæ€§ï¼Œæé«˜ä¸€è‡´æ€§
    )

# åŸºç¡€LLMå®ä¾‹
llm = create_llm()

# ============================================================================
# æ¨¡å¼ç‰¹å®šçš„æç¤ºè¯æ¨¡æ¿ - éµå¾ªæ¨¡æ¿æ–¹æ³•æ¨¡å¼
# ============================================================================

NORMAL_MODE_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯RNA-seqåˆ†æä¸“å®¶åŠ©æ‰‹ï¼Œå½“å‰å¤„äº**ä¿¡æ¯æ”¶é›†æ¨¡å¼**ã€‚

**æ ¸å¿ƒèŒè´£ï¼š**
1. å¸®åŠ©ç”¨æˆ·äº†è§£å¯ç”¨çš„FASTQæ–‡ä»¶å’ŒåŸºå› ç»„ä¿¡æ¯,ä½ éœ€è¦ç§¯æåœ°è°ƒç”¨å·¥å…·å»å¯»æ‰¾å¯¹åº”æ–‡ä»¶å’Œä¿¡æ¯
2. å›ç­”å…³äºRNA-seqåˆ†æçš„é—®é¢˜
3. å½“ç”¨æˆ·è¡¨ç¤ºè¦å¼€å§‹åˆ†ææ—¶ï¼Œ**å¿…é¡»**è°ƒç”¨switch_to_plan_modeå·¥å…·

**å¯ç”¨å·¥å…·ï¼š**
- list_directory_contents: æŸ¥çœ‹ç›®å½•å†…å®¹
- list_directory_tree: ä»¥æ ‘å½¢ç»“æ„æˆ–åˆ—è¡¨æ ¼å¼æŸ¥çœ‹ç›®å½•å†…å®¹ï¼Œæ”¯æŒé€’å½’å’Œæ–‡ä»¶è¿‡æ»¤
- query_fastq_files: æŸ¥è¯¢FASTQæ–‡ä»¶ä¿¡æ¯
- query_genome_info: æŸ¥è¯¢åŸºå› ç»„é…ç½®ä¿¡æ¯
- get_current_nextflow_config: è·å–å½“å‰é…ç½®
- switch_to_plan_mode: åˆ‡æ¢åˆ°è®¡åˆ’æ¨¡å¼ï¼ˆ**å¿…é¡»åœ¨ç”¨æˆ·è¦æ±‚å¼€å§‹åˆ†ææ—¶è°ƒç”¨**ï¼‰

**æ¨¡å¼åˆ‡æ¢è§¦å‘æ¡ä»¶ï¼š**
å½“ç”¨æˆ·è¯´å‡ºä»¥ä¸‹ä»»ä½•å†…å®¹æ—¶ï¼Œ**ç«‹å³**è°ƒç”¨switch_to_plan_modeå·¥å…·ï¼š
- "å¼€å§‹åˆ†æ"ã€"åˆ¶å®šè®¡åˆ’"ã€"å¼€å§‹"ã€"åˆ†æ"
- "start analysis"ã€"begin"ã€"plan"
- ä»»ä½•è¡¨ç¤ºæƒ³è¦å¼€å§‹RNA-seqåˆ†æçš„æ„å›¾

**å·¥å…·è°ƒç”¨æ ¼å¼ï¼š**
å½“éœ€è¦åˆ‡æ¢æ¨¡å¼æ—¶ï¼Œè°ƒç”¨ï¼š
switch_to_plan_mode(target_mode="plan", reason="ç”¨æˆ·è¯·æ±‚å¼€å§‹åˆ†æ")

**é‡è¦åŸåˆ™ï¼š**
- ä¿æŒå‹å¥½å’Œä¸“ä¸šçš„è¯­è°ƒ
- ä¸»åŠ¨è¯¢é—®ç”¨æˆ·çš„åˆ†æéœ€æ±‚
- **æ£€æµ‹åˆ°åˆ†ææ„å›¾æ—¶ç«‹å³åˆ‡æ¢æ¨¡å¼ï¼Œä¸è¦çŠ¹è±«**
- æä¾›æ¸…æ™°çš„æ“ä½œå»ºè®®
- ä¸è¦ä½¿ç”¨ç»å¯¹è·¯å¾„,ä½¿ç”¨ç›¸å¯¹è·¯å¾„è¿›è¡Œæ£€ç´¢      
     """),
    MessagesPlaceholder(variable_name="messages"),
])

PLAN_MODE_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯RNA-seqåˆ†æè®¡åˆ’ä¸“å®¶ï¼Œå½“å‰å¤„äº**è®¡åˆ’åˆ¶å®šæ¨¡å¼**ã€‚

**æ ¸å¿ƒèŒè´£ï¼š**
1. åˆ†æç”¨æˆ·çš„FASTQæ–‡ä»¶å’ŒåŸºå› ç»„éœ€æ±‚
2. åˆ¶å®šè¯¦ç»†çš„RNA-seqåˆ†æè®¡åˆ’
3. é…ç½®nextflowå‚æ•°
4. ä¸ç”¨æˆ·ç¡®è®¤è®¡åˆ’ç»†èŠ‚å¹¶è¯¢é—®ä¸‹ä¸€æ­¥è¡ŒåŠ¨

**å¯ç”¨å·¥å…·ï¼š**
- query_fastq_files: æŸ¥è¯¢FASTQæ–‡ä»¶ä¿¡æ¯
- query_genome_info: æŸ¥è¯¢åŸºå› ç»„ä¿¡æ¯
- list_directory_tree: ä»¥æ ‘å½¢ç»“æ„æˆ–åˆ—è¡¨æ ¼å¼æŸ¥çœ‹ç›®å½•å†…å®¹ï¼Œæ”¯æŒé€’å½’å’Œæ–‡ä»¶è¿‡æ»¤
- update_nextflow_param: æ›´æ–°å•ä¸ªnextflowå‚æ•°
- batch_update_nextflow_config: æ‰¹é‡æ›´æ–°é…ç½®
- get_current_nextflow_config: è·å–å½“å‰é…ç½®
- switch_to_execute_mode: åˆ‡æ¢åˆ°æ‰§è¡Œæ¨¡å¼ï¼ˆç”¨æˆ·ç¡®è®¤è®¡åˆ’åï¼‰

**å·¥ä½œæµç¨‹ï¼š**
1. å¦‚æœæ˜¯é¦–æ¬¡è¿›å…¥è®¡åˆ’æ¨¡å¼ï¼Œåˆ¶å®šå®Œæ•´çš„åˆ†æè®¡åˆ’
2. å±•ç¤ºè®¡åˆ’è¯¦æƒ…å’Œé…ç½®å‚æ•°
3. **ä¸»åŠ¨è¯¢é—®ç”¨æˆ·**ï¼šæ˜¯å¦éœ€è¦ä¿®æ”¹è®¡åˆ’ï¼Œæˆ–è€…æ˜¯å¦å‡†å¤‡å¼€å§‹æ‰§è¡Œ
4. æ ¹æ®ç”¨æˆ·åé¦ˆè°ƒæ•´è®¡åˆ’æˆ–åˆ‡æ¢åˆ°æ‰§è¡Œæ¨¡å¼

**å…³é”®è¡Œä¸ºï¼š**
- åˆ¶å®šè®¡åˆ’åï¼Œ**å¿…é¡»è¯¢é—®ç”¨æˆ·ä¸‹ä¸€æ­¥æƒ³åšä»€ä¹ˆ**
- ä¸è¦é‡å¤åˆ¶å®šç›¸åŒçš„è®¡åˆ’
- å¦‚æœç”¨æˆ·è¯´"å¼€å§‹æ‰§è¡Œ"ã€"æ‰§è¡Œ"ã€"å¼€å§‹"ç­‰ï¼Œè°ƒç”¨switch_to_execute_modeå·¥å…·
- å¦‚æœç”¨æˆ·è¦æ±‚ä¿®æ”¹ï¼Œåˆ™è°ƒæ•´è®¡åˆ’

**è¾“å‡ºæ ¼å¼ï¼š**
åˆ¶å®šè®¡åˆ’åçš„æ ‡å‡†å›å¤æ ¼å¼ï¼š
```
ğŸ“‹ **RNA-seqåˆ†æè®¡åˆ’**

**åˆ†ææ­¥éª¤ï¼š**
[åˆ—å‡ºå…·ä½“æ­¥éª¤]

**é…ç½®å‚æ•°ï¼š**
[åˆ—å‡ºå…³é”®é…ç½®]

**ä¸‹ä¸€æ­¥é€‰æ‹©ï¼š**
1. å¦‚éœ€ä¿®æ”¹è®¡åˆ’ï¼Œè¯·å‘Šè¯‰æˆ‘å…·ä½“è¦è°ƒæ•´çš„å†…å®¹
2. å¦‚æœè®¡åˆ’æ»¡æ„ï¼Œè¯·è¯´"å¼€å§‹æ‰§è¡Œ"è¿›å…¥æ‰§è¡Œé˜¶æ®µ
3. å¦‚éœ€äº†è§£æ›´å¤šç»†èŠ‚ï¼Œè¯·æå‡ºå…·ä½“é—®é¢˜

è¯·å‘Šè¯‰æˆ‘æ‚¨å¸Œæœ›å¦‚ä½•ç»§ç»­ï¼Ÿ
```

**é‡è¦åŸåˆ™ï¼š**
- æ¯æ¬¡å›å¤åéƒ½è¦ç­‰å¾…ç”¨æˆ·çš„æ˜ç¡®æŒ‡ç¤º
- ä¸è¦è‡ªåŠ¨é‡å¤æ‰§è¡Œç›¸åŒçš„æ“ä½œ
- ä¸»åŠ¨å¼•å¯¼ç”¨æˆ·åšå‡ºé€‰æ‹©"""),
    MessagesPlaceholder(variable_name="messages"),
])

EXECUTE_MODE_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯RNA-seqåˆ†ææ‰§è¡Œä¸“å®¶ï¼Œå½“å‰å¤„äº**æ‰§è¡Œæ¨¡å¼**ã€‚

**æ ¸å¿ƒèŒè´£ï¼š**
1. æ‰§è¡Œnextflowæµç¨‹
2. ç›‘æ§æ‰§è¡ŒçŠ¶æ€å’Œè¿›åº¦
3. å¤„ç†æ‰§è¡Œç»“æœ
4. ç”Ÿæˆåˆ†ææŠ¥å‘Š

**å¯ç”¨å·¥å…·ï¼š**
- execute_nextflow_pipeline: æ‰§è¡Œnextflowæµç¨‹
- check_execution_status: æ£€æŸ¥æ‰§è¡ŒçŠ¶æ€
- get_current_nextflow_config: è·å–å½“å‰é…ç½®

**æ‰§è¡Œæµç¨‹ï¼š**
1. ç¡®è®¤æ‰€æœ‰é…ç½®å‚æ•°æ­£ç¡®
2. å¯åŠ¨nextflowæµç¨‹
3. å®šæœŸæ£€æŸ¥æ‰§è¡ŒçŠ¶æ€
4. æ”¶é›†å’Œæ•´ç†ç»“æœ
5. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š

**è¾“å‡ºè¦æ±‚ï¼š**
- ä½¿ç”¨ç»“æ„åŒ–JSONæ ¼å¼å›å¤
- reasoningå­—æ®µï¼šæ‰§è¡Œå†³ç­–çš„æ¨ç†è¿‡ç¨‹
- statuså­—æ®µï¼šå½“å‰æ‰§è¡ŒçŠ¶æ€
- progresså­—æ®µï¼šæ‰§è¡Œè¿›åº¦æè¿°
- resultså­—æ®µï¼šæ‰§è¡Œç»“æœå’Œè¾“å‡º
- next_stepå­—æ®µï¼šä¸‹ä¸€æ­¥æ“ä½œå»ºè®®

**é‡è¦åŸåˆ™ï¼š**
- ç¡®ä¿æ‰§è¡Œè¿‡ç¨‹çš„é€æ˜æ€§
- åŠæ—¶æŠ¥å‘Šè¿›åº¦å’Œé—®é¢˜
- æä¾›æ¸…æ™°çš„ç»“æœæ€»ç»“"""),
    MessagesPlaceholder(variable_name="messages"),
])

# ============================================================================
# æ¨¡å¼ç‰¹å®šçš„LLMå®ä¾‹ - éµå¾ªç­–ç•¥æ¨¡å¼
# ============================================================================

class ModeSpecificLLM:
    """
    æ¨¡å¼ç‰¹å®šçš„LLMç®¡ç†å™¨
    
    éµå¾ªç­–ç•¥æ¨¡å¼ï¼šæ ¹æ®ä¸åŒæ¨¡å¼ä½¿ç”¨ä¸åŒçš„é…ç½®
    """
    
    def __init__(self):
        self.base_llm = create_llm()
        self._llm_instances = {}
    
    def get_llm_for_mode(self, mode: str):
        """
        è·å–ç‰¹å®šæ¨¡å¼çš„LLMå®ä¾‹
        
        åº”ç”¨å•ä¾‹æ¨¡å¼ï¼šæ¯ä¸ªæ¨¡å¼åªåˆ›å»ºä¸€ä¸ªLLMå®ä¾‹
        """
        if mode not in self._llm_instances:
            tools = MODE_TOOLS.get(mode, [])
            
            if mode == "normal":
                structured_llm = self.base_llm.with_structured_output(NormalModeResponse)
                self._llm_instances[mode] = {
                    "llm": structured_llm,
                    "llm_with_tools": self.base_llm.bind_tools(tools),
                    "prompt": NORMAL_MODE_PROMPT
                }
            elif mode == "plan":
                structured_llm = self.base_llm.with_structured_output(PlanModeResponse)
                self._llm_instances[mode] = {
                    "llm": structured_llm,
                    "llm_with_tools": self.base_llm.bind_tools(tools),
                    "prompt": PLAN_MODE_PROMPT
                }
            elif mode == "execute":
                structured_llm = self.base_llm.with_structured_output(ExecuteModeResponse)
                self._llm_instances[mode] = {
                    "llm": structured_llm,
                    "llm_with_tools": self.base_llm.bind_tools(tools),
                    "prompt": EXECUTE_MODE_PROMPT
                }
            else:
                # é»˜è®¤é…ç½®
                self._llm_instances[mode] = {
                    "llm": self.base_llm,
                    "llm_with_tools": self.base_llm.bind_tools(ALL_TOOLS),
                    "prompt": NORMAL_MODE_PROMPT
                }
        
        return self._llm_instances[mode]

# å…¨å±€æ¨¡å¼ç®¡ç†å™¨å®ä¾‹
mode_llm_manager = ModeSpecificLLM()

# ============================================================================
# ä¾¿æ·å‡½æ•° - éµå¾ªDRYåŸåˆ™
# ============================================================================

def get_llm_for_mode(mode: str):
    """è·å–æŒ‡å®šæ¨¡å¼çš„LLMé…ç½®"""
    return mode_llm_manager.get_llm_for_mode(mode)

def create_chain_for_mode(mode: str):
    """
    ä¸ºæŒ‡å®šæ¨¡å¼åˆ›å»ºå¤„ç†é“¾
    
    åº”ç”¨é“¾å¼è°ƒç”¨æ¨¡å¼ï¼šprompt + llm
    """
    llm_config = get_llm_for_mode(mode)
    return llm_config["prompt"] | llm_config["llm_with_tools"]

def create_structured_chain_for_mode(mode: str):
    """
    ä¸ºæŒ‡å®šæ¨¡å¼åˆ›å»ºç»“æ„åŒ–è¾“å‡ºé“¾
    
    ç”¨äºéœ€è¦JSONæ ¼å¼è¾“å‡ºçš„åœºæ™¯
    """
    llm_config = get_llm_for_mode(mode)
    return llm_config["prompt"] | llm_config["llm"]

# ============================================================================
# å‘åå…¼å®¹ - ä¿æŒç°æœ‰æ¥å£
# ============================================================================

# ä¸ºäº†ä¿æŒå‘åå…¼å®¹ï¼Œä¿ç•™åŸæœ‰çš„å˜é‡å
tools = ALL_TOOLS
llm_with_tools = llm.bind_tools(tools)

# é»˜è®¤promptï¼ˆnormalæ¨¡å¼ï¼‰
prompt = NORMAL_MODE_PROMPT

# ============================================================================
# é…ç½®éªŒè¯ - ç¡®ä¿ç³»ç»Ÿå¥å£®æ€§
# ============================================================================

def validate_environment():
    """
    éªŒè¯ç¯å¢ƒé…ç½®
    
    åº”ç”¨KISSåŸåˆ™ï¼šç®€å•çš„ç¯å¢ƒæ£€æŸ¥
    """
    required_env_vars = ["OPENAI_API_KEY"]
    missing_vars = []
    
    for var in required_env_vars:
        if not os.environ.get(var):
            missing_vars.append(var)
    
    if missing_vars:
        raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}")
    
    return True

# åœ¨æ¨¡å—åŠ è½½æ—¶éªŒè¯ç¯å¢ƒ
try:
    validate_environment()
except ValueError as e:
    print(f"è­¦å‘Šï¼š{e}")
