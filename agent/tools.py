import os
import json
import glob
import re
import subprocess
from typing import Dict, Any, List, Optional
from pathlib import Path
from langchain_core.tools import tool
from pydantic import BaseModel, Field

# ============================================================================
# è¾“å…¥æ¨¡å‹å®šä¹‰ - éµå¾ªæ¥å£éš”ç¦»åŸåˆ™
# ============================================================================

class DirectoryQueryArgs(BaseModel):
    """ç›®å½•æŸ¥è¯¢å‚æ•°æ¨¡å‹"""
    directory_path: str = Field(description="è¦æŸ¥è¯¢çš„ç›®å½•è·¯å¾„")

class FastqQueryArgs(BaseModel):
    """FASTQæ–‡ä»¶æŸ¥è¯¢å‚æ•°æ¨¡å‹"""
    directory_path: Optional[str] = Field(default=None, description="åŒ…å«FASTQæ–‡ä»¶çš„ç›®å½•è·¯å¾„ã€‚å¦‚æœä¸æŒ‡å®šï¼Œå°†è‡ªåŠ¨æœç´¢é»˜è®¤ä½ç½®ï¼šdata/fastqå’Œdata/results/fastp")
    pattern: str = Field(default="*.fastq*", description="æ–‡ä»¶åŒ¹é…æ¨¡å¼")

class GenomeQueryArgs(BaseModel):
    """åŸºå› ç»„æŸ¥è¯¢å‚æ•°æ¨¡å‹"""
    genome_name: Optional[str] = Field(default=None, description="åŸºå› ç»„åç§°ï¼Œå¦‚hg38ã€mm39ç­‰ã€‚å¦‚æœä¸æä¾›ï¼Œå°†è¿”å›æ‰€æœ‰åŸºå› ç»„ä¿¡æ¯")
    config_path: str = Field(default="config/genomes.json", description="åŸºå› ç»„é…ç½®æ–‡ä»¶è·¯å¾„")

class NextflowConfigArgs(BaseModel):
    """Nextflowé…ç½®å‚æ•°æ¨¡å‹"""
    param_name: str = Field(description="å‚æ•°åç§°")
    param_value: Any = Field(description="å‚æ•°å€¼")

class BatchConfigArgs(BaseModel):
    """æ‰¹é‡é…ç½®æ›´æ–°å‚æ•°æ¨¡å‹"""
    config_updates: Dict[str, Any] = Field(description="è¦æ›´æ–°çš„é…ç½®å­—å…¸")

class ModeSwitch(BaseModel):
    """æ¨¡å¼åˆ‡æ¢å‚æ•°æ¨¡å‹"""
    target_mode: str = Field(description="ç›®æ ‡æ¨¡å¼ï¼šplanæˆ–execute")
    reason: str = Field(description="åˆ‡æ¢åŸå› ")

class ExecutionArgs(BaseModel):
    """æ‰§è¡Œå‚æ•°æ¨¡å‹"""
    config_path: str = Field(default="config/nextflow.config", description="nextflowé…ç½®æ–‡ä»¶è·¯å¾„")
    work_dir: str = Field(default="./work", description="å·¥ä½œç›®å½•")

class TreeListArgs(BaseModel):
    """ç›®å½•æ ‘åˆ—è¡¨å‚æ•°æ¨¡å‹"""
    directory_path: str = Field(description="è¦æŸ¥è¯¢çš„ç›®å½•è·¯å¾„")
    max_depth: Optional[int] = Field(default=None, description="æœ€å¤§é€’å½’æ·±åº¦ï¼ŒNoneè¡¨ç¤ºæ— é™åˆ¶")
    file_pattern: Optional[str] = Field(default=None, description="æ–‡ä»¶åŒ¹é…æ¨¡å¼ï¼Œå¦‚*.txt")
    show_only_files: bool = Field(default=False, description="æ˜¯å¦åªæ˜¾ç¤ºæ–‡ä»¶ï¼Œä¸æ˜¾ç¤ºç›®å½•")
    output_format: str = Field(default="tree", description="è¾“å‡ºæ ¼å¼ï¼Œå¯é€‰'tree'ï¼ˆæ ‘å½¢ç»“æ„ï¼‰æˆ–'list'ï¼ˆç®€å•åˆ—è¡¨ï¼‰")

class TaskListArgs(BaseModel):
    """ä»»åŠ¡åˆ—è¡¨ç”Ÿæˆå‚æ•°æ¨¡å‹"""
    analysis_type: str = Field(default="standard", description="åˆ†æç±»å‹ï¼šstandardï¼ˆæ ‡å‡†ï¼‰ã€minimalï¼ˆæœ€å°ï¼‰ã€comprehensiveï¼ˆå…¨é¢ï¼‰")
    force_refresh: bool = Field(default=False, description="æ˜¯å¦å¼ºåˆ¶é‡æ–°æ£€æµ‹æ–‡ä»¶å’Œé…ç½®")

# ============================================================================
# ä¿¡æ¯æŸ¥è¯¢å·¥å…·ç»„ - éµå¾ªå•ä¸€èŒè´£åŸåˆ™
# ============================================================================

@tool(args_schema=DirectoryQueryArgs)
def list_directory_contents(directory_path: str) -> str:
    """
    åˆ—å‡ºæŒ‡å®šç›®å½•çš„å†…å®¹
    
    åº”ç”¨KISSåŸåˆ™ï¼šç®€å•ç›´æ¥çš„ç›®å½•åˆ—è¡¨åŠŸèƒ½
    """
    try:
        if not os.path.exists(directory_path):
            return f"é”™è¯¯ï¼šç›®å½• '{directory_path}' ä¸å­˜åœ¨"
        
        if not os.path.isdir(directory_path):
            return f"é”™è¯¯ï¼š'{directory_path}' ä¸æ˜¯ä¸€ä¸ªç›®å½•"
        
        contents = []
        for item in os.listdir(directory_path):
            item_path = os.path.join(directory_path, item)
            if os.path.isdir(item_path):
                contents.append(f"ğŸ“ {item}/")
            else:
                size = os.path.getsize(item_path)
                contents.append(f"ğŸ“„ {item} ({size} bytes)")
        
        if not contents:
            return f"ç›®å½• '{directory_path}' ä¸ºç©º"
        
        return f"ç›®å½• '{directory_path}' å†…å®¹ï¼š\n" + "\n".join(contents)
    
    except PermissionError:
        return f"é”™è¯¯ï¼šæ²¡æœ‰æƒé™è®¿é—®ç›®å½• '{directory_path}'"
    except Exception as e:
        return f"æŸ¥è¯¢ç›®å½•æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"

@tool(args_schema=FastqQueryArgs)
def query_fastq_files(directory_path: Optional[str] = None, pattern: str = "*.fastq*") -> str:
    """
    æŸ¥è¯¢FASTQæ–‡ä»¶ä¿¡æ¯ï¼Œæ”¯æŒé»˜è®¤è·¯å¾„å’Œç”¨æˆ·æŒ‡å®šè·¯å¾„
    
    å¦‚æœä¸æŒ‡å®šdirectory_pathï¼Œå°†æœç´¢é»˜è®¤çš„FASTQå­˜å‚¨ä½ç½®ï¼š
    - data/fastq (åŸå§‹FASTQæ–‡ä»¶)
    - data/results/fastp (è´¨æ§åçš„FASTQæ–‡ä»¶)
    
    éµå¾ªDRYåŸåˆ™ï¼šç»Ÿä¸€çš„FASTQæ–‡ä»¶æŸ¥è¯¢é€»è¾‘
    """
    try:
        # å®šä¹‰é»˜è®¤æœç´¢è·¯å¾„
        default_paths = ["data/fastq", "data/results/fastp"]
        search_paths = []
        
        if directory_path:
            # ç”¨æˆ·æŒ‡å®šäº†è·¯å¾„ï¼Œåªæœç´¢æŒ‡å®šè·¯å¾„
            if not os.path.exists(directory_path):
                return f"é”™è¯¯ï¼šæŒ‡å®šçš„ç›®å½• '{directory_path}' ä¸å­˜åœ¨"
            search_paths = [directory_path]
        else:
            # ä½¿ç”¨é»˜è®¤è·¯å¾„ï¼Œåªæœç´¢å­˜åœ¨çš„è·¯å¾„
            search_paths = [path for path in default_paths if os.path.exists(path)]
            
            if not search_paths:
                return f"é”™è¯¯ï¼šé»˜è®¤FASTQç›®å½•ä¸å­˜åœ¨ã€‚è¯·æ£€æŸ¥ä»¥ä¸‹ç›®å½•ï¼š{', '.join(default_paths)}"
        
        all_fastq_files = []
        searched_paths = []
        
        # åœ¨æ‰€æœ‰æœç´¢è·¯å¾„ä¸­æŸ¥æ‰¾FASTQæ–‡ä»¶
        for search_path in search_paths:
            searched_paths.append(search_path)
            search_pattern = os.path.join(search_path, "**", pattern)
            fastq_files = glob.glob(search_pattern, recursive=True)
            all_fastq_files.extend(fastq_files)
        
        if not all_fastq_files:
            return f"åœ¨æœç´¢è·¯å¾„ {', '.join(searched_paths)} ä¸­æœªæ‰¾åˆ°åŒ¹é… '{pattern}' çš„FASTQæ–‡ä»¶"
        
        # åˆ†ææ–‡ä»¶ä¿¡æ¯ - ä¿®å¤é…å¯¹é€»è¾‘ï¼ˆé¿å…æ–‡ä»¶è¦†ç›–ï¼‰
        all_files_list = []  # å­˜å‚¨æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯çš„åˆ—è¡¨
        single_files = []
        
        def is_processed_file(file_path: str) -> bool:
            """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦ä¸ºå¤„ç†åçš„æ–‡ä»¶"""
            file_name = os.path.basename(file_path).lower()
            processed_indicators = [
                'trimmed', 'fastp', 'cutadapt', 'processed', 'clean', 
                'filtered', 'qc', 'trim', 'adapter'
            ]
            # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«å¤„ç†åçš„æ ‡è¯†
            for indicator in processed_indicators:
                if indicator in file_name:
                    return True
            
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦åŒ…å«å¤„ç†åçš„ç›®å½•æ ‡è¯†
            path_processed_indicators = [
                'fastp', 'trimmed', 'processed', 'clean', 'qc', 
                'cutadapt', 'trim', 'filter', 'results'
            ]
            for indicator in path_processed_indicators:
                if indicator in file_path.lower():
                    return True
            
            return False
        
        # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰æ–‡ä»¶å¹¶å°è¯•è§£æé…å¯¹ä¿¡æ¯
        for file_path in sorted(all_fastq_files):
            file_name = os.path.basename(file_path)
            file_size = os.path.getsize(file_path)
            
            # æ”¹è¿›çš„R1/R2æ£€æµ‹é€»è¾‘
            # æ›´å…¨é¢çš„R1/R2æ¨¡å¼åŒ¹é…
            r1_patterns = [
                r'(.+)_1\.fastq', r'(.+)_R1\.fastq', r'(.+)_r1\.fastq',
                r'(.+)_1\.fq', r'(.+)_R1\.fq', r'(.+)_r1\.fq',
                r'(.+)_1\.fastq\.gz', r'(.+)_R1\.fastq\.gz', r'(.+)_r1\.fastq\.gz',
                r'(.+)_1\.fq\.gz', r'(.+)_R1\.fq\.gz', r'(.+)_r1\.fq\.gz',
                r'(.+)_1\.trimmed\.fastq', r'(.+)_R1\.trimmed\.fastq',
                r'(.+)_1\.trimmed\.fq', r'(.+)_R1\.trimmed\.fq',
                r'(.+)\.1\.fastq', r'(.+)\.R1\.fastq', r'(.+)\.r1\.fastq'
            ]
            
            r2_patterns = [
                r'(.+)_2\.fastq', r'(.+)_R2\.fastq', r'(.+)_r2\.fastq',
                r'(.+)_2\.fq', r'(.+)_R2\.fq', r'(.+)_r2\.fq',
                r'(.+)_2\.fastq\.gz', r'(.+)_R2\.fastq\.gz', r'(.+)_r2\.fastq\.gz',
                r'(.+)_2\.fq\.gz', r'(.+)_R2\.fq\.gz', r'(.+)_r2\.fq\.gz',
                r'(.+)_2\.trimmed\.fastq', r'(.+)_R2\.trimmed\.fastq',
                r'(.+)_2\.trimmed\.fq', r'(.+)_R2\.trimmed\.fq',
                r'(.+)\.2\.fastq', r'(.+)\.R2\.fastq', r'(.+)\.r2\.fastq'
            ]
            
            sample_id = None
            read_type = None
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºR1
            for pattern in r1_patterns:
                match = re.match(pattern, file_name, re.IGNORECASE)
                if match:
                    sample_id = match.group(1)
                    read_type = "R1"
                    break
            
            # å¦‚æœä¸æ˜¯R1ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºR2
            if not sample_id:
                for pattern in r2_patterns:
                    match = re.match(pattern, file_name, re.IGNORECASE)
                    if match:
                        sample_id = match.group(1)
                        read_type = "R2"
                        break
            
            # å­˜å‚¨æ–‡ä»¶ä¿¡æ¯
            file_info = {
                "path": file_path, 
                "size": file_size, 
                "name": file_name,
                "sample_id": sample_id,
                "read_type": read_type,
                "is_processed": is_processed_file(file_path)
            }
            
            if sample_id and read_type:
                # å¯èƒ½çš„é…å¯¹æ–‡ä»¶ï¼ŒåŠ å…¥åˆ—è¡¨è€Œä¸æ˜¯è¦†ç›–
                all_files_list.append(file_info)
            else:
                # æ— æ³•è¯†åˆ«é…å¯¹ä¿¡æ¯çš„æ–‡ä»¶ï¼Œå¯èƒ½æ˜¯çœŸæ­£çš„å•ç«¯æ–‡ä»¶
                single_files.append(file_info)
        
        # ç¬¬äºŒæ­¥ï¼šä¼˜å…ˆé€‰æ‹©åŸå§‹æ–‡ä»¶è¿›è¡Œé…å¯¹ï¼Œå¦‚æœæ²¡æœ‰åŸå§‹æ–‡ä»¶å†ä½¿ç”¨å¤„ç†åæ–‡ä»¶
        paired_files = {}
        remaining_files = all_files_list.copy()
        
        # è·å–æ‰€æœ‰æ ·æœ¬ID
        sample_ids = list(set(f["sample_id"] for f in all_files_list))
        
        for sample_id in sample_ids:
            # æ‰¾åˆ°æ­¤æ ·æœ¬çš„æ‰€æœ‰æ–‡ä»¶
            sample_files = [f for f in all_files_list if f["sample_id"] == sample_id]
            
            # åˆ†åˆ«æ”¶é›†åŸå§‹å’Œå¤„ç†åçš„R1/R2æ–‡ä»¶
            original_r1 = [f for f in sample_files if f["read_type"] == "R1" and not f["is_processed"]]
            original_r2 = [f for f in sample_files if f["read_type"] == "R2" and not f["is_processed"]]
            processed_r1 = [f for f in sample_files if f["read_type"] == "R1" and f["is_processed"]]
            processed_r2 = [f for f in sample_files if f["read_type"] == "R2" and f["is_processed"]]
            
            # ä¼˜å…ˆä½¿ç”¨åŸå§‹æ–‡ä»¶é…å¯¹
            r1_file = original_r1[0] if original_r1 else (processed_r1[0] if processed_r1 else None)
            r2_file = original_r2[0] if original_r2 else (processed_r2[0] if processed_r2 else None)
            
            if r1_file and r2_file:
                # å®Œæ•´çš„åŒç«¯é…å¯¹
                paired_files[sample_id] = {"R1": r1_file, "R2": r2_file}
                # ä»remaining_filesä¸­ç§»é™¤å·²é…å¯¹çš„æ–‡ä»¶
                remaining_files = [f for f in remaining_files if f != r1_file and f != r2_file]
            
        # å‰©ä½™æœªé…å¯¹çš„æ–‡ä»¶å½’ç±»ä¸ºå•ç«¯
        single_files.extend(remaining_files)
        
        # æŒ‰æ–‡ä»¶æ¥æºåˆ†ç±»ç»“æœï¼ˆåŸºäºæ–°çš„æ•°æ®ç»“æ„ï¼‰
        original_paired = {}
        original_single = []
        processed_paired = {}
        processed_single = []
        
        # åˆ†ç±»é…å¯¹æ–‡ä»¶
        for sample_id, files in paired_files.items():
            # æ£€æŸ¥R1å’ŒR2æ–‡ä»¶æ˜¯å¦éƒ½ä¸æ˜¯å¤„ç†åçš„æ–‡ä»¶
            r1_is_original = not files['R1']['is_processed']
            r2_is_original = not files['R2']['is_processed']
            
            if r1_is_original and r2_is_original:
                original_paired[sample_id] = files
            else:
                processed_paired[sample_id] = files
        
        # åˆ†ç±»å•ç«¯æ–‡ä»¶
        for file_info in single_files:
            if not file_info['is_processed']:
                original_single.append(file_info)
            else:
                processed_single.append(file_info)
        
        # æ„å»ºç»“æœ
        result = [f"FASTQæ–‡ä»¶æŸ¥è¯¢ç»“æœï¼š"]
        result.append(f"æœç´¢è·¯å¾„: {', '.join(searched_paths)}\n")
        
        # æ˜¾ç¤ºåŸå§‹æ–‡ä»¶ï¼ˆä¸»è¦ç”¨äºåˆ†æï¼‰
        if original_paired or original_single:
            result.append("ğŸ“ **åŸå§‹FASTQæ–‡ä»¶** (ä¸»è¦ï¼šç”¨äºåˆ†æ)ï¼š")
            
            if original_paired:
                result.append("  ğŸ“ åŒç«¯æµ‹åºæ–‡ä»¶ï¼š")
                for sample_id, files in original_paired.items():
                    result.append(f"    ğŸ“¦ æ ·æœ¬: {sample_id}")
                    if "R1" in files:
                        size_mb = files['R1']['size'] / (1024*1024)
                        result.append(f"      ğŸ“„ R1: {files['R1']['path']} ({size_mb:.2f} MB)")
                    if "R2" in files:
                        size_mb = files['R2']['size'] / (1024*1024)
                        result.append(f"      ğŸ“„ R2: {files['R2']['path']} ({size_mb:.2f} MB)")
            
            if original_single:
                result.append("  ğŸ“ å•ç«¯æµ‹åºæ–‡ä»¶ï¼š")
                for file_info in original_single:
                    size_mb = file_info['size'] / (1024*1024)
                    result.append(f"    ğŸ“„ {file_info['name']}: {file_info['path']} ({size_mb:.2f} MB)")
        
        # æ˜¾ç¤ºè´¨æ§åæ–‡ä»¶ï¼ˆæ¬¡è¦ï¼Œæ˜¾ç¤ºå¤„ç†çŠ¶æ€ï¼‰
        if processed_paired or processed_single:
            result.append("\nğŸ”¬ **è´¨æ§åæ–‡ä»¶** (æ¬¡è¦ï¼šæ˜¾ç¤ºå¤„ç†çŠ¶æ€)ï¼š")
            
            if processed_paired:
                result.append("  ğŸ“ åŒç«¯æµ‹åºæ–‡ä»¶ï¼š")
                for sample_id, files in processed_paired.items():
                    result.append(f"    ğŸ“¦ æ ·æœ¬: {sample_id}")
                    if "R1" in files:
                        size_mb = files['R1']['size'] / (1024*1024)
                        result.append(f"      ğŸ“„ R1: {files['R1']['path']} ({size_mb:.2f} MB)")
                    if "R2" in files:
                        size_mb = files['R2']['size'] / (1024*1024)
                        result.append(f"      ğŸ“„ R2: {files['R2']['path']} ({size_mb:.2f} MB)")
            
            if processed_single:
                result.append("  ğŸ“ å•ç«¯æµ‹åºæ–‡ä»¶ï¼š")
                for file_info in processed_single:
                    size_mb = file_info['size'] / (1024*1024)
                    result.append(f"    ğŸ“„ {file_info['name']}: {file_info['path']} ({size_mb:.2f} MB)")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_original = len(original_paired) + len(original_single)
        total_processed = len(processed_paired) + len(processed_single)
        result.append(f"\nğŸ“Š ç»Ÿè®¡: åŸå§‹æ–‡ä»¶ {total_original} ä¸ªï¼Œè´¨æ§æ–‡ä»¶ {total_processed} ä¸ª")
        
        # ä½¿ç”¨å»ºè®®
        if original_paired or original_single:
            result.append("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
            result.append("- âœ… ä¼˜å…ˆä½¿ç”¨åŸå§‹FASTQæ–‡ä»¶è¿›è¡ŒRNA-seqåˆ†æ")
            if processed_paired or processed_single:
                result.append("- ğŸ“‹ è´¨æ§æ–‡ä»¶å¯ç”¨äºéªŒè¯æ•°æ®å¤„ç†çŠ¶æ€")
            result.append("- ğŸš€ å¦‚éœ€å¼€å§‹åˆ†ææµç¨‹ï¼Œè¯·å‘Šè¯‰æˆ‘æ‚¨çš„éœ€æ±‚")
        elif processed_paired or processed_single:
            result.append("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
            result.append("- âš ï¸  ä»…æ‰¾åˆ°è´¨æ§åæ–‡ä»¶ï¼Œå»ºè®®æ£€æŸ¥æ˜¯å¦æœ‰åŸå§‹FASTQæ–‡ä»¶")
            result.append("- ğŸ“‹ è¿™äº›æ–‡ä»¶å¯æ˜¾ç¤ºæ•°æ®å¤„ç†çŠ¶æ€")
            result.append("- ğŸš€ å¦‚éœ€å¼€å§‹åˆ†ææµç¨‹ï¼Œè¯·å‘Šè¯‰æˆ‘æ‚¨çš„éœ€æ±‚")
        
        return "\n".join(result)
    
    except Exception as e:
        return f"æŸ¥è¯¢FASTQæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"

@tool(args_schema=GenomeQueryArgs)
def query_genome_info(genome_name: Optional[str] = None, config_path: str = "config/genomes.json") -> str:
    """
    æŸ¥è¯¢åŸºå› ç»„é…ç½®ä¿¡æ¯å¹¶åŠ¨æ€æ£€æŸ¥æ–‡ä»¶ç³»ç»ŸçŠ¶æ€ï¼Œè‡ªåŠ¨åŒæ­¥é…ç½®
    
    éµå¾ªå•ä¸€èŒè´£åŸåˆ™ï¼šä¸“é—¨å¤„ç†åŸºå› ç»„ä¿¡æ¯æŸ¥è¯¢å’Œé…ç½®åŒæ­¥
    å½“ä¸æä¾›genome_nameæ—¶ï¼Œè¿”å›æ‰€æœ‰åŸºå› ç»„çš„æ‘˜è¦ä¿¡æ¯
    å½“æä¾›genome_nameæ—¶ï¼Œè¿”å›ç‰¹å®šåŸºå› ç»„çš„è¯¦ç»†ä¿¡æ¯
    è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ–°åŸºå› ç»„å¹¶æ›´æ–°é…ç½®
    """
    try:
        if not os.path.exists(config_path):
            return f"é”™è¯¯ï¼šåŸºå› ç»„é…ç½®æ–‡ä»¶ '{config_path}' ä¸å­˜åœ¨"
        
        with open(config_path, 'r', encoding='utf-8') as f:
            genomes_config = json.load(f)
        
        # å®æ—¶æ›´æ–°é…ç½®çŠ¶æ€ï¼šæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¹¶æ›´æ–°é…ç½®
        for name, info in genomes_config.items():
            # ç»Ÿä¸€å¤„ç†fastaå’Œgtfè·¯å¾„ï¼ˆå…¼å®¹ä¸åŒå‘½åï¼‰
            fasta_path = info.get('fasta', info.get('fasta_path', ''))
            gtf_path = info.get('gtf', info.get('gtf_path', ''))
            
            # æ›´æ–°å®é™…å­˜åœ¨çŠ¶æ€
            if fasta_path:
                info['fasta_exists'] = os.path.exists(fasta_path)
            if gtf_path:
                info['gtf_exists'] = os.path.exists(gtf_path)
            
            # æ£€æŸ¥STARç´¢å¼•
            if fasta_path and os.path.exists(fasta_path):
                star_index_dir = os.path.join(os.path.dirname(fasta_path), "star_index")
                info['star_index_exists'] = (os.path.exists(star_index_dir) and 
                                            os.path.exists(os.path.join(star_index_dir, "SA")))
            else:
                info['star_index_exists'] = False
        
        # å¦‚æœæ²¡æœ‰æä¾›genome_nameï¼Œè¿”å›æ‰€æœ‰åŸºå› ç»„çš„æ‘˜è¦ä¿¡æ¯
        if genome_name is None:
            result = ["å¯ç”¨åŸºå› ç»„è¯¦ç»†çŠ¶æ€ä¿¡æ¯ï¼š"]
            result.append("=" * 80)
            
            for name, info in genomes_config.items():
                result.append(f"ğŸ“Š åŸºå› ç»„: {name}")
                result.append(f"   ç‰©ç§: {info.get('species', 'æœªçŸ¥')}")
                result.append(f"   ç‰ˆæœ¬: {info.get('version', 'æœªçŸ¥')}")
                
                # æ£€æŸ¥æœ¬åœ°æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                fasta_path = info.get('fasta', info.get('fasta_path', ''))
                gtf_path = info.get('gtf', info.get('gtf_path', ''))
                
                # æ£€æŸ¥FASTAæ–‡ä»¶çŠ¶æ€
                if fasta_path and info.get('fasta_exists', False):
                    file_size = os.path.getsize(fasta_path) / (1024**3)  # GB
                    fasta_status = f"âœ… å·²ä¸‹è½½ ({file_size:.2f} GB)"
                elif fasta_path:
                    fasta_status = "âŒ æœªä¸‹è½½ (é…ç½®å·²è®¾ç½®)"
                else:
                    fasta_status = "âš ï¸  æœªé…ç½®"
                
                # æ£€æŸ¥GTFæ–‡ä»¶çŠ¶æ€
                if gtf_path and info.get('gtf_exists', False):
                    file_size = os.path.getsize(gtf_path) / (1024**2)  # MB
                    gtf_status = f"âœ… å·²ä¸‹è½½ ({file_size:.2f} MB)"
                elif gtf_path:
                    gtf_status = "âŒ æœªä¸‹è½½ (é…ç½®å·²è®¾ç½®)"
                else:
                    gtf_status = "âš ï¸  æœªé…ç½®"
                
                # æ£€æŸ¥STARç´¢å¼•çŠ¶æ€
                if info.get('star_index_exists', False):
                    index_status = "âœ… å·²å»ºç«‹ç´¢å¼•"
                elif fasta_path:
                    index_status = "âŒ æœªå»ºç«‹ç´¢å¼•"
                else:
                    index_status = "âš ï¸  æ— æ³•æ£€æŸ¥ (FASTAæœªé…ç½®)"
                
                result.append(f"   ğŸ“ FASTAæ–‡ä»¶: {fasta_status}")
                result.append(f"   ğŸ“„ GTFæ–‡ä»¶: {gtf_status}")
                result.append(f"   ğŸ” STARç´¢å¼•: {index_status}")
                
                # æ˜¾ç¤ºæ–‡ä»¶è·¯å¾„
                if fasta_path:
                    result.append(f"   ğŸ“‚ FASTAè·¯å¾„: {fasta_path}")
                if gtf_path:
                    result.append(f"   ğŸ“‚ GTFè·¯å¾„: {gtf_path}")
                
                # æ˜¾ç¤ºä¸‹è½½URLï¼ˆå¦‚æœæœ‰ï¼‰
                if 'fasta_url' in info:
                    result.append(f"   ğŸ”— FASTA URL: {info['fasta_url']}")
                if 'gtf_url' in info:
                    result.append(f"   ğŸ”— GTF URL: {info['gtf_url']}")
                
                result.append("-" * 80)
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_genomes = len(genomes_config)
            downloaded_genomes = sum(1 for info in genomes_config.values() 
                                   if info.get('fasta_exists', False) and info.get('gtf_exists', False))
            indexed_genomes = sum(1 for info in genomes_config.values() 
                                if info.get('star_index_exists', False))
            
            result.append(f"ğŸ“ˆ ç»Ÿè®¡æ‘˜è¦ï¼š")
            result.append(f"   æ€»åŸºå› ç»„æ•°é‡: {total_genomes}")
            result.append(f"   å·²å®Œå…¨ä¸‹è½½: {downloaded_genomes}")
            result.append(f"   å·²å»ºç«‹ç´¢å¼•: {indexed_genomes}")
            result.append(f"   å‡†å¤‡å°±ç»ªç‡: {(indexed_genomes/total_genomes*100):.1f}%" if total_genomes > 0 else "   å‡†å¤‡å°±ç»ªç‡: 0.0%")
            
            return "\n".join(result)
        
        # å¦‚æœæä¾›äº†genome_nameï¼Œè¿”å›ç‰¹å®šåŸºå› ç»„çš„è¯¦ç»†ä¿¡æ¯
        if genome_name not in genomes_config:
            available_genomes = list(genomes_config.keys())
            return f"é”™è¯¯ï¼šåŸºå› ç»„ '{genome_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨åŸºå› ç»„ï¼š{', '.join(available_genomes)}"
        
        genome_info = genomes_config[genome_name]
        
        result = [f"åŸºå› ç»„ '{genome_name}' è¯¦ç»†ä¿¡æ¯ï¼š"]
        result.append(f"  ç‰©ç§: {genome_info.get('species', 'æœªçŸ¥')}")
        result.append(f"  ç‰ˆæœ¬: {genome_info.get('version', 'æœªçŸ¥')}")
        result.append(f"  FASTAæ–‡ä»¶: {genome_info.get('fasta', 'æœªé…ç½®')}")
        result.append(f"  GTFæ–‡ä»¶: {genome_info.get('gtf', 'æœªé…ç½®')}")
        
        if 'fasta_url' in genome_info:
            result.append(f"  FASTAä¸‹è½½URL: {genome_info['fasta_url']}")
        if 'gtf_url' in genome_info:
            result.append(f"  GTFä¸‹è½½URL: {genome_info['gtf_url']}")
        
        # æ£€æŸ¥æœ¬åœ°æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        fasta_path = genome_info.get('fasta', '')
        gtf_path = genome_info.get('gtf', '')
        
        if fasta_path and os.path.exists(fasta_path):
            result.append(f"  âœ… FASTAæ–‡ä»¶å·²å­˜åœ¨")
        elif fasta_path:
            result.append(f"  âŒ FASTAæ–‡ä»¶ä¸å­˜åœ¨")
        
        if gtf_path and os.path.exists(gtf_path):
            result.append(f"  âœ… GTFæ–‡ä»¶å·²å­˜åœ¨")
        elif gtf_path:
            result.append(f"  âŒ GTFæ–‡ä»¶ä¸å­˜åœ¨")
        
        return "\n".join(result)
    
    except json.JSONDecodeError:
        return f"é”™è¯¯ï¼šåŸºå› ç»„é…ç½®æ–‡ä»¶ '{config_path}' æ ¼å¼ä¸æ­£ç¡®"
    except Exception as e:
        return f"æŸ¥è¯¢åŸºå› ç»„ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"

# ============================================================================ 
# é…ç½®ç®¡ç†å·¥å…·ç»„ - éµå¾ªDRYåŸåˆ™
# ============================================================================ 
class AddGenomeArgs(BaseModel):
    """åŸºå› ç»„æ·»åŠ å‚æ•°æ¨¡å‹"""
    genome_name: str = Field(description="è¦æ·»åŠ çš„åŸºå› ç»„çš„å”¯ä¸€åç§°ï¼Œä¾‹å¦‚ 'danRer11'")
    species: str = Field(description="è¯¥åŸºå› ç»„æ‰€å±çš„ç‰©ç§ï¼Œä¾‹å¦‚ 'zebrafish'")
    fasta_url: str = Field(description="åŸºå› ç»„FASTAæ–‡ä»¶çš„URL")
    gtf_url: str = Field(description="åŸºå› ç»„GTFæ–‡ä»¶çš„URL")
    fasta_path: str = Field(description="FASTAæ–‡ä»¶çš„æœ¬åœ°å­˜å‚¨è·¯å¾„")
    gtf_path: str = Field(description="GTFæ–‡ä»¶çš„æœ¬åœ°å­˜å‚¨è·¯å¾„")

@tool(args_schema=AddGenomeArgs)
def add_new_genome(genome_name: str, species: str, fasta_url: str, gtf_url: str, fasta_path: str, gtf_path: str) -> str:
    """
    æ·»åŠ ä¸€ä¸ªå…¨æ–°çš„åŸºå› ç»„åˆ°é…ç½®æ–‡ä»¶(config/genomes.json)ï¼Œå¹¶éªŒè¯æ–‡ä»¶è·¯å¾„
    è‡ªåŠ¨æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œæä¾›çŠ¶æ€åé¦ˆ
    """
    try:
        config_path = "config/genomes.json"
        if not os.path.exists(config_path):
            return f"é”™è¯¯ï¼šåŸºå› ç»„é…ç½®æ–‡ä»¶ '{config_path}' ä¸å­˜åœ¨"

        with open(config_path, 'r+', encoding='utf-8') as f:
            genomes_config = json.load(f)

            if genome_name in genomes_config:
                return f"é”™è¯¯ï¼šåŸºå› ç»„ '{genome_name}' å·²å­˜åœ¨äºé…ç½®ä¸­ã€‚"

            # éªŒè¯æ–‡ä»¶è·¯å¾„å’Œå­˜åœ¨çŠ¶æ€
            fasta_exists = os.path.exists(fasta_path) if fasta_path else False
            gtf_exists = os.path.exists(gtf_path) if gtf_path else False
            
            # æ£€æŸ¥STARç´¢å¼•çŠ¶æ€
            star_index_exists = False
            if fasta_exists:
                star_index_dir = os.path.join(os.path.dirname(fasta_path), "star_index")
                star_index_exists = (os.path.exists(star_index_dir) and 
                                   os.path.exists(os.path.join(star_index_dir, "SA")))

            new_genome_entry = {
                "species": species,
                "version": genome_name,
                "fasta": fasta_path,
                "gtf": gtf_path,
                "fasta_url": fasta_url,
                "gtf_url": gtf_url,
                "fasta_exists": fasta_exists,
                "gtf_exists": gtf_exists,
                "star_index_exists": star_index_exists
            }

            genomes_config[genome_name] = new_genome_entry
            
            f.seek(0)
            json.dump(genomes_config, f, indent=2)
            f.truncate()

        # æ„å»ºçŠ¶æ€æŠ¥å‘Š
        status_report = [f"âœ… æˆåŠŸæ·»åŠ åŸºå› ç»„ '{genome_name}' (ç‰©ç§: {species}) åˆ° '{config_path}'ã€‚"]
        status_report.append("\nğŸ“Š æ–‡ä»¶çŠ¶æ€:")
        
        if fasta_exists:
            file_size = os.path.getsize(fasta_path) / (1024**3)  # GB
            status_report.append(f"   ğŸ“ FASTAæ–‡ä»¶: âœ… å·²å­˜åœ¨ ({file_size:.2f} GB)")
        else:
            status_report.append(f"   ğŸ“ FASTAæ–‡ä»¶: âŒ ä¸å­˜åœ¨ - éœ€è¦ä»URLä¸‹è½½")
            
        if gtf_exists:
            file_size = os.path.getsize(gtf_path) / (1024**2)  # MB
            status_report.append(f"   ğŸ“„ GTFæ–‡ä»¶: âœ… å·²å­˜åœ¨ ({file_size:.2f} MB)")
        else:
            status_report.append(f"   ğŸ“„ GTFæ–‡ä»¶: âŒ ä¸å­˜åœ¨ - éœ€è¦ä»URLä¸‹è½½")
            
        if star_index_exists:
            status_report.append(f"   ğŸ” STARç´¢å¼•: âœ… å·²å»ºç«‹")
        else:
            status_report.append(f"   ğŸ” STARç´¢å¼•: âŒ éœ€è¦æ„å»º")

        status_report.append(f"\nğŸ”— ä¸‹è½½é“¾æ¥:")
        status_report.append(f"   FASTA: {fasta_url}")
        status_report.append(f"   GTF: {gtf_url}")

        return "\n".join(status_report)

    except json.JSONDecodeError:
        return f"é”™è¯¯ï¼šåŸºå› ç»„é…ç½®æ–‡ä»¶ '{config_path}' æ ¼å¼ä¸æ­£ç¡®"
    except Exception as e:
        return f"æ·»åŠ æ–°åŸºå› ç»„æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"

# ============================================================================ 
# é…ç½®ç®¡ç†å·¥å…·ç»„ - éµå¾ªDRYåŸåˆ™
# ============================================================================ 


@tool(args_schema=NextflowConfigArgs)
def update_nextflow_param(param_name: str, param_value: Any) -> str:
    """
    æ›´æ–°å•ä¸ªnextflowå‚æ•°åˆ°AgentState
    
    è¿”å›ç‰¹æ®Šæ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œä¾›èŠ‚ç‚¹è§£æå¹¶æ›´æ–°çŠ¶æ€
    """
    try:
        # éªŒè¯å‚æ•°åç§°
        valid_params = [
            "srr_ids", "local_genome_path", "local_gtf_path", 
            "download_genome_url", "download_gtf_url", "local_fastq_files",
            "genome_version", "data", "run_download_srr", "run_download_genome", 
            "run_build_star_index", "run_fastp", "run_star_align", "run_featurecounts"
        ]
        
        if param_name not in valid_params:
            return f"âŒ é”™è¯¯ï¼šæ— æ•ˆçš„å‚æ•°å '{param_name}'\næœ‰æ•ˆå‚æ•°ï¼š{', '.join(valid_params)}"
        
        # ç±»å‹éªŒè¯å’Œè½¬æ¢
        if param_name.startswith("run_"):
            if isinstance(param_value, str):
                param_value = param_value.lower() in ['true', '1', 'yes', 'on']
            elif not isinstance(param_value, bool):
                return f"âŒ é”™è¯¯ï¼šå‚æ•° '{param_name}' å¿…é¡»æ˜¯å¸ƒå°”å€¼"
        
        # è¿”å›ç‰¹æ®Šæ ¼å¼ï¼ŒåŒ…å«é…ç½®æ›´æ–°æŒ‡ä»¤
        import json
        config_update = {param_name: param_value}
        
        return f"âœ… å‚æ•° '{param_name}' å·²è®¾ç½®ä¸º: {param_value}\n[CONFIG_UPDATE] {json.dumps(config_update, ensure_ascii=False)}"
    
    except Exception as e:
        return f"âŒ æ›´æ–°å‚æ•°æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"

@tool(args_schema=BatchConfigArgs)
def batch_update_nextflow_config(config_updates: Dict[str, Any]) -> str:
    """
    æ‰¹é‡æ›´æ–°nextflowé…ç½®
    
    éµå¾ªDRYåŸåˆ™ï¼šå¤ç”¨å•å‚æ•°æ›´æ–°é€»è¾‘
    """
    try:
        # éªŒè¯æ‰€æœ‰å‚æ•°
        valid_params = [
            "srr_ids", "local_genome_path", "local_gtf_path", 
            "download_genome_url", "download_gtf_url", "local_fastq_files",
            "genome_version", "data", "run_download_srr", "run_download_genome", 
            "run_build_star_index", "run_fastp", "run_star_align", "run_featurecounts"
        ]
        
        validated_updates = {}
        errors = []
        
        for param_name, param_value in config_updates.items():
            if param_name not in valid_params:
                errors.append(f"é”™è¯¯ï¼šæ— æ•ˆçš„å‚æ•°å '{param_name}'")
                continue
                
            # ç±»å‹éªŒè¯å’Œè½¬æ¢
            if param_name.startswith("run_"):
                if isinstance(param_value, str):
                    param_value = param_value.lower() in ['true', '1', 'yes', 'on']
                elif not isinstance(param_value, bool):
                    errors.append(f"é”™è¯¯ï¼šå‚æ•° '{param_name}' å¿…é¡»æ˜¯å¸ƒå°”å€¼")
                    continue
            
            validated_updates[param_name] = param_value
        
        if errors:
            return "æ‰¹é‡æ›´æ–°å¤±è´¥:\n" + "\n".join(errors)
        
        # è¿”å›ç‰¹æ®Šæ ¼å¼ï¼ŒåŒ…å«æ‰¹é‡é…ç½®æ›´æ–°æŒ‡ä»¤
        import json
        config_json = json.dumps(validated_updates, ensure_ascii=False)
        
        success_params = list(validated_updates.keys())
        summary = f"âœ… æ‰¹é‡æ›´æ–°å®Œæˆï¼š{len(success_params)} ä¸ªå‚æ•°å·²æ›´æ–°\n"
        summary += f"æ›´æ–°çš„å‚æ•°ï¼š{', '.join(success_params)}"
        
        return f"{summary}\n__CONFIG_UPDATE__:{config_json}"
    
    except Exception as e:
        return f"æ‰¹é‡æ›´æ–°é…ç½®æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"

# ============================================================================
# æ¨¡å¼æ§åˆ¶å·¥å…·ç»„ - éµå¾ªå¼€æ”¾å°é—­åŸåˆ™
# ============================================================================

@tool(args_schema=ModeSwitch)
def switch_to_plan_mode(target_mode: str, reason: str) -> str:
    """
    åˆ‡æ¢åˆ°è®¡åˆ’æ¨¡å¼
    
    éµå¾ªå•ä¸€èŒè´£åŸåˆ™ï¼šä¸“é—¨å¤„ç†æ¨¡å¼åˆ‡æ¢
    """
    if target_mode != "plan":
        return f"é”™è¯¯ï¼šæ­¤å·¥å…·åªèƒ½åˆ‡æ¢åˆ°planæ¨¡å¼ï¼Œæ”¶åˆ°ï¼š{target_mode}"
    
    return f"ğŸ”„ æ­£åœ¨åˆ‡æ¢åˆ°è®¡åˆ’æ¨¡å¼...\nåŸå› ï¼š{reason}\nâœ… æ¨¡å¼åˆ‡æ¢æˆåŠŸï¼ç°åœ¨å¯ä»¥å¼€å§‹åˆ¶å®šRNA-seqåˆ†æè®¡åˆ’ã€‚"

@tool(args_schema=ModeSwitch)
def switch_to_execute_mode(target_mode: str, reason: str) -> str:
    """
    åˆ‡æ¢åˆ°æ‰§è¡Œæ¨¡å¼
    
    éµå¾ªå•ä¸€èŒè´£åŸåˆ™ï¼šä¸“é—¨å¤„ç†æ¨¡å¼åˆ‡æ¢
    """
    if target_mode != "execute":
        return f"é”™è¯¯ï¼šæ­¤å·¥å…·åªèƒ½åˆ‡æ¢åˆ°executeæ¨¡å¼ï¼Œæ”¶åˆ°ï¼š{target_mode}"
    
    return f"ğŸ”„ æ­£åœ¨åˆ‡æ¢åˆ°æ‰§è¡Œæ¨¡å¼...\nåŸå› ï¼š{reason}\nâœ… æ¨¡å¼åˆ‡æ¢æˆåŠŸï¼å‡†å¤‡æ‰§è¡Œnextflowæµç¨‹ã€‚"

# ============================================================================
# æ‰§è¡Œæ§åˆ¶å·¥å…·ç»„ - éµå¾ªå•ä¸€èŒè´£åŸåˆ™
# ============================================================================

@tool(args_schema=ExecutionArgs)
def execute_nextflow_pipeline(config_path: str = "config/nextflow.config", work_dir: str = "./work") -> str:
    """
    æ‰§è¡Œnextflowæµç¨‹
    
    åº”ç”¨KISSåŸåˆ™ï¼šç®€å•çš„æµç¨‹æ‰§è¡Œ
    """
    try:
        # æ£€æŸ¥nextflowæ˜¯å¦å¯ç”¨
        result = subprocess.run(["nextflow", "-version"], capture_output=True, text=True)
        if result.returncode != 0:
            return "é”™è¯¯ï¼šnextflowæœªå®‰è£…æˆ–ä¸å¯ç”¨"
        
        # æ„å»ºæ‰§è¡Œå‘½ä»¤
        cmd = [
            "nextflow", "run", "main.nf",
            "-c", config_path,
            "-work-dir", work_dir
        ]
        
        return f"ğŸš€ å¼€å§‹æ‰§è¡Œnextflowæµç¨‹...\nå‘½ä»¤ï¼š{' '.join(cmd)}\nâ³ æµç¨‹æ­£åœ¨åå°è¿è¡Œï¼Œè¯·ä½¿ç”¨check_execution_statusæŸ¥çœ‹çŠ¶æ€ã€‚"
    
    except Exception as e:
        return f"æ‰§è¡Œnextflowæµç¨‹æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"

@tool
def check_execution_status() -> str:
    """
    æ£€æŸ¥æ‰§è¡ŒçŠ¶æ€
    
    åº”ç”¨KISSåŸåˆ™ï¼šç®€å•çš„çŠ¶æ€æ£€æŸ¥
    """
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰nextflowè¿›ç¨‹åœ¨è¿è¡Œ
        result = subprocess.run(["pgrep", "-f", "nextflow"], capture_output=True, text=True)
        
        if result.returncode == 0:
            return "âœ… Nextflowæµç¨‹æ­£åœ¨è¿è¡Œä¸­..."
        else:
            return "â¹ï¸ æ²¡æœ‰æ£€æµ‹åˆ°æ­£åœ¨è¿è¡Œçš„nextflowæµç¨‹"
    
    except Exception as e:
        return f"æ£€æŸ¥æ‰§è¡ŒçŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"

@tool
def get_current_nextflow_config() -> str:
    """
    è·å–å½“å‰nextflowé…ç½®çŠ¶æ€
    
    éµå¾ªå•ä¸€èŒè´£åŸåˆ™ï¼šä¸“é—¨è·å–é…ç½®ä¿¡æ¯
    """
    try:
        # è¿™é‡Œåº”è¯¥ä»çŠ¶æ€ç®¡ç†ä¸­è·å–é…ç½®ï¼Œæš‚æ—¶è¿”å›é»˜è®¤é…ç½®
        default_config = {
            "srr_ids": "",
            "local_genome_path": "",
            "local_gtf_path": "",
            "download_genome_url": "",
            "download_gtf_url": "",
            "local_fastq_files": "",
            "data": "./data",
            "run_download_srr": False,
            "run_download_genome": False,
            "run_build_star_index": False,
            "run_fastp": False,
            "run_star_align": False,
            "run_featurecounts": False
        }
        
        result = ["å½“å‰nextflowé…ç½®ï¼š"]
        for key, value in default_config.items():
            result.append(f"  {key}: {value}")
        
        return "\n".join(result)
    
    except Exception as e:
        return f"è·å–é…ç½®æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"

# ============================================================================
# ç›®å½•æ ‘å·¥å…·ç»„ - éµå¾ªå•ä¸€èŒè´£åŸåˆ™
# ============================================================================

def _generate_simple_list(directory_path: str, max_depth: Optional[int] = None,
                         file_pattern: Optional[str] = None, show_only_files: bool = False) -> str:
    """
    ç”Ÿæˆç®€å•åˆ—è¡¨æ ¼å¼çš„ç›®å½•å†…å®¹
    
    éµå¾ªå•ä¸€èŒè´£åŸåˆ™ï¼šä¸“é—¨å¤„ç†ç®€å•åˆ—è¡¨æ ¼å¼è¾“å‡º
    """
    try:
        path = Path(directory_path)
        if not path.exists():
            return f"é”™è¯¯ï¼šç›®å½• '{directory_path}' ä¸å­˜åœ¨"
        
        if not path.is_dir():
            return f"é”™è¯¯ï¼š'{directory_path}' ä¸æ˜¯ä¸€ä¸ªç›®å½•"
        
        contents = []
        
        # æ ¹æ®æœ€å¤§æ·±åº¦å†³å®šä½¿ç”¨rglobè¿˜æ˜¯glob
        if max_depth is None or max_depth > 1:
            # ä½¿ç”¨rglobè¿›è¡Œé€’å½’æŸ¥æ‰¾
            pattern = file_pattern if file_pattern else "*"
            for item in path.rglob(pattern):
                if max_depth is not None:
                    # è®¡ç®—ç›¸å¯¹æ·±åº¦
                    relative_path = item.relative_to(path)
                    depth = len(relative_path.parts) - 1
                    if depth > max_depth:
                        continue
                
                if show_only_files and item.is_dir():
                    continue
                
                if item.is_dir():
                    contents.append(f"ğŸ“ {item}/")
                else:
                    size = item.stat().st_size
                    contents.append(f"ğŸ“„ {item} ({size} bytes)")
        else:
            # åªæŸ¥æ‰¾å½“å‰ç›®å½•
            pattern = file_pattern if file_pattern else "*"
            for item in path.glob(pattern):
                if show_only_files and item.is_dir():
                    continue
                
                if item.is_dir():
                    contents.append(f"ğŸ“ {item}/")
                else:
                    size = item.stat().st_size
                    contents.append(f"ğŸ“„ {item} ({size} bytes)")
        
        if not contents:
            return f"ç›®å½• '{directory_path}' ä¸­æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å†…å®¹"
        
        return f"ç›®å½• '{directory_path}' å†…å®¹åˆ—è¡¨ï¼š\n" + "\n".join(contents)
    
    except Exception as e:
        return f"ç”Ÿæˆç®€å•åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"

def _generate_tree_structure(directory_path: str, max_depth: Optional[int] = None,
                           file_pattern: Optional[str] = None, show_only_files: bool = False,
                           prefix: str = "", current_depth: int = 0) -> str:
    """
    ç”Ÿæˆæ ‘å½¢ç»“æ„çš„ç›®å½•å†…å®¹
    
    éµå¾ªé€’å½’è®¾è®¡æ¨¡å¼ï¼šä½¿ç”¨é€’å½’æ„å»ºæ ‘å½¢ç»“æ„
    """
    try:
        path = Path(directory_path)
        if not path.exists():
            return f"é”™è¯¯ï¼šç›®å½• '{directory_path}' ä¸å­˜åœ¨"
        
        if not path.is_dir():
            return f"é”™è¯¯ï¼š'{directory_path}' ä¸æ˜¯ä¸€ä¸ªç›®å½•"
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§æ·±åº¦
        if max_depth is not None and current_depth > max_depth:
            return ""
        
        result = []
        
        try:
            items = sorted(path.iterdir(), key=lambda x: (x.is_file(), x.name.lower()))
        except PermissionError:
            return f"{prefix}âŒ æ²¡æœ‰æƒé™è®¿é—®æ­¤ç›®å½•"
        
        # è¿‡æ»¤æ–‡ä»¶
        if file_pattern:
            import fnmatch
            items = [item for item in items if item.is_dir() or fnmatch.fnmatch(item.name, file_pattern)]
        
        for i, item in enumerate(items):
            is_last = i == len(items) - 1
            
            # è·³è¿‡ç›®å½•ï¼ˆå¦‚æœåªæ˜¾ç¤ºæ–‡ä»¶ï¼‰
            if show_only_files and item.is_dir():
                continue
            
            # æ·»åŠ å½“å‰é¡¹
            if item.is_dir():
                connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
                result.append(f"{prefix}{connector}ğŸ“ {item.name}/")
                
                # é€’å½’å¤„ç†å­ç›®å½•
                if max_depth is None or current_depth < max_depth:
                    extension = "    " if is_last else "â”‚   "
                    subtree = _generate_tree_structure(
                        str(item), max_depth, file_pattern, show_only_files,
                        prefix + extension, current_depth + 1
                    )
                    if subtree:
                        result.append(subtree)
            else:
                connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
                size = item.stat().st_size
                result.append(f"{prefix}{connector}ğŸ“„ {item.name} ({size} bytes)")
        
        return "\n".join(result)
    
    except Exception as e:
        return f"ç”Ÿæˆæ ‘å½¢ç»“æ„æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"

@tool(args_schema=TreeListArgs)
def list_directory_tree(directory_path: str, max_depth: Optional[int] = None,
                       file_pattern: Optional[str] = None, show_only_files: bool = False,
                       output_format: str = "tree") -> str:
    """
    åˆ—å‡ºç›®å½•æ ‘ç»“æ„æˆ–ç®€å•åˆ—è¡¨
    
    éµå¾ªå•ä¸€èŒè´£åŸåˆ™ï¼šä¸“é—¨å¤„ç†ç›®å½•æ ‘å’Œåˆ—è¡¨æ˜¾ç¤º
    æä¾›å¤šç§è¾“å‡ºæ ¼å¼å’Œè¿‡æ»¤é€‰é¡¹
    """
    try:
        # éªŒè¯è¾“å‡ºæ ¼å¼
        if output_format not in ["tree", "list"]:
            return f"é”™è¯¯ï¼šä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼ '{output_format}'ã€‚æ”¯æŒçš„æ ¼å¼ï¼š'tree', 'list'"
        
        # æ ¹æ®è¾“å‡ºæ ¼å¼è°ƒç”¨ç›¸åº”çš„è¾…åŠ©å‡½æ•°
        if output_format == "tree":
            result = _generate_tree_structure(directory_path, max_depth, file_pattern, show_only_files)
            if not result.startswith("é”™è¯¯ï¼š"):
                result = f"ç›®å½• '{directory_path}' æ ‘å½¢ç»“æ„ï¼š\n{result}"
            return result
        else:  # output_format == "list"
            return _generate_simple_list(directory_path, max_depth, file_pattern, show_only_files)
    
    except Exception as e:
        return f"åˆ—å‡ºç›®å½•æ ‘æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"

@tool(args_schema=TaskListArgs)
def generate_analysis_task_list(analysis_type: str = "standard", force_refresh: bool = False) -> str:
    """
    ç”Ÿæˆæ™ºèƒ½RNA-seqåˆ†æä»»åŠ¡åˆ—è¡¨ï¼Œè‡ªåŠ¨æ£€æµ‹æœ¬åœ°æ–‡ä»¶å¹¶ç¡®å®šæœ€ä¼˜é…ç½®
    
    éµå¾ªæ™ºèƒ½é…ç½®åŸåˆ™ï¼šä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œè‡ªåŠ¨ç”Ÿæˆnextflowå‚æ•°
    """
    try:
        result = ["ğŸ“‹ **æ™ºèƒ½ä»»åŠ¡åˆ—è¡¨ç”Ÿæˆ**"]
        result.append("=" * 50)
        
        # ç¬¬1æ­¥ï¼šæ£€æµ‹æœ¬åœ°FASTQæ–‡ä»¶
        result.append("\nğŸ” **æ­¥éª¤1ï¼šæ£€æµ‹FASTQæ–‡ä»¶**")
        fastq_detection = _detect_local_fastq_files()
        result.extend(fastq_detection["summary"])
        
        # ç¬¬2æ­¥ï¼šæ£€æµ‹æœ¬åœ°åŸºå› ç»„æ–‡ä»¶  
        result.append("\nğŸ§¬ **æ­¥éª¤2ï¼šæ£€æµ‹åŸºå› ç»„æ–‡ä»¶**")
        genome_detection = _detect_local_genome_files()
        result.extend(genome_detection["summary"])
        
        # ç¬¬3æ­¥ï¼šç”Ÿæˆæ¨èé…ç½®
        result.append("\nâš™ï¸ **æ­¥éª¤3ï¼šç”Ÿæˆæ¨èé…ç½®**")
        recommended_config = _generate_recommended_config(
            fastq_detection["data"], 
            genome_detection["data"], 
            analysis_type
        )
        result.extend(recommended_config["summary"])
        
        # ç¬¬4æ­¥ï¼šç”Ÿæˆä»»åŠ¡åˆ—è¡¨
        result.append("\nğŸ“ **æ­¥éª¤4ï¼šåˆ†æä»»åŠ¡åˆ—è¡¨**")
        task_list = _generate_task_steps(recommended_config["config"], analysis_type)
        result.extend(task_list)
        
        # ç¬¬5æ­¥ï¼šæ˜¾ç¤ºæœ€ç»ˆé…ç½®
        result.append("\nğŸ¯ **æœ€ç»ˆæ¨èé…ç½®**")
        config_summary = _format_config_summary(recommended_config["config"])
        result.extend(config_summary)
        
        # ä½¿ç”¨å»ºè®®
        result.append("\nğŸ’¡ **ä½¿ç”¨å»ºè®®**")
        if recommended_config["config"].get("has_local_files"):
            result.append("âœ… æ£€æµ‹åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œé…ç½®å·²ä¼˜åŒ–ä¸ºä½¿ç”¨æœ¬åœ°èµ„æº")
        else:
            result.append("âš ï¸ æœªæ£€æµ‹åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œå°†éœ€è¦ä¸‹è½½æ•°æ®å’ŒåŸºå› ç»„")
        
        result.append("ğŸ“‹ é…ç½®å·²ç”Ÿæˆï¼Œå¯ç›´æ¥ç”¨äºnextflowæ‰§è¡Œ")
        
        return "\n".join(result)
        
    except Exception as e:
        return f"ç”Ÿæˆä»»åŠ¡åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"

def _detect_local_fastq_files() -> Dict[str, Any]:
    """æ£€æµ‹æœ¬åœ°FASTQæ–‡ä»¶"""
    try:
        # æœç´¢é»˜è®¤FASTQè·¯å¾„
        search_paths = ["data/fastq", "data/results/fastp", "fastq", "raw_data"]
        found_files = []
        
        for path in search_paths:
            if os.path.exists(path):
                for root, dirs, files in os.walk(path):
                    for file in files:
                        if file.endswith(('.fastq', '.fq', '.fastq.gz', '.fq.gz')):
                            found_files.append(os.path.join(root, file))
        
        if found_files:
            # åˆ†ææ–‡ä»¶ç±»å‹
            paired_files = {}
            single_files = []
            
            for file_path in found_files:
                file_name = os.path.basename(file_path)
                # ç®€åŒ–çš„é…å¯¹æ£€æµ‹
                if '_1.' in file_name or '_R1.' in file_name:
                    sample_id = file_name.split('_')[0]
                    if sample_id not in paired_files:
                        paired_files[sample_id] = {}
                    paired_files[sample_id]['R1'] = file_path
                elif '_2.' in file_name or '_R2.' in file_name:
                    sample_id = file_name.split('_')[0] 
                    if sample_id not in paired_files:
                        paired_files[sample_id] = {}
                    paired_files[sample_id]['R2'] = file_path
                else:
                    single_files.append(file_path)
            
            summary = [
                f"âœ… æ£€æµ‹åˆ° {len(found_files)} ä¸ªFASTQæ–‡ä»¶",
                f"   - åŒç«¯æ–‡ä»¶ï¼š{len(paired_files)} å¯¹æ ·æœ¬",
                f"   - å•ç«¯æ–‡ä»¶ï¼š{len(single_files)} ä¸ª",
                f"   - å»ºè®®é…ç½®ï¼šä½¿ç”¨æœ¬åœ°FASTQæ–‡ä»¶"
            ]
            
            return {
                "data": {
                    "found": True,
                    "paired_files": paired_files,
                    "single_files": single_files,
                    "total_files": len(found_files),
                    "recommended_path": search_paths[0] if os.path.exists(search_paths[0]) else None
                },
                "summary": summary
            }
        else:
            summary = [
                "âŒ æœªæ£€æµ‹åˆ°æœ¬åœ°FASTQæ–‡ä»¶",
                "   - æœç´¢è·¯å¾„ï¼š" + ", ".join(search_paths),
                "   - å»ºè®®é…ç½®ï¼šéœ€è¦æä¾›SRR IDæˆ–ä¸Šä¼ FASTQæ–‡ä»¶"
            ]
            
            return {
                "data": {"found": False},
                "summary": summary
            }
            
    except Exception as e:
        return {
            "data": {"found": False, "error": str(e)},
            "summary": [f"âŒ FASTQæ–‡ä»¶æ£€æµ‹å¤±è´¥ï¼š{str(e)}"]
        }

def _detect_local_genome_files() -> Dict[str, Any]:
    """æ£€æµ‹æœ¬åœ°åŸºå› ç»„æ–‡ä»¶"""
    try:
        # è¯»å–åŸºå› ç»„é…ç½®
        config_path = "config/genomes.json"
        if not os.path.exists(config_path):
            return {
                "data": {"found": False},
                "summary": ["âŒ åŸºå› ç»„é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"]
            }
        
        with open(config_path, 'r', encoding='utf-8') as f:
            genomes_config = json.load(f)
        
        available_genomes = []
        ready_genomes = []
        
        for name, info in genomes_config.items():
            fasta_path = info.get('fasta', '')
            gtf_path = info.get('gtf', '')
            
            fasta_exists = os.path.exists(fasta_path) if fasta_path else False
            gtf_exists = os.path.exists(gtf_path) if gtf_path else False
            
            available_genomes.append(name)
            
            if fasta_exists and gtf_exists:
                ready_genomes.append({
                    "name": name,
                    "fasta": fasta_path,
                    "gtf": gtf_path,
                    "species": info.get('species', 'unknown')
                })
        
        if ready_genomes:
            summary = [
                f"âœ… æ£€æµ‹åˆ° {len(ready_genomes)} ä¸ªå¯ç”¨åŸºå› ç»„",
                f"   - æ¨èä½¿ç”¨ï¼š{ready_genomes[0]['name']} ({ready_genomes[0]['species']})",
                f"   - å…¶ä»–å¯é€‰ï¼š{', '.join([g['name'] for g in ready_genomes[1:]])}" if len(ready_genomes) > 1 else ""
            ]
            summary = [s for s in summary if s]  # ç§»é™¤ç©ºå­—ç¬¦ä¸²
            
            return {
                "data": {
                    "found": True,
                    "ready_genomes": ready_genomes,
                    "total_available": len(available_genomes),
                    "recommended": ready_genomes[0]
                },
                "summary": summary
            }
        else:
            summary = [
                f"âš ï¸ é…ç½®ä¸­æœ‰ {len(available_genomes)} ä¸ªåŸºå› ç»„ï¼Œä½†æ–‡ä»¶æœªä¸‹è½½",
                f"   - å¯ç”¨åŸºå› ç»„ï¼š{', '.join(available_genomes)}",
                "   - å»ºè®®é…ç½®ï¼šéœ€è¦ä¸‹è½½åŸºå› ç»„æ–‡ä»¶"
            ]
            
            return {
                "data": {
                    "found": False,
                    "available_genomes": list(genomes_config.keys()),
                    "total_available": len(available_genomes)
                },
                "summary": summary
            }
            
    except Exception as e:
        return {
            "data": {"found": False, "error": str(e)},
            "summary": [f"âŒ åŸºå› ç»„æ–‡ä»¶æ£€æµ‹å¤±è´¥ï¼š{str(e)}"]
        }

def _generate_recommended_config(fastq_data: Dict, genome_data: Dict, analysis_type: str) -> Dict[str, Any]:
    """ç”Ÿæˆæ¨èçš„nextflowé…ç½®"""
    try:
        config = {
            "data": "./data",
            "run_fastp": True,
            "run_star_align": True,
            "run_featurecounts": True,
            "run_build_star_index": True,
            "has_local_files": False
        }
        
        summary = []
        
        # é…ç½®FASTQæ–‡ä»¶
        if fastq_data.get("found"):
            if fastq_data.get("recommended_path"):
                config["local_fastq_files"] = fastq_data["recommended_path"] + "/*.fastq*"
                config["run_download_srr"] = False
                summary.append("âœ… é…ç½®ä½¿ç”¨æœ¬åœ°FASTQæ–‡ä»¶")
                config["has_local_files"] = True
            else:
                summary.append("âš ï¸ æ£€æµ‹åˆ°FASTQæ–‡ä»¶ä½†è·¯å¾„ä¸æ˜ç¡®")
        else:
            config["run_download_srr"] = True
            config["srr_ids"] = ""  # éœ€è¦ç”¨æˆ·æä¾›
            summary.append("ğŸ“¥ é…ç½®ä¸ºä¸‹è½½SRRæ•°æ®ï¼ˆéœ€è¦ç”¨æˆ·æä¾›SRR IDï¼‰")
        
        # é…ç½®åŸºå› ç»„æ–‡ä»¶
        if genome_data.get("found") and genome_data.get("recommended"):
            recommended = genome_data["recommended"]
            config["local_genome_path"] = recommended["fasta"]
            config["local_gtf_path"] = recommended["gtf"]
            config["run_download_genome"] = False
            config["genome_version"] = recommended["name"]
            summary.append(f"âœ… é…ç½®ä½¿ç”¨æœ¬åœ°åŸºå› ç»„ï¼š{recommended['name']}")
            config["has_local_files"] = True
        else:
            config["run_download_genome"] = True
            config["genome_version"] = "hg38"  # é»˜è®¤
            summary.append("ğŸ“¥ é…ç½®ä¸ºä¸‹è½½åŸºå› ç»„æ–‡ä»¶ï¼ˆé»˜è®¤hg38ï¼‰")
        
        # æ ¹æ®åˆ†æç±»å‹è°ƒæ•´
        if analysis_type == "minimal":
            config["run_fastp"] = False
            summary.append("ğŸ”§ æœ€å°æ¨¡å¼ï¼šè·³è¿‡è´¨é‡æ§åˆ¶")
        elif analysis_type == "comprehensive":
            config["run_multiqc"] = True
            summary.append("ğŸ”§ å…¨é¢æ¨¡å¼ï¼šå¯ç”¨MultiQCæŠ¥å‘Š")
        
        return {
            "config": config,
            "summary": summary
        }
        
    except Exception as e:
        return {
            "config": {},
            "summary": [f"âŒ é…ç½®ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"]
        }

def _generate_task_steps(config: Dict, analysis_type: str) -> List[str]:
    """ç”Ÿæˆä»»åŠ¡æ­¥éª¤åˆ—è¡¨"""
    try:
        steps = []
        step_num = 1
        
        # æ•°æ®å‡†å¤‡æ­¥éª¤
        if config.get("run_download_srr"):
            steps.append(f"{step_num}. ğŸ“¥ ä¸‹è½½SRRæ•°æ®æ–‡ä»¶")
            step_num += 1
        
        if config.get("run_download_genome"):
            steps.append(f"{step_num}. ğŸ“¥ ä¸‹è½½åŸºå› ç»„å‚è€ƒæ–‡ä»¶")
            step_num += 1
        
        # ç´¢å¼•æ„å»º
        if config.get("run_build_star_index"):
            steps.append(f"{step_num}. ğŸ”¨ æ„å»ºSTARåŸºå› ç»„ç´¢å¼•")
            step_num += 1
        
        # æ•°æ®å¤„ç†æ­¥éª¤
        if config.get("run_fastp"):
            steps.append(f"{step_num}. ğŸ§¹ è´¨é‡æ§åˆ¶å’Œæ•°æ®æ¸…ç† (FastP)")
            step_num += 1
        
        if config.get("run_star_align"):
            steps.append(f"{step_num}. ğŸ¯ åºåˆ—æ¯”å¯¹åˆ°å‚è€ƒåŸºå› ç»„ (STAR)")
            step_num += 1
        
        if config.get("run_featurecounts"):
            steps.append(f"{step_num}. ğŸ“Š åŸºå› è¡¨è¾¾å®šé‡ (featureCounts)")
            step_num += 1
        
        # é¢å¤–æ­¥éª¤
        if config.get("run_multiqc"):
            steps.append(f"{step_num}. ğŸ“‹ ç”Ÿæˆç»¼åˆè´¨é‡æŠ¥å‘Š (MultiQC)")
            step_num += 1
        
        steps.append(f"{step_num}. ğŸ“ æ•´ç†è¾“å‡ºç»“æœå’Œæ—¥å¿—æ–‡ä»¶")
        
        return steps
        
    except Exception as e:
        return [f"âŒ ä»»åŠ¡æ­¥éª¤ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"]

def _format_config_summary(config: Dict) -> List[str]:
    """æ ¼å¼åŒ–é…ç½®æ‘˜è¦"""
    try:
        summary = []
        
        # æ•°æ®æº
        summary.append("**æ•°æ®æºé…ç½®ï¼š**")
        if config.get("local_fastq_files"):
            summary.append(f"  ğŸ“ FASTQæ–‡ä»¶ï¼š{config['local_fastq_files']}")
        elif config.get("srr_ids"):
            summary.append(f"  ğŸ“¥ SRRä¸‹è½½ï¼š{config['srr_ids']}")
        else:
            summary.append("  âš ï¸ FASTQï¼šéœ€è¦é…ç½®")
        
        # åŸºå› ç»„
        if config.get("local_genome_path"):
            summary.append(f"  ğŸ§¬ åŸºå› ç»„ï¼š{config.get('genome_version', 'local')}")
        else:
            summary.append(f"  ğŸ“¥ åŸºå› ç»„ä¸‹è½½ï¼š{config.get('genome_version', 'hg38')}")
        
        # åˆ†ææ­¥éª¤
        summary.append("\n**åˆ†ææµç¨‹ï¼š**")
        processes = []
        if config.get("run_fastp"):
            processes.append("è´¨é‡æ§åˆ¶")
        if config.get("run_star_align"):
            processes.append("åºåˆ—æ¯”å¯¹")
        if config.get("run_featurecounts"):
            processes.append("è¡¨è¾¾å®šé‡")
        
        summary.append(f"  ğŸ”¬ å¯ç”¨æµç¨‹ï¼š{' â†’ '.join(processes)}")
        summary.append(f"  ğŸ“‚ è¾“å‡ºç›®å½•ï¼š{config.get('data', './data')}")
        
        return summary
        
    except Exception as e:
        return [f"âŒ é…ç½®æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"]