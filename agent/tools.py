# agent/tools.py - v5.2 Architecture (with all tools implemented)
import os
import time
import json
import threading
import subprocess
import logging
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime


# ç§»é™¤è¿›åº¦æ¡æ¨¡å—å¯¼å…¥
# from .progress_bar import format_progress_update, get_progress_summary

# --- Module-level lock for config file operations ---
_config_lock = threading.Lock()

# --- Environment and Tool Mappings ---
ENVIRONMENT_TOOLS = {
    "qc_env": {
        "fastp": "fastp",
        "fasterq-dump": "sra-tools"
    },
    "align_env": {
        "star": "star"
    },
    "quant_env": {
        "featurecounts": "subread"
    }
}

# --- Tool Dependencies ---
TOOL_DEPENDENCIES = {
    # ç¯å¢ƒæ£€æŸ¥
    "check_environment": [],
    "setup_environment": ["check_environment"],
    
    # åŸºå› ç»„ç®¡ç†
    "search_genome": [],
    "download_genome": ["search_genome"],
    "build_star_index": ["search_genome"],
    
    # æ•°æ®ç®¡ç†
    "search_fastq": [],
    "download_fastq": ["search_fastq"],
    "validate_fastq": ["search_fastq"],
    
    # è´¨é‡æ§åˆ¶
    "run_fastp": ["validate_fastq"],
    
    # æ¯”å¯¹åˆ†æ
    "run_star_align": ["build_star_index", "run_fastp"],
    
    # å®šé‡åˆ†æ
    "run_featurecounts": ["run_star_align"],
    
    # ç»“æœæ•´ç†
    "collect_results": ["run_featurecounts"],
    "generate_report": ["run_fastp", "run_featurecounts"]
}

# --- Helper Functions ---
def _get_project_root() -> Path:
    """Returns the absolute path to the project root."""
    return Path(__file__).parent.parent.resolve()

def _get_genomes_config() -> dict:
    """Reads and parses the genomes.json config file."""
    config_path = _get_project_root() / 'config' / 'genomes.json'
    try:
        with open(config_path, 'r') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}

# --- Background Threads for Task Execution & Monitoring ---

def _monitor_and_parse_logs(task_id: str, log_file: Path, task_database: dict, db_lock: threading.Lock):
    """
    Tails a log file in a background thread, parses each new line,
    and updates the corresponding step in the task database.
    The thread terminates when the main task is no longer 'running'.
    """
    print(f"[{task_id}] Log monitor thread started for: {log_file}")
    cursor = 0
    while True:
        with db_lock:
            task_status = task_database.get(task_id, {}).get('status')
        
        if task_status not in ['starting', 'running']:
            print(f"[{task_id}] Main task status is '{task_status}'. Stopping log monitor.")
            break

        try:
            if log_file.exists():
                with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                    f.seek(cursor)
                    for line in f:
                        # ç®€åŒ–çš„æ—¥å¿—å¤„ç†ï¼Œä¸å†ä½¿ç”¨å¤æ‚çš„è§£æ
                        cleaned_line = line.strip()
                        with db_lock:
                            if task_id not in task_database: continue
                            # ç›´æ¥æ·»åŠ åˆ°ç¬¬ä¸€ä¸ªæ­¥éª¤çš„æ—¥å¿—ä¸­
                            if task_database[task_id]['details']['steps']:
                                task_database[task_id]['details']['steps'][0]['logs'].append(cleaned_line)
                    cursor = f.tell()
        except Exception as e:
            print(f"[{task_id}] Error reading log file: {e}")

        time.sleep(2) # Poll for new log entries every 2 seconds
    
    print(f"[{task_id}] Log monitor thread finished.")


# --- New Tools for v5.2 Architecture ---

def get_task_status(task_id: str, task_database: dict, db_lock: threading.Lock) -> dict:
    """
    Queries the status of a specific task ID. This is now a simple, thread-safe
    read from the shared task database, as background threads handle all updates.
    """
    print(f"Tool 'get_task_status' called for task: {task_id}")
    with db_lock:
        task_info = task_database.get(task_id)
        if not task_info:
            return {"status": "error", "message": f"Task '{task_id}' not found."}
        
        return {
            "status": "success",
            "task_id": task_id,
            "task_info": json.loads(json.dumps(task_info))  # ç¡®ä¿è¿”å›çš„æ˜¯å¯åºåˆ—åŒ–çš„JSON
        }

def list_available_genomes() -> dict:
    """Reads and returns the content of config/genomes.json."""
    print("Tool 'list_available_genomes' called.")
    genomes_data = _get_genomes_config()
    if not genomes_data:
        return {"error": "Could not read or find 'config/genomes.json'."}
    return {"status": "success", "genomes": genomes_data}


def list_files(path: str, recursive: bool = False) -> dict:
    """
    Lists files and directories in a given path relative to the project root.
    """
    print(f"Tool 'list_files' called for path: {path}, recursive: {recursive}")
    project_root = _get_project_root()
    target_path = (project_root / path).resolve()

    # Security check to prevent directory traversal
    if project_root not in target_path.parents and target_path != project_root:
        return {"status": "error", "message": "Access denied: Path is outside of the project directory."}

    if not target_path.exists():
        return {"status": "error", "message": f"Path does not exist: {target_path}"}
    
    results = []
    try:
        if recursive:
            for root, dirs, files in os.walk(target_path):
                rel_root = Path(root).relative_to(target_path)
                for name in dirs:
                    results.append(str(rel_root / name) + "/")
                for name in files:
                    results.append(str(rel_root / name))
        else:
            for entry in os.listdir(target_path):
                if (target_path / entry).is_dir():
                    results.append(entry + "/")
                else:
                    results.append(entry)
        return {"status": "success", "path": path, "contents": sorted(results)}
    except Exception as e:
        return {"status": "error", "message": str(e)}

def add_genome_to_config(genome_entry_json_str: str) -> dict:
    """
    Adds or updates a complete genome entry in config/genomes.json from a single JSON string.
    The LLM is responsible for generating the full entry, including the top-level key.
    Example input: '{"xenLae2": {"species": "xenopus_laevis", ...}}'
    """
    print("Tool 'add_genome_to_config' called.")

    # 1. è§£æç”±LLMç”Ÿæˆçš„ã€åŒ…å«é¡¶çº§é”®çš„JSONå­—ç¬¦ä¸²
    try:
        new_entry_dict = json.loads(genome_entry_json_str)
        if not isinstance(new_entry_dict, dict) or len(new_entry_dict) != 1:
            raise ValueError("JSON string must represent a dictionary with a single top-level key (the genome version).")
    except (json.JSONDecodeError, ValueError) as e:
        return {"status": "error", "message": f"Invalid JSON string provided: {e}"}

    # 2. æ›´æ–°é…ç½®æ–‡ä»¶
    config_path = _get_project_root() / 'config' / 'genomes.json'
    with _config_lock:
        try:
            genomes_config = _get_genomes_config()
            
            # å°†æ–°çš„å­—å…¸æ¡ç›®åˆå¹¶åˆ°ç°æœ‰é…ç½®ä¸­
            genomes_config.update(new_entry_dict)
            
            config_path.parent.mkdir(parents=True, exist_ok=True)
            with open(config_path, 'w') as f:
                # ä¿æŒä¸åŸæ–‡ä»¶ä¸€è‡´çš„ç¼©è¿›æ ¼å¼
                json.dump(genomes_config, f, indent=2)
                
            version_key = list(new_entry_dict.keys())[0]
            return {"status": "success", "message": f"Genome '{version_key}' was successfully added/updated in config/genomes.json."}
        except Exception as e:
            return {"status": "error", "message": f"Failed to write to config file: {e}"}

def download_genome_files(genome_name: str, task_database: dict, db_lock: threading.Lock, task_id_counter: int) -> dict:
    """
    A dedicated tool to download genome files and prepare the STAR index.
    It creates a specific plan and reuses the main execution logic.
    NOTE: This tool requires server-injected dependencies.
    """
    print(f"Tool 'download_genome_files' called for: {genome_name}")

    genomes_config = _get_genomes_config()
    genome_info = genomes_config.get(genome_name)
    if not genome_info:
        return {"status": "error", "message": f"Genome '{genome_name}' not found in config/genomes.json. Please add it first using 'add_genome_to_config'."}

    plan = {
        "srr_ids": [],
        "genome_name": genome_name,
        "tools_to_execute": ['download_genome', 'build_star_index'],
        "messages": [
            f"â„¹ï¸ **è®¡åˆ’**: å°†ä¸‹è½½ '{genome_name}' çš„åŸºå› ç»„æºæ–‡ä»¶ã€‚",
            f"â„¹ï¸ **è®¡åˆ’**: å°†ä¸º '{genome_name}' æ–°å»º STAR ç´¢å¼•ã€‚"
        ],
        "is_executable": True,
        "genome_info": genome_info
    }
    
    description = f"Download and prepare genome: {genome_name}"

    return execute_planned_task(plan, description, task_database, db_lock, task_id_counter)

# --- Fallback Tool ---

def unsupported_request(user_request: str) -> dict:
    """
    Handles requests that are not supported by the current tool set.
    """
    return {
        "status": "error",
        "message": f"ä¸æ”¯æŒæ­¤è¯·æ±‚: {user_request}",
        "suggestions": [
            "è¯·ä½¿ç”¨ 'plan_analysis_task' æ¥åˆ¶å®šåˆ†æè®¡åˆ’",
            "è¯·ä½¿ç”¨ 'execute_planned_task' æ¥æ‰§è¡Œåˆ†æä»»åŠ¡",
            "è¯·ä½¿ç”¨ 'list_available_genomes' æ¥æŸ¥çœ‹å¯ç”¨åŸºå› ç»„"
        ]
    }

# --- New Search Tools ---

def search_genome_tool(genome_name: str = None, **kwargs) -> dict:
    """
    æœç´¢å’Œåˆ—å‡ºåŸºå› ç»„ä¿¡æ¯ - å¯æŸ¥è¯¢ç‰¹å®šåŸºå› ç»„æˆ–åˆ—å‡ºæ‰€æœ‰å¯ç”¨åŸºå› ç»„
    """
    print(f"Tool 'search_genome_tool' called for genome '{genome_name}'.")
    project_root = _get_project_root()
    genomes_config = _get_genomes_config()
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šåŸºå› ç»„åç§°ï¼Œåˆ—å‡ºæ‰€æœ‰å¯ç”¨åŸºå› ç»„
    if not genome_name:
        available_genomes = []
        for name, info in genomes_config.items():
            fasta_path = project_root / info['fasta']
            gtf_path = project_root / info['gtf']
            star_index_path = fasta_path.parent / 'star_index'
            
            files_exist = fasta_path.is_file() and gtf_path.is_file()
            star_index_exists = star_index_path.is_dir() and (star_index_path / 'SA').is_file()
            
            available_genomes.append({
                "name": name,
                "species": info['species'],
                "version": info['version'],
                "exists_in_filesystem": files_exist,
                "star_index_exists": star_index_exists,
                "fasta_path": str(fasta_path),
                "gtf_path": str(gtf_path),
                "star_index_path": str(star_index_path)
            })
        
        return {
            "status": "success",
            "message": f"æ‰¾åˆ° {len(available_genomes)} ä¸ªå¯ç”¨åŸºå› ç»„",
            "available_genomes": available_genomes,
            "total_count": len(available_genomes)
        }
    
    # æ£€æŸ¥é…ç½®ä¸­æ˜¯å¦å­˜åœ¨
    genome_info = genomes_config.get(genome_name)
    if not genome_info:
        return {
            "status": "not_found",
            "message": f"åŸºå› ç»„ '{genome_name}' åœ¨é…ç½®ä¸­æœªæ‰¾åˆ°",
            "genome_name": genome_name,
            "exists_in_config": False,
            "exists_in_filesystem": False
        }
    
    # æ£€æŸ¥æ–‡ä»¶ç³»ç»Ÿä¸­æ˜¯å¦å­˜åœ¨
    fasta_path = project_root / genome_info['fasta']
    gtf_path = project_root / genome_info['gtf']
    star_index_path = fasta_path.parent / 'star_index'
    
    files_exist = fasta_path.is_file() and gtf_path.is_file()
    star_index_exists = star_index_path.is_dir() and (star_index_path / 'SA').is_file()
    
    return {
        "status": "success",
        "genome_name": genome_name,
        "exists_in_config": True,
        "exists_in_filesystem": files_exist,
        "star_index_exists": star_index_exists,
        "fasta_path": str(fasta_path),
        "gtf_path": str(gtf_path),
        "star_index_path": str(star_index_path),
        "genome_info": genome_info
    }

def search_fastq_tool(srr_id: str, **kwargs) -> dict:
    """
    æœç´¢å’ŒéªŒè¯FASTQæ–‡ä»¶ - æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§å¹¶éªŒè¯å®Œæ•´æ€§
    """
    print(f"Tool 'search_fastq_tool' called for SRR '{srr_id}'.")
    project_root = _get_project_root()
    data_dir = project_root / 'data'
    
    # æ£€æŸ¥ FASTQ æ–‡ä»¶
    fastq_dir = data_dir / 'fastq'
    r1_path = fastq_dir / f"{srr_id}_1.fastq.gz"
    r2_path = fastq_dir / f"{srr_id}_2.fastq.gz"
    
    r1_exists = r1_path.is_file()
    r2_exists = r2_path.is_file()
    
    # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
    validation_results = {}
    if r1_exists:
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            r1_size = r1_path.stat().st_size
            validation_results["r1_size"] = r1_size
            validation_results["r1_valid"] = r1_size > 0
        except Exception as e:
            validation_results["r1_valid"] = False
            validation_results["r1_error"] = str(e)
    
    if r2_exists:
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            r2_size = r2_path.stat().st_size
            validation_results["r2_size"] = r2_size
            validation_results["r2_valid"] = r2_size > 0
        except Exception as e:
            validation_results["r2_valid"] = False
            validation_results["r2_error"] = str(e)
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºé…å¯¹ç«¯æ•°æ®
    is_paired = r1_exists and r2_exists
    is_single = r1_exists and not r2_exists
    
    # ç»¼åˆéªŒè¯ç»“æœ
    overall_valid = (r1_exists and validation_results.get("r1_valid", False)) and \
                   (not is_paired or (r2_exists and validation_results.get("r2_valid", False)))
    
    return {
        "status": "success",
        "srr_id": srr_id,
        "r1_exists": r1_exists,
        "r2_exists": r2_exists,
        "r1_path": str(r1_path),
        "r2_path": str(r2_path),
        "is_paired": is_paired,
        "is_single": is_single,
        "validation_results": validation_results,
        "overall_valid": overall_valid,
        "message": f"FASTQæ–‡ä»¶éªŒè¯å®Œæˆ - {'æœ‰æ•ˆ' if overall_valid else 'å­˜åœ¨é—®é¢˜'}"
    }

# --- Environment Management Tools ---

def check_environment_tool(**kwargs) -> dict:
    """
    æ£€æŸ¥å½“å‰ç¯å¢ƒçŠ¶æ€ - ä¿®å¤ç‰ˆæœ¬
    """
    print("Tool 'check_environment_tool' called.")
    
    # æ£€æŸ¥å…³é”®å·¥å…· - åœ¨condaç¯å¢ƒä¸­æŸ¥æ‰¾
    tools_status = {}
    
    # å®šä¹‰å·¥å…·å’Œå¯¹åº”çš„condaç¯å¢ƒ
    key_tools = {
        "fastp": {"env": "qc_env", "description": "è´¨é‡æ§åˆ¶å·¥å…·"},
        "STAR": {"env": "align_env", "description": "æ¯”å¯¹å·¥å…·"}, 
        "featureCounts": {"env": "quant_env", "description": "å®šé‡å·¥å…·"},
        "samtools": {"env": "align_env", "description": "BAMå¤„ç†å·¥å…·"},
        "ascp": {"env": None, "description": "Asperaä¼ è¾“å·¥å…·"}
    }
    
    for tool, info in key_tools.items():
        try:
            if info["env"] is None:
                # å¯¹äºnextflowï¼Œæ£€æŸ¥ç³»ç»ŸPATH
                result = subprocess.run(["which", tool], 
                                      capture_output=True, 
                                      text=True)
                available = result.returncode == 0
                path = result.stdout.strip() if result.returncode == 0 else None
            else:
                # å¯¹äºå…¶ä»–å·¥å…·ï¼Œæ£€æŸ¥condaç¯å¢ƒä¸­çš„è·¯å¾„
                env_path = f"/opt/conda/envs/{info['env']}/bin/{tool}"
                available = os.path.exists(env_path)
                path = env_path if available else None
                
                # å¦‚æœå·¥å…·ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨conda runæ£€æŸ¥
                if not available:
                    try:
                        result = subprocess.run([
                            "bash", "-c", 
                            f"source /opt/conda/bin/activate {info['env']} && which {tool}"
                        ], capture_output=True, text=True)
                        available = result.returncode == 0
                        path = result.stdout.strip() if result.returncode == 0 else None
                    except Exception:
                        pass
            
            tools_status[tool] = {
                "available": available,
                "description": info["description"],
                "path": path,
                "environment": info["env"]
            }
            print(f"å·¥å…· {tool}: {'å¯ç”¨' if available else 'ä¸å¯ç”¨'} (ç¯å¢ƒ: {info['env']})")
        except Exception as e:
            tools_status[tool] = {
                "available": False,
                "description": info["description"],
                "error": str(e),
                "environment": info["env"]
            }
            print(f"æ£€æŸ¥å·¥å…· {tool} æ—¶å‡ºé”™: {e}")
    
    # æ£€æŸ¥condaç¯å¢ƒ
    conda_envs = ["qc_env", "align_env", "quant_env"]
    env_status = {}
    
    for env in conda_envs:
        try:
            # æ£€æŸ¥ç¯å¢ƒç›®å½•æ˜¯å¦å­˜åœ¨
            env_path = f"/opt/conda/envs/{env}"
            env_status[env] = {
                "exists": os.path.exists(env_path),
                "path": env_path
            }
            print(f"ç¯å¢ƒ {env}: {'å­˜åœ¨' if os.path.exists(env_path) else 'ä¸å­˜åœ¨'}")
        except Exception as e:
            env_status[env] = {
                "exists": False,
                "error": str(e)
            }
            print(f"æ£€æŸ¥ç¯å¢ƒ {env} æ—¶å‡ºé”™: {e}")
    
    # æ£€æŸ¥å…³é”®ç›®å½•
    directories = {
        "data_dir": "/app/data",
        "results_dir": "/app/data/results", 
        "fastq_dir": "/app/data/fastq",
        "genome_dir": "/app/genomes"
    }
    
    dir_status = {}
    for name, path in directories.items():
        dir_status[name] = {
            "exists": os.path.exists(path),
            "path": path,
            "writable": os.access(path, os.W_OK) if os.path.exists(path) else False
        }
    
    return {
        "status": "success",
        "message": "ç¯å¢ƒæ£€æŸ¥å®Œæˆ",
        "tools_status": tools_status,
        "conda_environments": env_status,
        "directories": dir_status,
        "summary": {
            "total_tools": len(tools_status),
            "available_tools": sum(1 for t in tools_status.values() if t["available"]),
            "total_envs": len(conda_envs),
            "existing_envs": sum(1 for e in env_status.values() if e["exists"])
        }
    }

def setup_environment_tool(environment_type: str = "conda", **kwargs) -> dict:
    """
    è®¾ç½®åˆ†æç¯å¢ƒ - ä¿®å¤ç‰ˆæœ¬
    """
    print(f"Tool 'setup_environment_tool' called for environment type '{environment_type}'.")
    
    if environment_type != "conda":
        return {
            "status": "error",
            "message": f"ä¸æ”¯æŒçš„ç¯å¢ƒç±»å‹: {environment_type}ï¼Œç›®å‰åªæ”¯æŒ conda"
        }
    
    # æ£€æŸ¥condaç¯å¢ƒ - ä¿®å¤ç‰ˆæœ¬
    envs_to_check = ["qc_env", "align_env", "quant_env"]
    env_status = {}
    
    for env in envs_to_check:
        try:
            # æ£€æŸ¥ç¯å¢ƒç›®å½•æ˜¯å¦å­˜åœ¨
            env_path = f"/opt/conda/envs/{env}"
            if os.path.exists(env_path):
                # æ£€æŸ¥ç¯å¢ƒä¸­çš„å…³é”®å·¥å…·
                tools_to_check = {
                    "qc_env": ["fastp"],
                    "align_env": ["STAR", "samtools"],
                    "quant_env": ["featureCounts"]
                }
                
                available_tools = []
                missing_tools = []
                
                for tool in tools_to_check.get(env, []):
                    # é¦–å…ˆæ£€æŸ¥å·¥å…·æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    tool_path = f"{env_path}/bin/{tool}"
                    if os.path.exists(tool_path):
                        available_tools.append(tool)
                    else:
                        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨conda runæ£€æŸ¥
                        try:
                            result = subprocess.run([
                                "bash", "-c", 
                                f"source /opt/conda/bin/activate {env} && which {tool}"
                            ], capture_output=True, text=True)
                            if result.returncode == 0:
                                available_tools.append(tool)
                            else:
                                missing_tools.append(tool)
                        except Exception:
                            missing_tools.append(tool)
                
                env_status[env] = {
                    "exists": True,
                    "path": env_path,
                    "available_tools": available_tools,
                    "missing_tools": missing_tools,
                    "ready": len(missing_tools) == 0
                }
            else:
                env_status[env] = {
                    "exists": False,
                    "path": env_path,
                    "available_tools": [],
                    "missing_tools": [],
                    "ready": False
                }
        except Exception as e:
            env_status[env] = {
                "exists": False,
                "error": str(e),
                "ready": False
            }
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    directories = [
        "/app/data/results",
        "/app/data/fastq", 
        "/app/data/results/fastp",
        "/app/data/results/star",
        "/app/data/results/featurecounts"
    ]
    
    created_dirs = []
    failed_dirs = []
    
    for dir_path in directories:
        try:
            os.makedirs(dir_path, exist_ok=True)
            created_dirs.append(dir_path)
        except Exception as e:
            failed_dirs.append(f"{dir_path} (é”™è¯¯: {e})")
    
    # æ£€æŸ¥condaå‘½ä»¤æ˜¯å¦å¯ç”¨
    conda_available = False
    conda_path = None
    
    try:
        result = subprocess.run(["which", "conda"], 
                              capture_output=True, 
                              text=True)
        if result.returncode == 0:
            conda_available = True
            conda_path = result.stdout.strip()
    except Exception:
        pass
    
    return {
        "status": "success" if conda_available else "warning",
        "message": "ç¯å¢ƒè®¾ç½®æ£€æŸ¥å®Œæˆ",
        "conda_available": conda_available,
        "conda_path": conda_path,
        "environments": env_status,
        "directories": {
            "created": created_dirs,
            "failed": failed_dirs
        },
        "summary": {
            "total_envs": len(envs_to_check),
            "ready_envs": sum(1 for e in env_status.values() if e.get("ready", False)),
            "total_dirs": len(directories),
            "created_dirs": len(created_dirs)
        }
    }


def _parse_fastp_json(json_file_path: str) -> dict:
    """
    è§£æ fastp JSON æŠ¥å‘Šæ–‡ä»¶
    """
    try:
        with open(json_file_path, 'r') as f:
            data = json.load(f)
        
        summary = data.get('summary', {})
        before_filtering = summary.get('before_filtering', {})
        after_filtering = summary.get('after_filtering', {})
        
        return {
            "total_reads": before_filtering.get('total_reads', 0),
            "total_bases": before_filtering.get('total_bases', 0),
            "q20_rate": before_filtering.get('q20_rate', 0),
            "q30_rate": before_filtering.get('q30_rate', 0),
            "clean_reads": after_filtering.get('total_reads', 0),
            "clean_bases": after_filtering.get('total_bases', 0),
            "clean_q20_rate": after_filtering.get('q20_rate', 0),
            "clean_q30_rate": after_filtering.get('q30_rate', 0),
            "filtered_reads": before_filtering.get('total_reads', 0) - after_filtering.get('total_reads', 0),
            "filter_rate": (before_filtering.get('total_reads', 0) - after_filtering.get('total_reads', 0)) / before_filtering.get('total_reads', 1) * 100
        }
    except Exception as e:
        return {"error": f"è§£æfastp JSONå¤±è´¥: {e}"}

def _parse_star_log(log_file_path: str) -> dict:
    """
    è§£æ STAR log æ–‡ä»¶
    """
    try:
        with open(log_file_path, 'r') as f:
            lines = f.readlines()
        
        result = {}
        for line in lines:
            line = line.strip()
            if "Number of input reads" in line:
                result["input_reads"] = int(line.split()[-1])
            elif "Uniquely mapped reads number" in line:
                result["unique_mapped"] = int(line.split()[-1])
            elif "Number of reads mapped to multiple loci" in line:
                result["multi_mapped"] = int(line.split()[-1])
            elif "Number of reads unmapped" in line:
                result["unmapped"] = int(line.split()[-1])
            elif "Uniquely mapped reads %" in line:
                result["unique_mapping_rate"] = float(line.split()[-1].replace('%', ''))
            elif "Number of reads mapped to too many loci" in line:
                result["too_many_loci"] = int(line.split()[-1])
            elif "Number of reads unmapped: too many mismatches" in line:
                result["too_many_mismatches"] = int(line.split()[-1])
            elif "Number of reads unmapped: too short" in line:
                result["too_short"] = int(line.split()[-1])
            elif "Number of reads unmapped: other" in line:
                result["unmapped_other"] = int(line.split()[-1])
        
        if "input_reads" in result and result["input_reads"] > 0:
            result["total_mapping_rate"] = (result.get("unique_mapped", 0) + result.get("multi_mapped", 0)) / result["input_reads"] * 100
        
        return result
    except Exception as e:
        return {"error": f"è§£æSTAR logå¤±è´¥: {e}"}

def _parse_featurecounts_summary(summary_file_path: str) -> dict:
    """
    è§£æ featureCounts summary æ–‡ä»¶
    """
    try:
        with open(summary_file_path, 'r') as f:
            lines = f.readlines()
        
        result = {}
        for line in lines:
            line = line.strip()
            if line and '\t' in line:
                parts = line.split('\t')
                if len(parts) >= 2:
                    status = parts[0]
                    count = int(parts[1])
                    result[status] = count
        
        # è®¡ç®—æ€»readså’Œassignedæ¯”ä¾‹
        total_reads = sum(result.values())
        assigned_reads = result.get("Assigned", 0)
        
        if total_reads > 0:
            result["assigned_rate"] = assigned_reads / total_reads * 100
            result["total_reads"] = total_reads
        
        return result
    except Exception as e:
        return {"error": f"è§£æfeatureCounts summaryå¤±è´¥: {e}"}

def collect_results_tool(srr_ids: List[str], **kwargs) -> dict:
    """
    æ”¶é›†åˆ†æç»“æœå¹¶è§£æå…³é”®æŒ‡æ ‡
    """
    print(f"Tool 'collect_results_tool' called for SRRs {srr_ids}.")
    project_root = _get_project_root()
    data_dir = project_root / 'data'
    
    # æ”¶é›†å„ç§ç»“æœæ–‡ä»¶
    results = {
        "fastp_results": [],
        "star_results": [],
        "featurecounts_results": {},
        "analysis_summary": {}
    }
    
    # è§£ææ¯ä¸ªæ ·æœ¬çš„ç»“æœ
    for srr_id in srr_ids:
        sample_summary = {"srr_id": srr_id}
        
        # fastp ç»“æœè§£æ
        fastp_dir = data_dir / 'results' / 'fastp' / srr_id
        json_file = fastp_dir / f"{srr_id}.json"
        if json_file.exists():
            fastp_stats = _parse_fastp_json(str(json_file))
            sample_summary["fastp_stats"] = fastp_stats
            results["fastp_results"].append({
                "srr_id": srr_id,
                "html_report": str(fastp_dir / f"{srr_id}.html"),
                "json_report": str(json_file),
                "stats": fastp_stats
            })
        
        # STAR ç»“æœè§£æ
        bam_dir = data_dir / 'results' / 'bam' / srr_id
        log_file = bam_dir / f"{srr_id}.Log.final.out"
        if log_file.exists():
            star_stats = _parse_star_log(str(log_file))
            sample_summary["star_stats"] = star_stats
            results["star_results"].append({
                "srr_id": srr_id,
                "bam_file": str(bam_dir / f"{srr_id}.Aligned.sortedByCoord.out.bam"),
                "log_file": str(log_file),
                "stats": star_stats
            })
        
        results["analysis_summary"][srr_id] = sample_summary
    
    # featureCounts ç»“æœè§£æ
    featurecounts_dir = data_dir / 'results' / 'featurecounts'
    summary_file = featurecounts_dir / "counts.txt.summary"
    if summary_file.exists():
        featurecounts_stats = _parse_featurecounts_summary(str(summary_file))
        results["featurecounts_results"] = {
            "counts_file": str(featurecounts_dir / "counts.txt"),
            "summary_file": str(summary_file),
            "stats": featurecounts_stats
        }
    
    return {
        "status": "success",
        "message": f"ç»“æœæ”¶é›†å’Œè§£æå®Œæˆ",
        "srr_ids": srr_ids,
        "results": results
    }

def generate_report_tool(srr_ids: List[str], **kwargs) -> dict:
    """
    ç”Ÿæˆåˆ†ææŠ¥å‘Š - åŒ…å«è¯¦ç»†çš„ä¸Šæ¸¸åˆ†ææ€»ç»“
    """
    print(f"Tool 'generate_report_tool' called for SRR IDs: {srr_ids}")
    project_root = _get_project_root()
    data_dir = project_root / 'data'
    results_dir = data_dir / 'results'
    
    try:
        # æ”¶é›†å¹¶è§£æç»“æœæ•°æ®
        results_data = collect_results_tool(srr_ids)
        if results_data["status"] != "success":
            return results_data
        
        results = results_data["results"]
        
        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        report_dir = results_dir / 'reports'
        report_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        report_content = f"""
        <html>
        <head>
            <title>RNA-seq åˆ†ææŠ¥å‘Š</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }}
                .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; margin-bottom: 20px; }}
                .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
                .success {{ color: green; }}
                .error {{ color: red; }}
                .warning {{ color: orange; }}
                table {{ border-collapse: collapse; width: 100%; margin: 10px 0; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
                .metric {{ font-weight: bold; color: #333; }}
                .value {{ font-family: monospace; }}
                .summary-box {{ background-color: #f9f9f9; padding: 15px; border-radius: 5px; margin: 10px 0; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>RNA-seq åˆ†ææŠ¥å‘Š</h1>
                <p><strong>åˆ†ææ—¶é—´:</strong> {time.strftime('%Y-%m-%d %H:%M:%S')}</p>
                <p><strong>åˆ†ææ ·æœ¬:</strong> {', '.join(srr_ids)}</p>
            </div>
            
            <div class="section">
                <h2>ğŸ“Š ä¸Šæ¸¸åˆ†ææ€»ç»“</h2>
        """
        
        # æ·»åŠ æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†ç»Ÿè®¡
        for srr_id in srr_ids:
            sample_summary = results["analysis_summary"].get(srr_id, {})
            report_content += f"""
                <div class="summary-box">
                    <h3>æ ·æœ¬: {srr_id}</h3>
            """
            
            # fastp ç»Ÿè®¡
            if "fastp_stats" in sample_summary:
                fastp_stats = sample_summary["fastp_stats"]
                if "error" not in fastp_stats:
                    report_content += f"""
                        <h4>ğŸ” è´¨é‡æ§åˆ¶ (fastp)</h4>
                        <table>
                            <tr><th>æŒ‡æ ‡</th><th>åŸå§‹æ•°æ®</th><th>è¿‡æ»¤å</th></tr>
                            <tr><td>æ€»readsæ•°</td><td class="value">{fastp_stats.get('total_reads', 0):,}</td><td class="value">{fastp_stats.get('clean_reads', 0):,}</td></tr>
                            <tr><td>æ€»ç¢±åŸºæ•°</td><td class="value">{fastp_stats.get('total_bases', 0):,}</td><td class="value">{fastp_stats.get('clean_bases', 0):,}</td></tr>
                            <tr><td>Q20æ¯”ä¾‹</td><td class="value">{fastp_stats.get('q20_rate', 0):.2f}%</td><td class="value">{fastp_stats.get('clean_q20_rate', 0):.2f}%</td></tr>
                            <tr><td>Q30æ¯”ä¾‹</td><td class="value">{fastp_stats.get('q30_rate', 0):.2f}%</td><td class="value">{fastp_stats.get('clean_q30_rate', 0):.2f}%</td></tr>
                            <tr><td>è¿‡æ»¤readsæ•°</td><td colspan="2" class="value">{fastp_stats.get('filtered_reads', 0):,} ({fastp_stats.get('filter_rate', 0):.2f}%)</td></tr>
                        </table>
                    """
                else:
                    report_content += f"<p class='error'>fastpç»Ÿè®¡è§£æå¤±è´¥: {fastp_stats['error']}</p>"
            
            # STAR ç»Ÿè®¡
            if "star_stats" in sample_summary:
                star_stats = sample_summary["star_stats"]
                if "error" not in star_stats:
                    report_content += f"""
                        <h4>ğŸ¯ åºåˆ—æ¯”å¯¹ (STAR)</h4>
                        <table>
                            <tr><th>æŒ‡æ ‡</th><th>æ•°å€¼</th></tr>
                            <tr><td>è¾“å…¥readsæ•°</td><td class="value">{star_stats.get('input_reads', 0):,}</td></tr>
                            <tr><td>å”¯ä¸€æ¯”å¯¹reads</td><td class="value">{star_stats.get('unique_mapped', 0):,} ({star_stats.get('unique_mapping_rate', 0):.2f}%)</td></tr>
                            <tr><td>å¤šé‡æ¯”å¯¹reads</td><td class="value">{star_stats.get('multi_mapped', 0):,}</td></tr>
                            <tr><td>æ€»æ¯”å¯¹ç‡</td><td class="value">{star_stats.get('total_mapping_rate', 0):.2f}%</td></tr>
                            <tr><td>æœªæ¯”å¯¹reads</td><td class="value">{star_stats.get('unmapped', 0):,}</td></tr>
                        </table>
                    """
                else:
                    report_content += f"<p class='error'>STARç»Ÿè®¡è§£æå¤±è´¥: {star_stats['error']}</p>"
            
            report_content += "</div>"
        
        # featureCounts ç»Ÿè®¡
        if "featurecounts_results" in results and "stats" in results["featurecounts_results"]:
            fc_stats = results["featurecounts_results"]["stats"]
            if "error" not in fc_stats:
                report_content += f"""
                    <div class="summary-box">
                        <h3>ğŸ“ˆ åŸºå› å®šé‡ (featureCounts)</h3>
                        <table>
                            <tr><th>çŠ¶æ€</th><th>readsæ•°</th><th>æ¯”ä¾‹</th></tr>
                """
                
                total_reads = fc_stats.get("total_reads", 0)
                for status, count in fc_stats.items():
                    if status not in ["total_reads", "assigned_rate"]:
                        percentage = (count / total_reads * 100) if total_reads > 0 else 0
                        report_content += f"<tr><td>{status}</td><td class='value'>{count:,}</td><td class='value'>{percentage:.2f}%</td></tr>"
                
                report_content += f"""
                            <tr><td><strong>æ€»reads</strong></td><td class='value'><strong>{total_reads:,}</strong></td><td class='value'><strong>100%</strong></td></tr>
                        </table>
                        <p><strong>åŸºå› åˆ†é…ç‡:</strong> <span class='value'>{fc_stats.get('assigned_rate', 0):.2f}%</span></p>
                    </div>
                """
            else:
                report_content += f"<p class='error'>featureCountsç»Ÿè®¡è§£æå¤±è´¥: {fc_stats['error']}</p>"
        
        report_content += """
            </div>
            
            <div class="section">
                <h2>âœ… åˆ†ææ­¥éª¤å®Œæˆæƒ…å†µ</h2>
                <ul>
                    <li class="success">âœ“ æ•°æ®ä¸‹è½½å’ŒéªŒè¯</li>
                    <li class="success">âœ“ è´¨é‡æ§åˆ¶ (fastp)</li>
                    <li class="success">âœ“ åºåˆ—æ¯”å¯¹ (STAR)</li>
                    <li class="success">âœ“ åŸºå› å®šé‡ (featureCounts)</li>
                </ul>
            </div>
            
            <div class="section">
                <h2>ğŸ“ ç»“æœæ–‡ä»¶ä½ç½®</h2>
                <ul>
                    <li><strong>è´¨é‡æ§åˆ¶ç»“æœ:</strong> <code>data/results/fastp/</code></li>
                    <li><strong>æ¯”å¯¹ç»“æœ:</strong> <code>data/results/bam/</code></li>
                    <li><strong>å®šé‡ç»“æœ:</strong> <code>data/results/featurecounts/</code></li>
                    <li><strong>åˆ†ææŠ¥å‘Š:</strong> <code>data/results/reports/</code></li>
                </ul>
            </div>
            
            <div class="section">
                <h2>ğŸ’¡ è´¨é‡è¯„ä¼°å»ºè®®</h2>
                <ul>
                    <li><strong>æµ‹åºè´¨é‡:</strong> Q30æ¯”ä¾‹åº” > 80%ï¼ŒQ20æ¯”ä¾‹åº” > 90%</li>
                    <li><strong>æ¯”å¯¹è´¨é‡:</strong> å”¯ä¸€æ¯”å¯¹ç‡åº” > 70%ï¼Œæ€»æ¯”å¯¹ç‡åº” > 80%</li>
                    <li><strong>å®šé‡è´¨é‡:</strong> åŸºå› åˆ†é…ç‡åº” > 60%</li>
                </ul>
            </div>
        </body>
        </html>
        """
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = report_dir / f"report_{'_'.join(srr_ids)}.html"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        return {
            "status": "success",
            "message": "åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆï¼ŒåŒ…å«è¯¦ç»†çš„ä¸Šæ¸¸åˆ†ææ€»ç»“",
            "report_file": str(report_file),
            "srr_ids": srr_ids,
            "results_summary": results["analysis_summary"]
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"ç”ŸæˆæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {e}"
        }

# --- Reactæ¨¡å¼ä¸“ç”¨å·¥å…· ---

def react_status_tool(**kwargs) -> dict:
    """
    Reactæ¨¡å¼çŠ¶æ€æŸ¥è¯¢å·¥å…·
    """
    print("Tool 'react_status_tool' called.")
    
    return {
        "status": "success",
        "react_mode": True,
        "current_cycle": kwargs.get("current_cycle", 0),
        "max_cycles": kwargs.get("max_cycles", 10),
        "available_tools": list(ENVIRONMENT_TOOLS.keys()) + [
            "search_genome_tool", "search_fastq_tool", "download_genome_tool",
            "download_fastq_tool", "validate_fastq_tool", "build_star_index_tool",
            "run_fastp_tool", "run_star_align_tool", "run_featurecounts_tool",
            "collect_results_tool", "generate_report_tool"
        ],
        "tool_dependencies": TOOL_DEPENDENCIES
    }

def validate_tool_call_format(tool_name: str, arguments: str) -> dict:
    """
    éªŒè¯å·¥å…·è°ƒç”¨æ ¼å¼æ˜¯å¦æ­£ç¡®
    """
    print(f"Tool 'validate_tool_call_format' called for tool '{tool_name}'.")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç¦æ­¢çš„æ ¼å¼
    forbidden_patterns = [
        "REDACTED_SPECIAL_TOKEN",
        "<JSON>",
        "function",
        "```"
    ]
    
    errors = []
    for pattern in forbidden_patterns:
        if pattern in arguments:
            errors.append(f"æ£€æµ‹åˆ°ç¦æ­¢çš„æ ¼å¼: {pattern}")
    
    if errors:
        return {
            "status": "error",
            "message": "å·¥å…·è°ƒç”¨æ ¼å¼é”™è¯¯",
            "errors": errors,
            "tool_name": tool_name,
            "arguments": arguments
        }
    
    # å°è¯•è§£æJSON
    try:
        import json
        parsed_args = json.loads(arguments)
        return {
            "status": "success",
            "message": "å·¥å…·è°ƒç”¨æ ¼å¼æ­£ç¡®",
            "tool_name": tool_name,
            "arguments": parsed_args
        }
    except json.JSONDecodeError as e:
        return {
            "status": "error",
            "message": f"JSONè§£æé”™è¯¯: {e}",
            "tool_name": tool_name,
            "arguments": arguments
        }


def check_files_exist_tool(file_type: str, **kwargs) -> dict:
    """
    é€šç”¨æ–‡ä»¶éªŒè¯å·¥å…· - æ£€æŸ¥å„ç§ç±»å‹çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    """
    print(f"Tool 'check_files_exist_tool' called for file_type '{file_type}'.")
    
    project_root = _get_project_root()
    work_dir = project_root / 'data'
    genomes_config = _get_genomes_config()
    
    results = {
        "file_type": file_type,
        "exists": False,
        "details": {},
        "message": ""
    }
    
    try:
        if file_type == "star_index":
            # æ£€æŸ¥STARç´¢å¼•
            genome_name = kwargs.get('genome_name')
            if not genome_name:
                return {
                    "status": "error",
                    "message": "æ£€æŸ¥STARç´¢å¼•éœ€è¦æä¾›genome_nameå‚æ•°"
                }
            
            genome_info = genomes_config.get(genome_name)
            if not genome_info:
                return {
                    "status": "error",
                    "message": f"åŸºå› ç»„ '{genome_name}' åœ¨é…ç½®ä¸­æœªæ‰¾åˆ°"
                }
            
            star_index_path = project_root / genome_info['fasta'].replace('.fa', '/star_index')
            exists = star_index_path.is_dir() and (star_index_path / 'SA').is_file()
            
            results.update({
                "exists": exists,
                "file_path": str(star_index_path),
                "details": {
                    "index_dir": str(star_index_path),
                    "has_sa_file": (star_index_path / 'SA').is_file() if star_index_path.is_dir() else False
                },
                "message": f"STARç´¢å¼•æ£€æŸ¥å®Œæˆ - {'å­˜åœ¨' if exists else 'ä¸å­˜åœ¨'}"
            })
            
        elif file_type == "gtf_file":
            # æ£€æŸ¥GTFæ–‡ä»¶
            genome_name = kwargs.get('genome_name')
            if not genome_name:
                return {
                    "status": "error",
                    "message": "æ£€æŸ¥GTFæ–‡ä»¶éœ€è¦æä¾›genome_nameå‚æ•°"
                }
            
            genome_info = genomes_config.get(genome_name)
            if not genome_info:
                return {
                    "status": "error",
                    "message": f"åŸºå› ç»„ '{genome_name}' åœ¨é…ç½®ä¸­æœªæ‰¾åˆ°"
                }
            
            gtf_path = project_root / genome_info['gtf']
            exists = gtf_path.is_file()
            
            results.update({
                "exists": exists,
                "file_path": str(gtf_path),
                "file_size": gtf_path.stat().st_size if exists else 0,
                "message": f"GTFæ–‡ä»¶æ£€æŸ¥å®Œæˆ - {'å­˜åœ¨' if exists else 'ä¸å­˜åœ¨'}"
            })
            
        elif file_type == "bam_files":
            # æ£€æŸ¥BAMæ–‡ä»¶
            srr_ids = kwargs.get('srr_ids')
            if not srr_ids:
                return {
                    "status": "error",
                    "message": "æ£€æŸ¥BAMæ–‡ä»¶éœ€è¦æä¾›srr_idså‚æ•°"
                }
            
            bam_results = {}
            all_exist = True
            
            for srr_id in srr_ids:
                bam_file = work_dir / 'results' / 'bam' / srr_id / f"{srr_id}.bam"
                exists = bam_file.is_file()
                bam_results[srr_id] = {
                    "bam_path": str(bam_file),
                    "exists": exists,
                    "file_size": bam_file.stat().st_size if exists else 0
                }
                if not exists:
                    all_exist = False
            
            results.update({
                "exists": all_exist,
                "srr_ids": srr_ids,
                "details": bam_results,
                "message": f"BAMæ–‡ä»¶æ£€æŸ¥å®Œæˆ - {'å…¨éƒ¨å­˜åœ¨' if all_exist else 'éƒ¨åˆ†ç¼ºå¤±'}"
            })
            
        elif file_type == "fastq_files":
            # æ£€æŸ¥FASTQæ–‡ä»¶
            srr_ids = kwargs.get('srr_ids')
            if not srr_ids:
                return {
                    "status": "error",
                    "message": "æ£€æŸ¥FASTQæ–‡ä»¶éœ€è¦æä¾›srr_idså‚æ•°"
                }
            
            fastq_results = {}
            all_exist = True
            
            for srr_id in srr_ids:
                r1_path = work_dir / 'fastq' / f"{srr_id}_1.fastq.gz"
                r2_path = work_dir / 'fastq' / f"{srr_id}_2.fastq.gz"
                r1_exists = r1_path.is_file()
                r2_exists = r2_path.is_file()
                
                fastq_results[srr_id] = {
                    "r1_path": str(r1_path),
                    "r2_path": str(r2_path),
                    "r1_exists": r1_exists,
                    "r2_exists": r2_exists,
                    "is_paired": r1_exists and r2_exists
                }
                
                if not (r1_exists and r2_exists):
                    all_exist = False
            
            results.update({
                "exists": all_exist,
                "srr_ids": srr_ids,
                "details": fastq_results,
                "message": f"FASTQæ–‡ä»¶æ£€æŸ¥å®Œæˆ - {'å…¨éƒ¨å­˜åœ¨' if all_exist else 'éƒ¨åˆ†ç¼ºå¤±'}"
            })
            
        elif file_type == "fastp_results":
            # æ£€æŸ¥FASTPç»“æœæ–‡ä»¶
            srr_ids = kwargs.get('srr_ids')
            if not srr_ids:
                return {
                    "status": "error",
                    "message": "æ£€æŸ¥FASTPç»“æœéœ€è¦æä¾›srr_idså‚æ•°"
                }
            
            fastp_results = {}
            all_exist = True
            
            for srr_id in srr_ids:
                html_file = work_dir / 'results' / 'fastp' / srr_id / f"{srr_id}.html"
                json_file = work_dir / 'results' / 'fastp' / srr_id / f"{srr_id}.json"
                html_exists = html_file.is_file()
                json_exists = json_file.is_file()
                
                fastp_results[srr_id] = {
                    "html_path": str(html_file),
                    "json_path": str(json_file),
                    "html_exists": html_exists,
                    "json_exists": json_exists,
                    "complete": html_exists and json_exists
                }
                
                if not (html_exists and json_exists):
                    all_exist = False
            
            results.update({
                "exists": all_exist,
                "srr_ids": srr_ids,
                "details": fastp_results,
                "message": f"FASTPç»“æœæ£€æŸ¥å®Œæˆ - {'å…¨éƒ¨å­˜åœ¨' if all_exist else 'éƒ¨åˆ†ç¼ºå¤±'}"
            })
            
        elif file_type == "counts_file":
            # æ£€æŸ¥featureCountsç»“æœæ–‡ä»¶
            counts_file = work_dir / 'results' / 'featurecounts' / 'counts.txt'
            exists = counts_file.is_file()
            
            results.update({
                "exists": exists,
                "file_path": str(counts_file),
                "file_size": counts_file.stat().st_size if exists else 0,
                "message": f"featureCountsç»“æœæ£€æŸ¥å®Œæˆ - {'å­˜åœ¨' if exists else 'ä¸å­˜åœ¨'}"
            })
            
        else:
            return {
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}ã€‚æ”¯æŒçš„ç±»å‹: star_index, gtf_file, bam_files, fastq_files"
            }
        
        return {
            "status": "success",
            **results
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": f"æ£€æŸ¥æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}"
        }

def start_analysis_tool(plan: str) -> dict:
    """
    ç¡®è®¤åˆ†æè®¡åˆ’å¹¶è§¦å‘åˆ†æçŠ¶æ€çš„è½¬æ¢ã€‚
    è¿™ä¸ªå·¥å…·æ˜¯ä¸€ä¸ªä¿¡å·ï¼Œå‘Šè¯‰æœåŠ¡å™¨ä» CONVERSING åˆ‡æ¢åˆ° ANALYZING çŠ¶æ€ã€‚
    """
    print(f"Tool 'start_analysis_tool' called with plan: {plan}")
    return {
        "status": "success",
        "message": f"åˆ†æè®¡åˆ’å·²ç¡®è®¤ï¼Œå‡†å¤‡å¼€å§‹æ‰§è¡Œã€‚è®¡åˆ’è¯¦æƒ…: {plan}"
    }

def run_nextflow_pipeline(
    srr_ids: str = "",
    local_genome_path: str = "",
    local_gtf_path: str = "",
    download_genome_url: str = "",
    download_gtf_url: str = "",
    fastq_dir: str = "",
    fastq_pattern: str = "*_{1,2}.fastq.gz",
    data: str = "./data",
    run_download_srr: bool = False,
    run_download_genome: bool = False,
    run_build_star_index: bool = False,
    run_fastp: bool = False,
    run_star_align: bool = False,
    run_featurecounts: bool = False,
    resume: bool = False,
    star_overhang: int = 100,
    star_threads: int = 4,
    fastp_threads: int = 4,
    featurecounts_threads: int = 4,
    **kwargs
) -> dict:
    """
    è¿è¡Œ Nextflow RNA-seq åˆ†æå·¥ä½œæµã€‚
    æ­¤å‡½æ•°åŠ¨æ€æ„å»ºå¹¶æ‰§è¡Œ 'nextflow run main.nf' å‘½ä»¤ã€‚
    """
    print("Tool 'run_nextflow_pipeline' called.")
    project_root = _get_project_root()
    main_nf_path = project_root / 'main.nf'

    if not main_nf_path.exists():
        return {
            "status": "error",
            "message": f"Nextflow ä¸»æ–‡ä»¶æœªæ‰¾åˆ°: {main_nf_path}"
        }

    # æ„å»º Nextflow å‘½ä»¤
    command = ["nextflow", "run", str(main_nf_path), "-with-trace"]

    # --- æ·»åŠ æ‰€æœ‰å‚æ•° ---
    if srr_ids:
        command.extend(["--srr_ids", srr_ids])
    if local_genome_path:
        command.extend(["--local_genome_path", local_genome_path])
    if local_gtf_path:
        command.extend(["--local_gtf_path", local_gtf_path])
    if download_genome_url:
        command.extend(["--download_genome_url", download_genome_url])
    if download_gtf_url:
        command.extend(["--download_gtf_url", download_gtf_url])
    if fastq_dir:
        command.extend(["--fastq_dir", fastq_dir])
    if fastq_pattern:
        command.extend(["--fastq_pattern", fastq_pattern])
    if data:
        command.extend(["--data", data])

    # --- æ·»åŠ å¸ƒå°”æ§åˆ¶å‚æ•° ---
    if run_download_srr:
        command.append("--run_download_srr")
    if run_download_genome:
        command.append("--run_download_genome")
    if run_build_star_index:
        command.append("--run_build_star_index")
    if run_fastp:
        command.append("--run_fastp")
    if run_star_align:
        command.append("--run_star_align")
    if run_featurecounts:
        command.append("--run_featurecounts")
    
    # --- æ·»åŠ å…¶ä»–å‚æ•° ---
    if resume:
        command.append("-resume") # Nextflow resume flag
    command.extend(["--star_overhang", str(star_overhang)])
    command.extend(["--star_threads", str(star_threads)])
    command.extend(["--fastp_threads", str(fastp_threads)])
    command.extend(["--featurecounts_threads", str(featurecounts_threads)])

    print(f"æ‰§è¡Œ Nextflow å‘½ä»¤: {' '.join(command)}")

    try:
        # è®¾ç½®ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        # ç¡®ä¿ NXF_HOME è®¾ç½®åœ¨é¡¹ç›®ç›®å½•å†…ï¼Œä»¥é¿å…æƒé™é—®é¢˜
        env['NXF_HOME'] = str(project_root / '.nextflow')
        
        # æ‰§è¡Œå‘½ä»¤
        result = subprocess.run(
            command,
            capture_output=True,
            text=True,
            env=env,
            cwd=project_root  # åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œ
        )

        if result.returncode == 0:
            return {
                "status": "success",
                "message": "Nextflow å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ",
                "stdout": result.stdout,
                "stderr": result.stderr,
                "command": " ".join(command),
            }
        else:
            return {
                "status": "error",
                "message": "Nextflow å·¥ä½œæµæ‰§è¡Œå¤±è´¥",
                "stdout": result.stdout,
                "stderr": result.stderr,
                "command": " ".join(command),
            }
    except FileNotFoundError:
        return {
            "status": "error",
            "message": "Nextflow å‘½ä»¤æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿ Nextflow å·²å®‰è£…å¹¶åœ¨ PATH ä¸­ã€‚"
        }
    except subprocess.TimeoutExpired:
        return {
            "status": "error",
            "message": "Nextflow å·¥ä½œæµæ‰§è¡Œè¶…æ—¶ã€‚"
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"è¿è¡Œ Nextflow å·¥ä½œæµæ—¶å‘ç”Ÿé”™è¯¯: {e}"
        }

def _setup_pipeline_logger() -> logging.Logger:
    """
    è®¾ç½® RNA-seq æµç¨‹ä¸“ç”¨çš„æ—¥å¿—è®°å½•å™¨
    """
    project_root = _get_project_root()
    log_file = project_root / 'para.log.txt'
    
    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    logger = logging.getLogger('rna_seq_pipeline')
    logger.setLevel(logging.INFO)
    
    # é¿å…é‡å¤æ·»åŠ å¤„ç†å™¨
    if not logger.handlers:
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter(
            '%(asctime)s - [RNA-seq Pipeline] - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(formatter)
        
        # æ·»åŠ å¤„ç†å™¨åˆ°æ—¥å¿—è®°å½•å™¨
        logger.addHandler(file_handler)
    
    return logger

def execute_rna_seq_pipeline(
    srr_ids: str = "",
    local_genome_path: str = "",
    local_gtf_path: str = "",
    download_genome_url: str = "",
    download_gtf_url: str = "",
    fastq_dir: str = "",
    fastq_pattern: str = "*_{1,2}.fastq.gz",
    data: str = "./data",
    run_download_srr: bool = False,
    run_download_genome: bool = False,
    run_build_star_index: bool = False,
    run_fastp: bool = False,
    run_star_align: bool = False,
    run_featurecounts: bool = False,
    resume: bool = False,
    star_overhang: int = 100,
    star_threads: int = 4,
    fastp_threads: int = 4,
    featurecounts_threads: int = 4,
    **kwargs
) -> dict:
    """
    åœ¨ ANALYZING çŠ¶æ€ä¸‹ï¼Œè¿è¡Œå®Œæ•´çš„ RNA-seq åˆ†æå·¥ä½œæµã€‚
    æ­¤å·¥å…·æ¥æ”¶æ‰€æœ‰å¿…è¦çš„å‚æ•°ï¼Œå¹¶ä¸€æ¬¡æ€§å¯åŠ¨ Nextflow æµç¨‹ã€‚
    """
    # è®¾ç½®æ—¥å¿—è®°å½•å™¨
    logger = _setup_pipeline_logger()
    
    # è®°å½•å‡½æ•°å¼€å§‹æ‰§è¡Œå’Œå‚æ•°ä¿¡æ¯
    logger.info("=" * 80)
    logger.info("å¼€å§‹æ‰§è¡Œ RNA-seq åˆ†ææµç¨‹")
    logger.info(f"æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("è¾“å…¥å‚æ•°:")
    logger.info(f"  - srr_ids: {srr_ids}")
    logger.info(f"  - local_genome_path: {local_genome_path}")
    logger.info(f"  - local_gtf_path: {local_gtf_path}")
    logger.info(f"  - download_genome_url: {download_genome_url}")
    logger.info(f"  - download_gtf_url: {download_gtf_url}")
    logger.info(f"  - fastq_dir: {fastq_dir}")
    logger.info(f"  - fastq_pattern: {fastq_pattern}")
    logger.info(f"  - data: {data}")
    logger.info(f"  - run_download_srr: {run_download_srr}")
    logger.info(f"  - run_download_genome: {run_download_genome}")
    logger.info(f"  - run_build_star_index: {run_build_star_index}")
    logger.info(f"  - run_fastp: {run_fastp}")
    logger.info(f"  - run_star_align: {run_star_align}")
    logger.info(f"  - run_featurecounts: {run_featurecounts}")
    logger.info(f"  - resume: {resume}")
    logger.info(f"  - star_overhang: {star_overhang}")
    logger.info(f"  - star_threads: {star_threads}")
    logger.info(f"  - fastp_threads: {fastp_threads}")
    logger.info(f"  - featurecounts_threads: {featurecounts_threads}")
    
    print(f"Tool 'execute_rna_seq_pipeline' called.")
    
    try:
        # è®°å½•å¼€å§‹è°ƒç”¨ Nextflow å·¥ä½œæµ
        logger.info("å¼€å§‹è°ƒç”¨ run_nextflow_pipeline å‡½æ•°")
        
        # è°ƒç”¨ Nextflow å·¥ä½œæµ
        result = run_nextflow_pipeline(
            srr_ids=srr_ids,
            local_genome_path=local_genome_path,
            local_gtf_path=local_gtf_path,
            download_genome_url=download_genome_url,
            download_gtf_url=download_gtf_url,
            fastq_dir=fastq_dir,
            fastq_pattern=fastq_pattern,
            data=data,
            run_download_srr=run_download_srr,
            run_download_genome=run_download_genome,
            run_build_star_index=run_build_star_index,
            run_fastp=run_fastp,
            run_star_align=run_star_align,
            run_featurecounts=run_featurecounts,
            resume=resume,
            star_overhang=star_overhang,
            star_threads=star_threads,
            fastp_threads=fastp_threads,
            featurecounts_threads=featurecounts_threads,
            **kwargs
        )
        
        # è®°å½•æ‰§è¡Œç»“æœ
        if result.get('status') == 'success':
            logger.info("âœ… run_nextflow_pipeline æ‰§è¡ŒæˆåŠŸ")
            logger.info(f"æ‰§è¡Œçš„å‘½ä»¤: {result.get('command', 'N/A')}")
            if result.get('stdout'):
                logger.info("æ ‡å‡†è¾“å‡º:")
                for line in result['stdout'].split('\n'):
                    if line.strip():
                        logger.info(f"  {line}")
        else:
            logger.error("âŒ run_nextflow_pipeline æ‰§è¡Œå¤±è´¥")
            logger.error(f"é”™è¯¯ä¿¡æ¯: {result.get('message', 'N/A')}")
            logger.error(f"æ‰§è¡Œçš„å‘½ä»¤: {result.get('command', 'N/A')}")
            if result.get('stderr'):
                logger.error("æ ‡å‡†é”™è¯¯è¾“å‡º:")
                for line in result['stderr'].split('\n'):
                    if line.strip():
                        logger.error(f"  {line}")
        
        # è®°å½•å‡½æ•°å®ŒæˆçŠ¶æ€
        logger.info(f"RNA-seq åˆ†ææµç¨‹æ‰§è¡Œå®Œæˆï¼ŒçŠ¶æ€: {result.get('status', 'unknown')}")
        logger.info("=" * 80)
        
        return result
        
    except Exception as e:
        # è®°å½•å¼‚å¸¸ä¿¡æ¯
        logger.error(f"âŒ æ‰§è¡Œ RNA-seq åˆ†ææµç¨‹æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
        logger.error(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
        logger.error("=" * 80)
        
        # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä¿æŒåŸæœ‰çš„é”™è¯¯å¤„ç†é€»è¾‘
        raise e