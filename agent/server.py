# agent/server.py

import json
import time
import asyncio
import os
import tempfile
import re
import json
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, AsyncGenerator, Dict, Any
from threading import Lock
from fastapi.middleware.cors import CORSMiddleware

# --- æ–°å¢: LLM å’Œç¯å¢ƒç›¸å…³å¯¼å…¥ ---
import openai
from dotenv import load_dotenv

# --- æ–°å¢: ä»æˆ‘ä»¬çš„æ¨¡å—å¯¼å…¥ ---
from agent.prompt import SYSTEM_PROMPT, TOOLS
import agent.tools as tool_module # å¯¼å…¥æ•´ä¸ªæ¨¡å—ä»¥ä¾¿äºå‡½æ•°æŸ¥æ‰¾

# --- 1. å®šä¹‰æ•°æ®æ¨¡å‹ ---
class StandardMessage(BaseModel):
    role: str
    content: str

class ChatInput(BaseModel):
    messages: List[StandardMessage]


# --- 2. åˆ›å»º FastAPI åº”ç”¨å®ä¾‹ ---
app = FastAPI(
    title="LLM é©±åŠ¨çš„æµå¼ Agent æœåŠ¡å™¨",
    description="è¿™æ˜¯ä¸€ä¸ªç”±å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨ã€æ”¯æŒå·¥å…·è°ƒç”¨ã€ç¬¦åˆOpenAIæ¥å£æ ‡å‡†çš„AgentæœåŠ¡å™¨ã€‚",
    version="1.0.0", # ç‰ˆæœ¬è·ƒè¿
)

# --- 3. é…ç½®å’Œåˆå§‹åŒ– ---
# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
# è¿™åº”è¯¥åœ¨è®¿é—®ä»»ä½•ç¯å¢ƒå˜é‡ä¹‹å‰å®Œæˆ
# åŠ è½½é¡¹ç›®æ ¹ç›®å½•çš„ .env æ–‡ä»¶ (å¦‚æœå­˜åœ¨)ï¼Œ
# è¿™ä½¿å¾—åœ¨å®¹å™¨å¤–æœ¬åœ°è¿è¡Œä¹Ÿæˆä¸ºå¯èƒ½ã€‚
# åœ¨å®¹å™¨å†…ï¼Œè¿™äº›å˜é‡ä¸»è¦ç”± docker-compose çš„ env_file æŒ‡ä»¤æ³¨å…¥ã€‚
load_dotenv()

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
# ä»ç¯å¢ƒå˜é‡ä¸­è·å–é…ç½®
api_key = os.getenv("OPENAI_API_KEY")
base_url = os.getenv("OPENAI_API_BASE")
model_name = os.getenv("OPENAI_MODEL_NAME", "default-model") 

if not api_key or not base_url:
    raise ValueError("è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•çš„ .env æ–‡ä»¶ä¸­è®¾ç½® OPENAI_API_KEY å’Œ OPENAI_API_BASE")

client = openai.OpenAI(
    api_key=api_key,
    base_url=base_url,
)

# --- 3. æ·»åŠ  CORS ä¸­é—´ä»¶ ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- 3.5. å…¨å±€ä»»åŠ¡æ•°æ®åº“ (å†…å­˜ä¸­) ---
# ä½¿ç”¨å­—å…¸æ¥å­˜å‚¨ä»»åŠ¡çŠ¶æ€ã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¿™åº”è¯¥è¢«æ›¿æ¢ä¸ºçœŸæ­£çš„æ•°æ®åº“ã€‚
TASK_DATABASE: Dict[str, Dict[str, Any]] = {}
# ä½¿ç”¨é”æ¥ç¡®ä¿å¯¹ TASK_DATABASE çš„çº¿ç¨‹å®‰å…¨è®¿é—®
db_lock = Lock()
# ä»»åŠ¡ ID è®¡æ•°å™¨
task_id_counter = 1


# --- 4. å®šä¹‰ API ç«¯ç‚¹ ---

@app.get("/")
def read_root():
    """æ ¹è·¯å¾„ï¼Œç”¨äºæ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦åœ¨çº¿ã€‚"""
    return {"message": "ä½ å¥½ï¼Œæµå¼ Agent æœåŠ¡å™¨æ­£åœ¨è¿è¡Œï¼"}

@app.get("/models")
async def list_models():
    """
    å“åº”å®¢æˆ·ç«¯è·å–æ¨¡å‹åˆ—è¡¨çš„è¯·æ±‚ã€‚
    ç°åœ¨æ¨¡å‹åˆ—è¡¨ç›´æ¥ä»ç¯å¢ƒå˜é‡ä¸­è¯»å–ã€‚
    """
    return {
        "object": "list",
        "data": [
            {
                "id": model_name,
                "object": "model",
                "created": int(time.time()),
                "owned_by": "system",
            }
        ],
    }

async def stream_agent_response(chat_input: ChatInput) -> AsyncGenerator[str, None]:
    """
    ä¸€ä¸ªç”± LLM é©±åŠ¨çš„ã€æ”¯æŒå·¥å…·è°ƒç”¨çš„ Agent å“åº”ç”Ÿæˆå™¨ã€‚
    ç°åœ¨æ”¯æŒReactæ¨¡å¼ï¼šæ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿå¾ªç¯ã€‚
    """
    # æ£€æŸ¥æ˜¯å¦ä¸ºé¦–æ¬¡äº¤äº’ï¼ˆåªæœ‰ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä¸”æ²¡æœ‰å†å²è®°å½•ï¼‰
    is_first_interaction = len(chat_input.messages) == 1 and chat_input.messages[0].role == "user"
    
    # 1. å‡†å¤‡å‘é€ç»™ LLM çš„æ¶ˆæ¯ï¼Œç¡®ä¿æˆ‘ä»¬çš„ç³»ç»Ÿæç¤ºæ˜¯å”¯ä¸€çš„
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    # è¿‡æ»¤æ‰ä»»ä½•å¯èƒ½ä»ä¸Šæ¸¸ä¼ å…¥çš„ system æ¶ˆæ¯ï¼Œåªä¿ç•™ user å’Œ assistant çš„æ¶ˆæ¯
    for msg in chat_input.messages:
        if msg.role != "system":
            messages.append({"role": msg.role, "content": msg.content})
    
    # å¦‚æœæ˜¯é¦–æ¬¡äº¤äº’ï¼Œå‘é€æ¬¢è¿ä¿¡æ¯
    if is_first_interaction:
        welcome_message = """ğŸ‘‹ æ¬¢è¿ä½¿ç”¨RNA-seq åˆ†æå¹³å°

æ‚¨å¯ä»¥è¿™æ ·å¼€å§‹ï¼š
1. è¾“å…¥SRRå·å’Œå‚è€ƒåŸºå› ç»„åï¼Œä¾‹å¦‚ï¼š`å¸®æˆ‘åˆ†æSRR17469059 åŸºå› ç»„ç”¨mm10`
2. ç³»ç»Ÿä¼šè‡ªåŠ¨å®Œæˆæ•°æ®ä¸‹è½½ã€è´¨é‡æ§åˆ¶ã€æ¯”å¯¹ã€å®šé‡å’ŒæŠ¥å‘Šç”Ÿæˆã€‚
3. æ”¯æŒæ™ºèƒ½è·³è¿‡å·²å®Œæˆæ­¥éª¤ï¼ŒèŠ‚çœè®¡ç®—èµ„æºã€‚

å¸¸ç”¨å‘½ä»¤ï¼š
- æŸ¥çœ‹å¯ç”¨åŸºå› ç»„ï¼š`åˆ—å‡ºå¯ç”¨åŸºå› ç»„`
- æ·»åŠ æ–°åŸºå› ç»„ï¼š`æ·»åŠ åŸºå› ç»„ mm10 ç‰©ç§ mouse ...`
- æŸ¥è¯¢åˆ†æè¿›åº¦ï¼š`æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€`

ç¥æ‚¨åˆ†æé¡ºåˆ©ï¼"""
        
        # å‘é€æ¬¢è¿ä¿¡æ¯
        welcome_chunk = {
            "id": "welcome",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": "welcome",
            "choices": [{"index": 0, "delta": {"content": welcome_message}, "finish_reason": None}]
        }
        yield f"data: {json.dumps(welcome_chunk)}\n\n"
        
        # æ·»åŠ æ¬¢è¿ä¿¡æ¯åˆ°æ¶ˆæ¯å†å²
        messages.append({"role": "assistant", "content": welcome_message})

    # Reactæ¨¡å¼çŠ¶æ€è·Ÿè¸ª
    react_cycle_count = 0
    max_react_cycles = 100  # é˜²æ­¢æ— é™å¾ªç¯
    
    while react_cycle_count < max_react_cycles:
        react_cycle_count += 1
        
        # 2. è°ƒç”¨ LLMï¼Œè®©å®ƒè¿›è¡Œæ€è€ƒå¹¶å†³å®šè¡ŒåŠ¨
        print(f"--- Reactå¾ªç¯ {react_cycle_count}: è°ƒç”¨ LLM (æ¨¡å‹: {model_name}) ---")
        print(f"å‘é€çš„æ¶ˆæ¯: {messages}")
        print(f"å¯ç”¨çš„å·¥å…·: {TOOLS}")
        
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=messages,
                tools=TOOLS,
                temperature=0.0
            )
        except Exception as e:
            print(f"è°ƒç”¨ LLM API æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
            yield "data: [DONE]\n\n"
            return

        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

            # 3. å‘é€æ€è€ƒé˜¶æ®µçš„ä¿¡æ¯
        thought_chunk = {
            "id": response.id, 
            "object": "chat.completion.chunk", 
            "created": response.created, 
            "model": response.model,
            "choices": [{"index": 0, "delta": {"content": f"ğŸ¤” æ€è€ƒé˜¶æ®µ (å¾ªç¯ {react_cycle_count}): LLMæ­£åœ¨åˆ†æå½“å‰æƒ…å†µå¹¶åˆ¶å®šè¡ŒåŠ¨è®¡åˆ’...\n"}, "finish_reason": None}]
            }
        yield f"data: {json.dumps(thought_chunk)}\n\n"
            
            # å¼ºåˆ¶ç«‹å³æ˜¾ç¤º
        yield f"data: {json.dumps({'id': response.id, 'object': 'chat.completion.chunk', 'created': response.created, 'model': response.model, 'choices': [{'index': 0, 'delta': {'content': ''}, 'finish_reason': None}]})}\n\n"

        # 4. æ£€æŸ¥ LLM æ˜¯å¦å†³å®šè°ƒç”¨å·¥å…·
        if tool_calls:
            print(f"--- Reactå¾ªç¯ {react_cycle_count}: LLM å†³å®šè°ƒç”¨å·¥å…·: {tool_calls} ---")
            messages.append(response_message)  # å°† assistant çš„å›å¤ï¼ˆåŒ…æ‹¬å·¥å…·è°ƒç”¨è¯·æ±‚ï¼‰æ·»åŠ åˆ°å†å²è®°å½•ä¸­

            # 5. æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨ï¼ˆè¡ŒåŠ¨é˜¶æ®µï¼‰
            action_chunk = {
                "id": response.id, 
                "object": "chat.completion.chunk", 
                "created": response.created, 
                "model": response.model,
                "choices": [{"index": 0, "delta": {"content": f"ğŸ”§ è¡ŒåŠ¨é˜¶æ®µ (å¾ªç¯ {react_cycle_count}): æ‰§è¡Œ {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨..."}, "finish_reason": None}]
            }
            yield f"data: {json.dumps(action_chunk)}\n\n"

            for tool_call in tool_calls:
                function_name = tool_call.function.name
                
                try:
                    function_args = json.loads(tool_call.function.arguments)
                except json.JSONDecodeError:
                    print(f"âŒ JSONè§£æé”™è¯¯: {tool_call.function.arguments}")
                    function_response = f"é”™è¯¯: LLM è¿”å›äº†æ— æ•ˆçš„ JSON å‚æ•°: {tool_call.function.arguments}"
                    messages.append({"tool_call_id": tool_call.id, "role": "tool", "name": function_name, "content": function_response})
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹æ®Štokenæˆ–é”™è¯¯æ ¼å¼
                if "REDACTED_SPECIAL_TOKEN" in tool_call.function.arguments:
                    print(f"âŒ æ£€æµ‹åˆ°ç‰¹æ®Štoken: {tool_call.function.arguments}")
                    function_response = f"é”™è¯¯: æ£€æµ‹åˆ°ç‰¹æ®Štokenï¼Œè¯·ä½¿ç”¨æ ‡å‡†çš„å·¥å…·è°ƒç”¨æ ¼å¼"
                    messages.append({"tool_call_id": tool_call.id, "role": "tool", "name": function_name, "content": function_response})
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æœ‰functionæ ‡ç­¾
                if "function" in tool_call.function.arguments:
                    print(f"âŒ æ£€æµ‹åˆ°functionæ ‡ç­¾: {tool_call.function.arguments}")
                    function_response = f"é”™è¯¯: æ£€æµ‹åˆ°functionæ ‡ç­¾ï¼Œè¯·ä½¿ç”¨æ ‡å‡†çš„å·¥å…·è°ƒç”¨æ ¼å¼"
                    messages.append({"tool_call_id": tool_call.id, "role": "tool", "name": function_name, "content": function_response})
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æœ‰JSONæ ‡ç­¾
                if "<JSON>" in tool_call.function.arguments:
                    print(f"âŒ æ£€æµ‹åˆ°JSONæ ‡ç­¾: {tool_call.function.arguments}")
                    function_response = f"é”™è¯¯: æ£€æµ‹åˆ°JSONæ ‡ç­¾ï¼Œè¯·ä½¿ç”¨æ ‡å‡†çš„å·¥å…·è°ƒç”¨æ ¼å¼"
                    messages.append({"tool_call_id": tool_call.id, "role": "tool", "name": function_name, "content": function_response})
                    continue
                
                # å°è¯•æ¸…ç†å’Œä¿®å¤å‚æ•°æ ¼å¼
                cleaned_args = tool_call.function.arguments
                if "REDACTED_SPECIAL_TOKEN" in cleaned_args:
                    # å°è¯•æå–JSONéƒ¨åˆ†
                    import re
                    json_match = re.search(r'\{[^}]*\}', cleaned_args)
                    if json_match:
                        cleaned_args = json_match.group(0)
                        print(f"ğŸ”§ å°è¯•æ¸…ç†å‚æ•°: {cleaned_args}")
                        try:
                            function_args = json.loads(cleaned_args)
                        except json.JSONDecodeError:
                            print(f"âŒ æ¸…ç†åçš„å‚æ•°ä»ç„¶æ— æ•ˆ: {cleaned_args}")
                            function_response = f"é”™è¯¯: æ— æ³•è§£æå·¥å…·å‚æ•°ï¼Œè¯·é‡æ–°è°ƒç”¨å·¥å…·"
                            messages.append({"tool_call_id": tool_call.id, "role": "tool", "name": function_name, "content": function_response})
                            continue
                    else:
                        print(f"âŒ æ— æ³•ä»å‚æ•°ä¸­æå–æœ‰æ•ˆJSON: {cleaned_args}")
                        function_response = f"é”™è¯¯: æ— æ³•è§£æå·¥å…·å‚æ•°ï¼Œè¯·é‡æ–°è°ƒç”¨å·¥å…·"
                        messages.append({"tool_call_id": tool_call.id, "role": "tool", "name": function_name, "content": function_response})
                        continue
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªREDACTED_SPECIAL_TOKENï¼ˆæ›´ä¸¥æ ¼çš„æ£€æŸ¥ï¼‰
                if cleaned_args.count("REDACTED_SPECIAL_TOKEN") > 1:
                    print(f"âŒ æ£€æµ‹åˆ°å¤šä¸ªREDACTED_SPECIAL_TOKEN: {cleaned_args}")
                    function_response = f"é”™è¯¯: æ£€æµ‹åˆ°å¤šä¸ªREDACTED_SPECIAL_TOKENï¼Œè¯·ä½¿ç”¨æ ‡å‡†çš„å·¥å…·è°ƒç”¨æ ¼å¼"
                    messages.append({"tool_call_id": tool_call.id, "role": "tool", "name": function_name, "content": function_response})
                    continue

                print(f"ğŸ”§ æ‰§è¡Œå·¥å…·: {function_name}")
                print(f"ğŸ“ å‚æ•°: {function_args}")
                print(f"ğŸ” å‚æ•°ç±»å‹: {type(function_args)}")

                # å®æ—¶è¾“å‡ºå·¥å…·è°ƒç”¨ä¿¡æ¯
                tool_call_chunk = {
                    "id": response.id, 
                    "object": "chat.completion.chunk", 
                    "created": response.created, 
                    "model": response.model,
                    "choices": [{"index": 0, "delta": {"content": f"ğŸ”§ è°ƒç”¨å·¥å…·: {function_name} (å‚æ•°: {function_args})\n"}, "finish_reason": None}]
                }
                yield f"data: {json.dumps(tool_call_chunk)}\n\n"
                
                # å¼ºåˆ¶ç«‹å³æ˜¾ç¤º
                yield f"data: {json.dumps({'id': response.id, 'object': 'chat.completion.chunk', 'created': response.created, 'model': response.model, 'choices': [{'index': 0, 'delta': {'content': ''}, 'finish_reason': None}]})}\n\n"

                # --- æœ€ç»ˆé‡æ„ï¼šå®Œå…¨ç»Ÿä¸€çš„å·¥å…·è°ƒç”¨é€»è¾‘ ---
                # --- æ–°çš„ã€æ¨¡å—åŒ–çš„å·¥å…·é›† ---
                available_tools = {
                    # v5.2 Tools
                    "get_task_status": tool_module.get_task_status,
                    "list_files": tool_module.list_files,
                    "add_genome_to_config": tool_module.add_genome_to_config,
                    "unsupported_request": tool_module.unsupported_request,
                    # Reactæ¨¡å¼å·¥å…·
                    "check_environment_tool": tool_module.check_environment_tool,
                    "setup_environment_tool": tool_module.setup_environment_tool,
                    "search_genome_tool": tool_module.search_genome_tool,
                    "search_fastq_tool": tool_module.search_fastq_tool,
                    "download_genome_tool": tool_module.download_genome_tool,
                    "download_fastq_tool": tool_module.download_fastq_tool,
                    "validate_fastq_tool": tool_module.validate_fastq_tool,
                    "check_files_exist_tool": tool_module.check_files_exist_tool,
                    "build_star_index_tool": tool_module.build_star_index_tool,
                    "run_fastp_tool": tool_module.run_fastp_tool,
                    "run_star_align_tool": tool_module.run_star_align_tool,
                    "run_featurecounts_tool": tool_module.run_featurecounts_tool,
                    "collect_results_tool": tool_module.collect_results_tool,
                    "generate_report_tool": tool_module.generate_report_tool,
                    "react_status_tool": tool_module.react_status_tool,
                    "react_plan_tool": tool_module.react_plan_tool,
                    "react_evaluate_tool": tool_module.react_evaluate_tool,
                    "react_summary_tool": tool_module.react_summary_tool,
                    "validate_tool_call_format": tool_module.validate_tool_call_format,
                }
                
                global task_id_counter
                function_response = ""

                try:
                    function_to_call = available_tools.get(function_name)
                    if not function_to_call:
                        function_response = f"é”™è¯¯ï¼šæœªçŸ¥çš„å·¥å…·åç§° '{function_name}'"
                    else:
                        # --- æ–°çš„ã€æ›´æ¸…æ™°çš„ä¾èµ–æ³¨å…¥é€»è¾‘ ---
                        tool_kwargs = function_args.copy()

                        # å®šä¹‰å“ªäº›å·¥å…·éœ€è¦å“ªäº›å…±äº«èµ„æº
                        TOOLS_NEEDING_DB = {
                            "get_task_status",
                        }
                        TOOLS_NEEDING_COUNTER = {}

                        # æ³¨å…¥æ•°æ®åº“å’Œé”
                        if function_name in TOOLS_NEEDING_DB:
                            tool_kwargs["task_database"] = TASK_DATABASE
                            tool_kwargs["db_lock"] = db_lock
                        
                        # æ³¨å…¥ä»»åŠ¡IDè®¡æ•°å™¨
                        if function_name in TOOLS_NEEDING_COUNTER:
                            # ä¼ é€’å½“å‰çš„è®¡æ•°å™¨å€¼
                            tool_kwargs["task_id_counter"] = task_id_counter

                        # è°ƒç”¨å·¥å…·
                        tool_result = function_to_call(**tool_kwargs)

                        # å¦‚æœå·¥å…·æ›´æ–°äº†è®¡æ•°å™¨ï¼Œåˆ™åŒæ­¥å›å…¨å±€è®¡æ•°å™¨
                        # è¿™æ˜¯ä¸ºäº†è®© create_task å’Œ download_genome éƒ½èƒ½å®‰å…¨åœ°å¢åŠ ID
                        if function_name in TOOLS_NEEDING_COUNTER and isinstance(tool_result, dict):
                            with db_lock:
                                task_id_counter = tool_result.get("updated_task_id_counter", task_id_counter)

                        # ç»Ÿä¸€å¤„ç†è¿”å›ç»“æœ
                        # å¦‚æœç»“æœæ˜¯å­—å…¸ï¼Œåºåˆ—åŒ–ä¸º JSON å­—ç¬¦ä¸²
                        if isinstance(tool_result, dict):
                            function_response = json.dumps(tool_result, ensure_ascii=False, indent=2)
                            # æå–å…³é”®ä¿¡æ¯ç”¨äºæ˜¾ç¤º
                            if "message" in tool_result:
                                display_message = tool_result["message"]
                            elif "status" in tool_result:
                                display_message = f"çŠ¶æ€: {tool_result['status']}"
                            else:
                                display_message = "æ‰§è¡Œå®Œæˆ"
                        else:
                            # å¦åˆ™ï¼Œç›´æ¥ä½¿ç”¨è¿”å›çš„å­—ç¬¦ä¸²
                            function_response = str(tool_result)
                            display_message = function_response

                        # å®æ—¶è¾“å‡ºå·¥å…·æ‰§è¡Œç»“æœ
                        tool_result_chunk = {
                            "id": response.id, 
                            "object": "chat.completion.chunk", 
                            "created": response.created, 
                            "model": response.model,
                            "choices": [{"index": 0, "delta": {"content": f"âœ… å·¥å…· {function_name} æ‰§è¡Œå®Œæˆ: {display_message}\n\n"}, "finish_reason": None}]
                        }
                        yield f"data: {json.dumps(tool_result_chunk)}\n\n"
                        
                        # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
                        yield f"data: {json.dumps({'id': response.id, 'object': 'chat.completion.chunk', 'created': response.created, 'model': response.model, 'choices': [{'index': 0, 'delta': {'content': ''}, 'finish_reason': None}]})}\n\n"

                except Exception as e:
                    # ç»Ÿä¸€çš„å¼‚å¸¸å¤„ç†
                    function_response = f"æ‰§è¡Œå·¥å…· '{function_name}' æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}"
                    
                    # å®æ—¶è¾“å‡ºå·¥å…·æ‰§è¡Œé”™è¯¯
                    tool_error_chunk = {
                        "id": response.id, 
                        "object": "chat.completion.chunk", 
                        "created": response.created, 
                        "model": response.model,
                        "choices": [{"index": 0, "delta": {"content": f"âŒ å·¥å…· {function_name} æ‰§è¡Œå¤±è´¥: {e}"}, "finish_reason": None}]
                    }
                    yield f"data: {json.dumps(tool_error_chunk)}\n\n"

                # 6. å°†å·¥å…·æ‰§è¡Œç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²ä¸­
                messages.append(
                    {
                        "tool_call_id": tool_call.id,
                        "role": "tool",
                        "name": function_name,
                        "content": function_response,
                    }
                )
            
            # 7. è§‚å¯Ÿé˜¶æ®µ - åˆ†æå·¥å…·æ‰§è¡Œç»“æœ
            observation_chunk = {
                "id": response.id, 
                "object": "chat.completion.chunk", 
                "created": response.created, 
                "model": response.model,
                "choices": [{"index": 0, "delta": {"content": f"ğŸ‘€ è§‚å¯Ÿé˜¶æ®µ (å¾ªç¯ {react_cycle_count}): æ­£åœ¨åˆ†æå·¥å…·æ‰§è¡Œç»“æœå¹¶è¯„ä¼°ä¸‹ä¸€æ­¥è¡ŒåŠ¨...\n"}, "finish_reason": None}]
            }
            yield f"data: {json.dumps(observation_chunk)}\n\n"
            
            # å¼ºåˆ¶ç«‹å³æ˜¾ç¤º
            yield f"data: {json.dumps({'id': response.id, 'object': 'chat.completion.chunk', 'created': response.created, 'model': response.model, 'choices': [{'index': 0, 'delta': {'content': ''}, 'finish_reason': None}]})}\n\n"
            
            # 8. æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­Reactå¾ªç¯
            # åˆ†æå·¥å…·æ‰§è¡Œç»“æœï¼Œå†³å®šæ˜¯å¦ç»§ç»­å¾ªç¯
            should_continue = False
            
            for tool_call in tool_calls:
                function_name = tool_call.function.name
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯è®¡åˆ’å·¥å…·ï¼Œå¦‚æœæ˜¯ï¼Œéœ€è¦ç»§ç»­æ‰§è¡Œ
                if function_name in ["plan_analysis_task", "react_plan_tool"]:
                    should_continue = True
                    break
                
                # æ£€æŸ¥æ˜¯å¦æœ‰éœ€è¦è¿›ä¸€æ­¥å¤„ç†çš„å·¥å…·ï¼ˆå¦‚é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡ï¼‰
                if function_name in ["download_genome_files"]:
                    should_continue = True
                    break
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æŸ¥è¯¢å·¥å…·ï¼Œå¦‚æœæ˜¯ï¼Œéœ€è¦ç»§ç»­æ‰§è¡Œï¼ˆè®©LLMå†³å®šä¸‹ä¸€æ­¥ï¼‰
                if function_name in ["get_task_status", "list_available_genomes", "list_files", "search_genome_tool", "search_fastq_tool", "validate_fastq_tool", "check_environment_tool", "check_files_exist_tool", "build_star_index_tool"]:
                    should_continue = True
                    break
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†æå·¥å…·ï¼Œå¦‚æœæ˜¯ï¼Œéœ€è¦ç»§ç»­æ‰§è¡Œï¼ˆè®©LLMå†³å®šä¸‹ä¸€æ­¥ï¼‰
                if function_name in ["run_fastp_tool", "run_star_align_tool", "run_featurecounts_tool", "collect_results_tool"]:
                    should_continue = True
                    break
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ç»ˆå·¥å…·ï¼Œå¦‚æœæ˜¯ï¼Œå¯ä»¥ç»“æŸå¾ªç¯
                if function_name in ["generate_report_tool", "react_summary_tool"]:
                    should_continue = False
                    break
            
            if should_continue:
                # ç»§ç»­Reactå¾ªç¯ï¼Œè®©LLMå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
                print(f"--- Reactå¾ªç¯ {react_cycle_count}: ç»§ç»­å¾ªç¯ï¼Œç­‰å¾…LLMå†³å®šä¸‹ä¸€æ­¥ ---")
                continue
            else:
                # æ‰€æœ‰å·¥å…·éƒ½æ‰§è¡Œå®Œæˆï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆå›å¤
                print(f"--- Reactå¾ªç¯ {react_cycle_count}: æ‰€æœ‰å·¥å…·æ‰§è¡Œå®Œæˆï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆå›å¤ ---")
                break

        else:
            # LLMæ²¡æœ‰è°ƒç”¨å·¥å…·ï¼Œå¯èƒ½åªæ˜¯è¾“å‡ºæ€è€ƒ
            print(f"--- Reactå¾ªç¯ {react_cycle_count}: LLM è¾“å‡ºå†…å®¹ï¼Œæ— å·¥å…·è°ƒç”¨ ---")
            content = response_message.content or ""
            
            # å°†LLMçš„æ€è€ƒå†…å®¹ä»¥OpenAIæ ¼å¼å‘é€åˆ°å‰ç«¯ï¼Œä½†ä¸ç»ˆæ­¢Reactå¾ªç¯
            if content.strip():
                # ä»¥OpenAIæ ¼å¼å‘é€æ€è€ƒå†…å®¹ï¼Œè®©CherryStudioèƒ½å¤Ÿæ­£å¸¸æ˜¾ç¤º
                response_chunk = {
                    "id": response.id, 
                    "object": "chat.completion.chunk", 
                    "created": response.created, 
                    "model": response.model,
                    "choices": [{"index": 0, "delta": {"content": content}, "finish_reason": None}]
                }
                yield f"data: {json.dumps(response_chunk)}\n\n"
            
            # å°†LLMçš„å›å¤æ·»åŠ åˆ°æ¶ˆæ¯å†å²ä¸­
            messages.append(response_message)
            
            # æ£€æŸ¥å†…å®¹æ˜¯å¦åŒ…å«æ˜ç¡®çš„ç»“æŸä¿¡å·
            if any(keyword in content.lower() for keyword in ["åˆ†æå®Œæˆ", "æµç¨‹ç»“æŸ", "ä»»åŠ¡å®Œæˆ", "æŠ¥å‘Šç”Ÿæˆå®Œæˆ", "æœ€ç»ˆæ€»ç»“", "åˆ†ææŠ¥å‘Š"]):
                print(f"--- Reactå¾ªç¯ {react_cycle_count}: æ£€æµ‹åˆ°å®Œæˆä¿¡å·ï¼Œç»“æŸå¾ªç¯ ---")
                break
            else:
                # ç»§ç»­Reactå¾ªç¯ï¼Œè®©LLMå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
                print(f"--- Reactå¾ªç¯ {react_cycle_count}: ç»§ç»­å¾ªç¯ï¼Œç­‰å¾…LLMå†³å®šä¸‹ä¸€æ­¥ ---")
                continue

    # 9. æœ€ç»ˆå›å¤é˜¶æ®µ - ç”Ÿæˆæ€»ç»“æ€§å›å¤
    if react_cycle_count > 0:
        print(f"--- ç”Ÿæˆæœ€ç»ˆå›å¤ (Reactå¾ªç¯å®Œæˆ: {react_cycle_count}) ---")
        print(f"å‘é€çš„æ¶ˆæ¯: {messages}")
        
        try:
            final_response = client.chat.completions.create(
                model=model_name,
                messages=messages,
                stream=True, # ä»¥æµå¼æ¨¡å¼è·å–æœ€ç»ˆå›å¤
                temperature=0.0
            )
            # æµå¼ä¼ è¾“æœ€ç»ˆå›å¤
            for chunk in final_response:
                content = chunk.choices[0].delta.content
                if content:
                    response_chunk = {
                        "id": chunk.id, "object": chunk.object, "created": chunk.created, "model": chunk.model,
                        "choices": [{"index": 0, "delta": {"content": content}, "finish_reason": None}]
                    }
                    yield f"data: {json.dumps(response_chunk)}\n\n"
        except Exception as e:
            print(f"ç”Ÿæˆæœ€ç»ˆå›å¤æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"

    # å‘é€ç»“æŸæ ‡å¿—
    final_chunk = {
        "id": response.id, "object": "chat.completion.chunk", "created": response.created, "model": response.model,
        "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}]
    }
    yield f"data: {json.dumps(final_chunk)}\n\n"
    yield "data: [DONE]\n\n"

@app.post("/chat/completions")
async def chat_completions(chat_input: ChatInput):
    """
    æ ¸å¿ƒèŠå¤©æ¥å£ï¼Œç°åœ¨å®ƒä¼šè°ƒç”¨çœŸæ­£çš„ Agent é€»è¾‘ã€‚
    """
    return StreamingResponse(stream_agent_response(chat_input), media_type="text/event-stream")




@app.get("/task-status/{task_id}")
def get_task_status_endpoint(task_id: str):
    """ç”¨äºè½®è¯¢é•¿æ—¶ä»»åŠ¡çŠ¶æ€çš„ç«¯ç‚¹ã€‚"""
    print(f"--- [çŠ¶æ€ç«¯ç‚¹] æ­£åœ¨æŸ¥è¯¢ä»»åŠ¡: {task_id} ---")
    try:
        # ç›´æ¥è°ƒç”¨æˆ‘ä»¬çš„å·¥å…·å‡½æ•°ï¼Œå¹¶ä¼ å…¥æ‰€éœ€çš„ä¾èµ–
        status_result = tool_module.get_task_status(
            task_id=task_id,
            task_database=TASK_DATABASE,
            db_lock=db_lock
        )
        # å¦‚æœå·¥å…·å‡½æ•°è¿”å›äº†å®ƒè‡ªå·±çš„é”™è¯¯ï¼ˆä¾‹å¦‚ "æ‰¾ä¸åˆ°ä»»åŠ¡"ï¼‰ï¼Œåˆ™å°†å…¶è½¬æ¢ä¸º 404
        if not status_result or status_result.get("status") == "error":
            raise HTTPException(status_code=404, detail=status_result.get("message", f"æ‰¾ä¸åˆ°ä»»åŠ¡ '{task_id}'ã€‚"))
        
        return status_result
    except HTTPException as he:
        raise he # é‡æ–°æŠ›å‡ºå·²æœ‰çš„ HTTP å¼‚å¸¸
    except Exception as e:
        print(f"!!! [çŠ¶æ€ç«¯ç‚¹] æŸ¥è¯¢çŠ¶æ€æ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# --- 5. å¯åŠ¨æœåŠ¡å™¨ ---
if __name__ == "__main__":
    import uvicorn
    print("æ­£åœ¨å¯åŠ¨æµå¼ Agent æœåŠ¡å™¨ v0.3.0 ï¼Œè®¿é—® http://localhost:48001")
    uvicorn.run(app, host="0.0.0.0", port=48001)